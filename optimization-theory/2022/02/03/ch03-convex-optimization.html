<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>[OptimizationTheory] CH03. Convex Optimization | YANG’s note</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="[OptimizationTheory] CH03. Convex Optimization" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Optimization theory summary note." />
<meta property="og:description" content="Optimization theory summary note." />
<link rel="canonical" href="https://ndo04343.github.io/blog/optimization-theory/2022/02/03/ch03-convex-optimization.html" />
<meta property="og:url" content="https://ndo04343.github.io/blog/optimization-theory/2022/02/03/ch03-convex-optimization.html" />
<meta property="og:site_name" content="YANG’s note" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-02-03T00:00:00-06:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="[OptimizationTheory] CH03. Convex Optimization" />
<script type="application/ld+json">
{"url":"https://ndo04343.github.io/blog/optimization-theory/2022/02/03/ch03-convex-optimization.html","@type":"BlogPosting","headline":"[OptimizationTheory] CH03. Convex Optimization","dateModified":"2022-02-03T00:00:00-06:00","datePublished":"2022-02-03T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://ndo04343.github.io/blog/optimization-theory/2022/02/03/ch03-convex-optimization.html"},"description":"Optimization theory summary note.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://ndo04343.github.io/blog/feed.xml" title="YANG's note" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">YANG&#39;s note</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/reports/">Reports</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/self-reflection/">Self-Reflection</a><a class="page-link" href="/blog/categories/">Categories</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">[OptimizationTheory] CH03. Convex Optimization</h1><p class="page-description">Optimization theory summary note.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-02-03T00:00:00-06:00" itemprop="datePublished">
        Feb 3, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      3 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#optimization-theory">optimization-theory</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/ch03-convex-optimization.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="3.1.-Introduction">3.1. Introduction<a class="anchor-link" href="#3.1.-Introduction"> </a></h4><p>Consider the objective function that is differentiable in optimization problem. If $\mathbf{w}^*$ is optimal, $\nabla_\mathbf{w} L$ must be zero vector. This condition is called gradient necessary condition. In optimization problem, we have to find global optimal. However, we only find local optimal everytime. Also, if we find the solution of $\nabla_\mathbf{w} L = \mathbf{0}$, there is no guarantee that the solution is a global optimal. <br /><br /></p>
<p>Here is something to think about.</p>
<blockquote><p>[Dauphin14] Y. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, Y. Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization.</p>
</blockquote>
<p>Above paper suggest that local minima problem is actually a very rare case that does not occur in a high dimensional space. First reason is gradient necessary condition. Every elements in gradient must be a zero at some $\mathbf{w}^*$. But this is very rare case. Also, in every direction, gradient must form a convex shape in a high-dimensional space. But, the probability of that happening is close to zero.</p>
<blockquote><p>Intuitively, in high dimensions, the chance that all the directions around a critical point lead upward is exponentially small w.r.t. the number of dimensions, unless the critical point is the global minimum or stands at an error level close to it, i.e., it is unlikely one can find a way to go further down.</p>
</blockquote>
<p>In this context, since both local minima and the global minima are same, it can be seen that a convex function can be good loss function.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="3.2.-Convex-Optimization">3.2. Convex Optimization<a class="anchor-link" href="#3.2.-Convex-Optimization"> </a></h4><h5 id="Definition.3.1.-Convex-Set">Definition.3.1. Convex Set<a class="anchor-link" href="#Definition.3.1.-Convex-Set"> </a></h5><p>A set $S$ is said to be convex if</p>
$$
\mathbf{x}, \mathbf{y} \in S, \,\ \text{then} \,\ t\mathbf{x} + (1 - t)\mathbf{y} \in S, \,\ \text{for} \,\ t \in [0, 1]
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Definition.3.2.-Convex-Function">Definition.3.2. Convex Function<a class="anchor-link" href="#Definition.3.2.-Convex-Function"> </a></h5><p>A function $f: S \rightarrow \mathbb{R}$ is said to be convex if</p>
$$
^\forall \mathbf{x}_1, \mathbf{x}_2 \in S, \,\ f(t\mathbf{x}_1 + (1 - t)\mathbf{x}_2) \le tf(\mathbf{x}_1) + (1 - t)f(\mathbf{x}_2) \,\ \text{for} \,\ t \in [0, 1]
$$<p>where $S$ is convex subset of a real vector space.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Theorem.3.1.">Theorem.3.1.<a class="anchor-link" href="#Theorem.3.1."> </a></h5><p>In convex function, some local minimum is a global minimum.</p>
<p><strong>Proof.</strong><br />
Trivial(proof by contradiction).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Theorem.3.2.">Theorem.3.2.<a class="anchor-link" href="#Theorem.3.2."> </a></h5><p>A twice differentiable function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is a convex function if and only if $\nabla^2 f \succeq 0$.</p>
<p><strong>Proof.</strong><br />
Suppose $f$ has a positive semidefinite hessian matrix.<br />
Then for some $\mathbf{x}_0, \mathbf{x}_1$ in the domain, and $t \in [0, 1]$, we have</p>
$$
g(t) = f(t\mathbf{x}_0 + (1- t)\mathbf{x}_1)
$$<p>which have the first and second derivative</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\begin{matrix}
\frac{dg}{dt} = (\mathbf{x}_0 - \mathbf{x}_1)^T \nabla_\mathbf{x} f(t\mathbf{x}_0 + (1 - t)\mathbf{x}_1) \\
\frac{d^2g}{dt^2} = (\mathbf{x}_0 - \mathbf{x}_1)^T \nabla_\mathbf{x}^2 f(t\mathbf{x}_0 + (1 - t)\mathbf{x}_1)(\mathbf{x}_0 - \mathbf{x}_1) \\
\end{matrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since the hessian matrix of $f$ is positive semidefinite, $\frac{d^2g}{dt^2} \ge 0$ for $t \in [0, 1]$.<br />
Then we can get</p>
$$
\begin{matrix}
g(0) \ge g(t) + g^\prime(t)(-t) \\
g(1) \ge g(t) + g^\prime(t)(1 - t) \\
\end{matrix} \quad (\because \,\ \text{Taylor's theorem})
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then 
$$
g(t) \le tg(1) + (1 - t)g(0)
$$</p>
$$
\therefore \,\ ^\forall \mathbf{x}_0, \mathbf{x}_1 \in D, \,\ f(t\mathbf{x}_0 + (1 - t)\mathbf{x}_1) \le tf(\mathbf{x}_0) + (1 - t)\mathbf{x}_1 \quad \blacksquare
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Definition.3.3.-Convex-Optimization-Problem">Definition.3.3. Convex Optimization Problem<a class="anchor-link" href="#Definition.3.3.-Convex-Optimization-Problem"> </a></h5><p>A convex optimization problem is an optimization problem in which the objective function is a convex function and the feasible set is a convex set.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="3.3.-Properties">3.3. Properties<a class="anchor-link" href="#3.3.-Properties"> </a></h4><p>Followings are convex function:</p>
<ul>
<li>Exponential function</li>
<li>Power function(in some case)</li>
<li>Logarithmic function</li>
<li>Affine function</li>
<li>Quadratic function </li>
<li>Mean square error</li>
<li>Max function</li>
<li>Norm function</li>
<li>Softmax function</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Theorem.3.3.">Theorem.3.3.<a class="anchor-link" href="#Theorem.3.3."> </a></h5><p>Let $f(\mathbf{x}) = h(g(\mathbf{x})) = h(g_1(\mathbf{x}), \cdots, g_k(\mathbf{x}))$ <br />
where $g: \mathbb{R}^n \rightarrow \mathbb{R}^k, \,\ h : \mathbb{R}^k \rightarrow \mathbb{R}, \,\ \text{and} \,\ f : \mathbb{R}^n \rightarrow \mathbb{R}$.<br />
Then</p>
<ul>
<li>$f$ is convex if $h$ is convex and nondecreasing in each argument, g is convex.</li>
<li>$f$ is convex if $h$ is convex and nonincreasing in each argument, g is concave.</li>
</ul>
<p><strong>Proof.</strong><br />
Trivial(by chain rule and above theorems).</p>

</div>
</div>
</div>
</div>





  </div><a class="u-url" href="/blog/optimization-theory/2022/02/03/ch03-convex-optimization.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Heesung Yang&#39;s note</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/ndo04343" target="_blank" title="ndo04343"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
