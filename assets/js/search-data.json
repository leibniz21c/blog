{
  
    
        "post0": {
            "title": "[NDIR] 연구 중간 보고",
            "content": "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 .",
            "url": "https://ndo04343.github.io/blog/report/encrypted/2022/03/01/NDIR-midterm-report-220302.html",
            "relUrl": "/report/encrypted/2022/03/01/NDIR-midterm-report-220302.html",
            "date": " • Mar 1, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "[NDIR] Near-Duplicate Image Detection System Using Coarse-to-Fine Matching Scheme Based on Global and Local CNN Features",
            "content": "1. Abstract . They proposed a coarse-to-fine feature matching scheme using both global and local CNN features for real-time near-duplicate image detection. | . 2. Proposed Method Summary . Image(&#39;fig1.png&#39;, width=500) . Figure 1 shows the framework of the proposed approach. . Summary) . Coarse matching stage Generate CFMs(convolutional feature maps) by feeding images into a pre-trained CNN model. | Extract global features from each image using sum-pooling. | Match these features between images to obtain the candidate images of a given query from an image database. | | And there are some candidate images. For those, . Fine matching stage Generate local regions Generate $N$ number of local regions by detecting maximum points of central cropped CFMs(total $N_P$, i.e., $N_P = 256$). | Generate by detecting local maximum points of central cropped | . | Match to further detect the near-duplicate versions of the query. | | 2.1. CFM Generation . The output of a convolutional layer is a set of feature maps, i.e., CFMs. In their approach, they feed each image to a pretrained CNN model, and use the output of the last convolutional layer for feature extraction. . Image(&quot;fig2.png&quot;, width=400) . Figure 2 shows the 256 CFMs generated from the last convolutional layer(the fifth convolutional layer) after feeding an image into the pre-trained AlexNet model, where the sizes of CFMs are proportional to the size of the original image. . 2.2. Coarse Matching Stage . They first match global features to implement a coarse feature matching to efficiently filter most irrelevant images of a given query. . 2.2.1. The Extraction of Global CNN Feature . They adopt sum-pooling operation on the CFMs to extract the global features. . For a given Image $I_i$, they feed it into the pretrained AlexNet model and collect the output of the fifth convolutional layer to form a set of CFMs, denoted as $ text{MS}_i = { M_i^1, M_i^2, cdots, M_i^k, cdots , M_i^M }$, where $1 le k le M$ and $M = 256$. For each CFM, i.e., $M_i^k$, its size and activations are denoted as $W times H$ and $F_i^k = { f_i^k(x, y) : 1 le x le W, 1 le y le H }$, respectively. Subsequently, they use Equation (1) to extract global features by sum-pooling operation: . $$ tag{1} phi(F_i^k) = sum_{y = 1}^{H} sum_{x = 1}^{W} f_i^k(x, y) $$ After this, hey concatenate all of these feature values to obtain a 256-dimensional feature vector . $$ partial(I_i) = ( phi(F_i^1), phi(F_2^k), cdots, phi(F_M^k)) $$, and normalize the feature vector as . $$ V(I_i) = frac{ partial (I_i)}{|| partial (I_i)||_2}. $$ 2.2.2. Global Feature Matching . For a given query image $I_q$ and a database image $I_d$, there are $V(I_q)$ and $V(I_d)$. Then they employ Equation (2) to compute the inner product of the two feature vectors to measure the global similarity between the two images. . $$ tag{2} text{SIM}(I_q, I_d) = left&lt; V(I_q), V(I_d) right&gt; $$ After computing the similarity, they sort all similarity values in descending order $ { text{SIM}_1, text{SIM}_2, cdots, text{SIM}_{N_D} }$, where $ text{SIM}_1 &gt; text{SIM}_2 &gt; cdots &gt; text{SIM}_{N_D}$ and $N_D$ means the number of database images. In their method, they only keep $N_ text{Top} = 1000$ detected images of the query as its candidate images, and remove the others. . 2.3. Fine Matching Stage . 2.3.1. Central Cropping . Denote the ratio between the area of each cropped CFM and that of each original CFM as $ alpha$. If the area of an original CFM is $S_ text{CFM} = W times H$, the area of a corpped CFM is $S_ text{CFM}^ prime = alpha times S_ text{CFM}$, where $0 &lt; alpha &lt; 1$. . To generate the cropped CFM, they set the coordinates of the central point of the cropped CFM by . $$ tag{3} begin{cases} x_c = left lfloor frac{W}{2} right rfloor + 1 y_c = left lfloor frac{H}{2} right rfloor + 1 end{cases} $$Thus, the cropped CFM can be denoted as $R = [(x_c, y_c), W^ prime, H^ prime]$, which will be used for local region detection. . 2.3.2. Local Region Detection . Image(&#39;fig3.png&#39;, width=500) . Above figure is the flowchart of local region generation. There are two main process. . CFM based $ rightarrow$ $N$ local regions | GBVS based $ rightarrow$ all of local regions | They detect the points with maximum activations, i.e., maximum points on the cropped CFMs, and the generate the regions surrounding these maximum points for local CNN feature extraction. For a CFM $M_i^k$, they select the maximum value among all the activations by . $$ tag{4} m_i^k = max left { f_i^k (x, y) : 1 le x le W, 1 le y le H right } $$Also, they generate the saliency map by the graph-based visual saliency detection(GBVS) algorithm. Then, they also apply central cropping on the saliency map, and detect the local maximum values of the saliency map. The patch surrounding the local maximum points are used as the local region, where the side length of the patch is denoted as $L$. . Suppose $N_P$ maximum points are generated from the CFMs in total. Thus, $N_p$ corresponding local regions are generated. . They will not use all the regions generated from CFMs for feature extraction. Instead, they sort these regions in descending order according to the activation values of the corresponding maximum points, and select the first $N$ regions for local feature extraction. | However, they use all the regions generated from the saliency map for local feature extraction. | . After these, $M$ local regions are extracted. . I am not sure why the local regions made of the local maximum points of the saliency map are finite and the M value is a fixed representation. . 2.3.3. Local Features Extraction and Matching . For $M$ local regions, they extract 256-dimensional feature vectors by sum-pooling the activations of CFMs, and then normalize it. Thus, for a given image $I_i$, they can extract a set of 256-simensional normalized local feature vectors $VS_{I_i} = { V_1(I_i), cdots, V_M(I_i) }$. . For a query image $I_q$ and a candiate image $I_c$, there are $VS_{I_q} = { V_1(I_q), cdots, V_M(I_q) }$ and $VS_{I_c} = { V_1(I_c), cdots, V_M(I_c) }$. By comparing a query feature vector $V_k(I_q)$ to each feature vector $V_j(I_c)$ in $VS_c$, where $i le j le M$, they can obtain $M$ similarity scores and then select the maximum score as the matching score of $V_k(I_q)$. . $$ max left { left&lt; V_k(I_q), V_j(I_c) right&gt; : j = 1,2, cdots, M right } $$Thus, there are $M$ matching scores in total. And they sum up all the matching scores as the final similarity between the query image $I_q$ and the candidate image $I_c$ by Equation (6). . $$ tag{6} text{SIM}(I_q, I_c) = sum_{k = 1}^{M} max left { left&lt; V_k(I_q), V_j(I_c) right&gt; : j = 1,2, cdots, M right } $$Finally, they compare the similarity score to a pre-set threshold to determine whether the candidate image $I_c$ is near-duplicate version of the query $I_q$. . 3. Experiments . 3.1. Datasets and Evaluation Criteria . Three near-duplicate image detection dataset . Oxford5K dataset | Holidays dataset | Paris6K dataset | . All of those seems like non-identical near duplicate image dataset. . Performance measure with mAP and average query time. . 3.2. Parameter Determination . Parameters . $ alpha$ : central cropping ratio | $N$ : maximum number of regions | $L$ : side length of regions | . The parameter $ alpha$ can be easily set as 0.5 by following experiment that fix parameters $L$ and $N$ to the default values, 3 and 100. . Image(&quot;fig4-5.png&quot;, width=400) . The parameter $N$ can be easily set as 100. . Image(&quot;fig6-7.png&quot;, width=400) . The parameter $L$ can be easily set as 5. . Image(&quot;fig8.png&quot;, width=400) . Image(&quot;fig9.png&quot;, width=400) . 3.3. Performance When Using Different Pre-Trained Networks . They chose four famous convolutional neural networks including AlexNet, VGG16, VGG19, and ResNet-18, where the last convolutional layers or ReLu layers. . Image(&quot;table1.png&quot;) . To find a good trade-off between accuracy and efficiency, they chose AlexNet in their method. . 3.4. Performance Comparison . After selecting the parameters, they use Oxford5K as the baseline dataset to compare the detection performances between their method and its two other versions in the aspects of detection accuracy, average time cost, and average memory consumption. The two versions are just using global CNN features or local CNN features. . Image(&quot;table2.png&quot;) . Overall, the proposed method generally outperforms state-of-the-art methods. . Image(&quot;table3.png&quot;) . Image(&quot;fig10.png&quot;, width=500) . Above figure shows several examples of detection results of the proposed method. The four queries are listed in the first column, and the corresponding top four ranked detection results of each query are shown in the following columns. . 4. Conclusions . The pre-trained CNN models are originally designed for image classification. Thus, it might be more effective to adopt the transfer learning methods to generate a fine-tuned CNN model for near-duplicate image detection. | The saliency mapo is generated by an unsupervised method to locate potential object regions for local feature extraction. IN future work, it can be useful to use the supervised object recognition methods to accurately locate the object regions for local feature extraction to further improve the detection performance. | . Commnets . We may be handle local feature extraction module. As the author said, selecting local features using models used for object detection is better than using saliency maps. | It is a simple but very good idea to use global features to remove irrelevant images first, and then to increase efficiency by removing local features. | Actually, the task seems a little different from what I thought. The near-duplicate image refered by the author seems to be a non-identical near-duplicate image. Even in this case, it is questionable whether it can be used in applications such as copyright protection and redundancy elimination described in the introduction. This content should be dealt with in the identical near-duplication image retrieval, so I do not know if the data set selection is correct. | .",
            "url": "https://ndo04343.github.io/blog/near-duplicate%20image%20detection/paper-review/2022/02/25/Near-Duplicate-Image-Detection-System-Using-Coarse-to-Fine-Matching-Scheme-Based-on-Global-and-Local-CNN-Features.html",
            "relUrl": "/near-duplicate%20image%20detection/paper-review/2022/02/25/Near-Duplicate-Image-Detection-System-Using-Coarse-to-Fine-Matching-Scheme-Based-on-Global-and-Local-CNN-Features.html",
            "date": " • Feb 25, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "[Neural Network] Introduction of Autoencoder",
            "content": "1. Overview . An autoencoder is a type of artificial neural network used to learn efficient data encoding and decoding in an unsupervised(more exactly, self-supervised) manner. The aim of autoencoder is to learn a representation(encoding) for a set of data, typically for dimensionality reduction. . Several variants exists: . Sparse autoencoders | Denoising autoencoders | Variational autoencoders | etc | . Applications: . Dimensionality reduction | Data compression | Representation learning | Face recognition | etc | . 2. Autoencoder . 2.1. Architecture . An autoencoder learns to copy its input at its output . Image(&quot;fig1.png&quot;, width=500) . A simple form of autoencoder is composed of an input layer, a hidden layer, and an output layer. . $$ phi : mathbf{x} in mathbb{R}^d rightarrow mathbf{h} in mathbb{R}^p $$ $$ mathbf{h} = sigma(W mathbf{x} + mathbf{b}) in mathbb{R}^p $$where $ mathbf{h}$ is called code, hidden variable, or latent variable. . $$ psi : mathbf{h} in mathbb{R}^p rightarrow mathbf{x}^ prime in mathbb{R}^d $$ $$ mathbf{x}^ prime = sigma^ prime (W^ prime mathbf{h} + mathbf{b}^ prime) $$ It is often constrained that $W = (W^ prime)^T$. Encoder maps the input into the code, and decoder maps the code to a reconstruction of the original input. . Image(&quot;fig2.png&quot;, width=600) . Autoencoder can have a deep architecture. In this time, input-output relationship is . $$ hat{ mathbf{x}} = varphi_o( W_o varphi_L ( W_L varphi_{L - 1} ( cdots (W_1 mathbf{x} + mathbf{b}_1) cdots) + mathbf{b}_L) + mathbf{b}_o) $$ 2.2. Training . Autoencoders are trained to minimize reconstruction errors such as mean squared errors. Here is some tip to training autoencoders. . Continuous input : MSE | . $$ mathcal{L}( mathbf{x}, mathbf{x}^ prime) = mathbb{E} left[ || mathbf{x} - mathbf{x}^ prime||^2 right] $$where the expectation is taken over training samples. . Discrete input : cross-entropy $$ mathcal{L}( mathbf{x}, mathbf{x}^ prime) = - frac{1}{N} sum_{i = 1}^N (x_i log_ hat{x_i} + (1 - x_i) log (1 - hat{x_i}) ) $$ where softmax activation function is uesd at output nodes. | . The training of autoencoder can be performed through the backprop. . 2.3. Properties . In general, autoencoders perform compression by setting. Therefore, there must be information loss. . If linear activations are used, or only a single sigmoid hidden layer, then the optimal solution to an autoencoder is strongly related to principal component analysis(PCA). If $p , text{(number of hidden nodes)} &lt; d , text{(number of input nodes)}$, the weights of autoencoder span the same vector subspace as the one spanned by the first $p$ principal components, and the output of the autoencoder is an orthogonal projection onto this subspace. The autoencoder weights are not equal to the principal components, and are generally not orthogonal, yet the principal components may be recovered from them using the singular value decomposition. . Autoencoder is more powerful than PCA due to its nonlinearity. . 3. Sparse Autoencoder . $$ p ge d $$Sparse autoencoder may include more hidden units thatn inputs, but only a small number of the hidden units are allowed to be active at once. . Image(&quot;fig3.png&quot;, width=200) . Only above orange hidden nodes can activate at once. . It learns representations in a way that encourages sparsity, imporved performance is obtained on classification tasks. Specifically, a sparse autoencoder is an autoencoder whose training criterion involves a sparsity penalty $ Omega( mathbf{h})$ on the code. A popular sparsity constraint is the KL divergence with additionally considering L1 or L2 regularization constraint . $$ frac{1}{N} sum_{i = 1}^N mathcal{L}(x_i, hat{x_i}) + lambda(||W|| + ||W||^2) + beta sum_{j = 1}^m D_{KL}( rho || hat{ rho_j}) $$where $ hat{ rho_j} = frac{1}{N} sum_{i = 1}^N h_j(x_i)$ is an average activation of the hidden unit $j$. . 4. Denoising Autoencoder . DAEs take a partially corrupted input and are trained to recover the original undistorted input. In pactice, the objective of denoising autoencoders is that of clearning the corrupted input, or denoising. The input is corrupted by through stochastic mapping . $$ overset{ sim}{ mathbf{x}} sim q_D( overset{ sim}{ mathbf{x}} | mathbf{x}) $$ Image(&quot;fig4.png&quot;, width=600) . There are some kinds of corruption processes. . Gaussian noise : $$ overset{ sim}{ mathbf{x}} | mathbf{x} sim mathcal{N}( mathbf{x}, mathbf{ sigma}^2I) $$ . | Masking noise : A fraction of the input chosen at random for each example is forced to 0. . | Salt-and-pepper noise : A fraction of the input chosen at random for each example is set to its minimum of maximum value with uniform probability. . | . Notice that the corrption of the input is performed only during the training phase of the DAE. Once the model has learnt the optimal parameters, in order to extract the representations from the original data, no corruption is added. . 5. Simple Implementation of Basic Autoencoder . In this time, we construct autoencoder model, basically. Our purpose is to learn the representation of the MNIST dataset and compress it into a one-dimensional latent variable. After that, we will try to generate various number handwriting by adjusting the one-dimensional latent variable. . We have to import following modules. . import torch import torchvision import torch.nn.functional as F from torch import nn, optim from torchvision import transforms, datasets . Get MNIST dataset and create dataset and module. . trainset = datasets.MNIST( root=&#39;./data/&#39;, train=True, download=True, transform=transforms.ToTensor() ) train_loader = torch.utils.data.DataLoader( dataset=trainset, batch_size=64, shuffle=True, num_workers=1 ) . Create model. For good performance, we have to use . Good generalization performance | Xavier initialize | Batch normalization | Proper learning rate | etc | . In pytorch, nn.Linear layers are initialized xavier initializing, automatically. . class AE(nn.Module): def __init__(self): super(AE, self).__init__() # Encoder self.encoder = nn.Sequential( nn.Linear(28*28, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Linear(128, 64), nn.BatchNorm1d(64), nn.ReLU(), nn.Linear(64, 32), nn.BatchNorm1d(32), nn.ReLU(), nn.Linear(32, 1), nn.BatchNorm1d(1), ) # Decoder self.decoder = nn.Sequential( nn.Linear(1, 32), nn.BatchNorm1d(32), nn.ReLU(), nn.Linear(32, 64), nn.BatchNorm1d(64), nn.ReLU(), nn.Linear(64, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Linear(128, 28*28), nn.BatchNorm1d(28*28), nn.Sigmoid(), ) def forward(self, x): latent = self.encoder(x) decoded = self.decoder(latent) return latent, decoded . dev = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) print(&quot;Device :&quot;, dev) . Device : cuda . model = AE().to(dev) optim = torch.optim.Adam(model.parameters(), lr=0.001) criterion = nn.MSELoss() . def train(model, train_loader, epoch): model.train() for i in range(epoch): sum_loss = 0.0 for data, label in train_loader: data = data.view(-1, 28*28).to(dev) label = label.to(dev) predict = model(data)[1] loss = criterion(predict, data) sum_loss += loss optim.zero_grad() loss.backward() optim.step() print(&quot;EPOCH {}: loss - {}&quot;.format(i + 1, sum_loss)) train(model, train_loader, 100) . EPOCH 1: loss - 144.8663787841797 EPOCH 2: loss - 90.95703887939453 EPOCH 3: loss - 74.48108673095703 EPOCH 4: loss - 67.24845123291016 EPOCH 5: loss - 63.10108947753906 EPOCH 6: loss - 56.50653839111328 EPOCH 7: loss - 50.313140869140625 EPOCH 8: loss - 48.538490295410156 EPOCH 9: loss - 47.55280303955078 EPOCH 10: loss - 46.87140655517578 EPOCH 11: loss - 46.44161605834961 EPOCH 12: loss - 45.992095947265625 EPOCH 13: loss - 45.9116325378418 EPOCH 14: loss - 45.62765884399414 EPOCH 15: loss - 45.36041259765625 EPOCH 16: loss - 45.25257873535156 EPOCH 17: loss - 44.98619079589844 EPOCH 18: loss - 44.94349670410156 EPOCH 19: loss - 44.7140007019043 EPOCH 20: loss - 44.542564392089844 EPOCH 21: loss - 44.31476974487305 EPOCH 22: loss - 44.24300003051758 EPOCH 23: loss - 44.26637268066406 EPOCH 24: loss - 44.159706115722656 EPOCH 25: loss - 44.04599380493164 EPOCH 26: loss - 44.11685562133789 EPOCH 27: loss - 43.9760627746582 EPOCH 28: loss - 43.82699966430664 EPOCH 29: loss - 43.77445602416992 EPOCH 30: loss - 43.685997009277344 EPOCH 31: loss - 43.655799865722656 EPOCH 32: loss - 43.6832275390625 EPOCH 33: loss - 43.557071685791016 EPOCH 34: loss - 43.550148010253906 EPOCH 35: loss - 43.56989669799805 EPOCH 36: loss - 43.48973083496094 EPOCH 37: loss - 43.35897445678711 . We stop the learning because the loss value shows no significant change. Of course, there are other good methods such as lr scheduler, but since it is a simple experiment, learn only this. . And then, create various non-handwriting data. . import matplotlib.pyplot as plt import numpy as np plt.figure(figsize=(10, 10)) with torch.no_grad(): model.eval() for idx, i in enumerate(np.arange(-1, 1, 0.01)): data = model.decoder(torch.Tensor([[i]]).to(dev)) data = data.view(28, 28) if idx == 100: break plt.subplot(10, 10, idx + 1) plt.imshow(data.cpu().numpy(), cmap=&#39;gray&#39;) plt.show() . Comment . The reason why autoencoders have great significance is because of the limitations of data labeling. It is one of the big issues to have to label each data used when training a neural network. However, the autoencoder is very attractive from the point of view of being able to learn and process representations on data from the point of view of a model (even labeling is irrelevant). These self-supervised learning, semi-supervised learning, and unsupervised learning are fields that are being continuously studied. Since it can be used for the overall model, these contents are not limited to a specific field, but should be looked at in all fields using neural networks. Representatively GAN, self-supervised learning, SimCLR, and autoencoder. .",
            "url": "https://ndo04343.github.io/blog/neural-network/auto-encoder/representation-learning/2022/02/24/introduction-of-autoencoder.html",
            "relUrl": "/neural-network/auto-encoder/representation-learning/2022/02/24/introduction-of-autoencoder.html",
            "date": " • Feb 24, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "[InformationTheory] CH02. Entropy Between Two Distributions",
            "content": "2.1. Cross Entropy . Definition.2.1. Cross Entropy . For probability mess functions $p$ and $q$, the cross entropy $H[p, q]$ is $$ H[p, q] = mathbb{E}_{x sim p}[ log q(x)] = - sum_{i = 1}^N p(x_i) log_2 q(x_i) dx. $$ . Also, for probability density functions $p$ and $q$, the cross entropy $H[p, q]$ is $$ H[p, q] = mathbb{E}_{x sim p}[ log q(x)] = - int_x p(x) log_2 q(x) dx. $$ . Ex) $$ begin{matrix} p = [ frac{1}{3} , frac{1}{3} , frac{1}{3}] q_1 = [ frac{1}{3} , frac{1}{3} , frac{1}{3}] q_2 = [ frac{1}{6} , frac{2}{3} , frac{1}{6}] q_3 = [ frac{1}{12} , frac{5}{6} , frac{1}{12}] r_1 = [ frac{1}{6} , frac{1}{6} , frac{2}{3}] r_2 = [ frac{1}{12} , frac{1}{12} , frac{5}{6}] end{matrix} $$ . $$ begin{matrix} H[p, q_1] = 1.584962500721156 H[p, q_2] = 1.9182958340544896 H[p, q_3] = 2.477653135758702 H[p, r_1] = 1.9182958340544896 H[p, r_2] = 2.477653135758702 end{matrix} $$ Theorem.2.1. . If $p = q$, . $$ H[p, q] = H[p] $$ 2.2. Kullback-Leibler Divergence . Definition.2.2. Kullback-Leibler Divergence . For probability mess functions $p$ and $q$, the KL divergence $D_{KL}(p||q)$ is $$ D_{KL}(p||q) = mathbb{E}_{x sim p} left[ frac{ log p(x)}{ log q(x)} right] = H[p, q] - H[p] = sum_{i = 1}^N p(x_i) log_2 frac{p(x_i)}{q(x_i)}. $$ . For probability density functions $p$ and $q$, the KL divergence $D_{KL}(p||q)$ is $$ D_{KL}(p||q) = mathbb{E}_{x sim p} left[ frac{ log p(x)}{ log q(x)} right] = H[p, q] - H[p] = int_x p(x) log_2 frac{p(x)}{q(x)}. $$ . Ex) $$ begin{matrix} p = [ frac{1}{3} , frac{1}{3} , frac{1}{3}] q_1 = [ frac{1}{3} , frac{1}{3} , frac{1}{3}] q_2 = [ frac{1}{6} , frac{2}{3} , frac{1}{6}] q_3 = [ frac{1}{12} , frac{5}{6} , frac{1}{12}] r_1 = [ frac{1}{6} , frac{1}{6} , frac{2}{3}] r_2 = [ frac{1}{12} , frac{1}{12} , frac{5}{6}] end{matrix} $$ . $$ begin{matrix} D_{KL}(p||q_1) = 0.0 D_{KL}(p||q_2) = 0.3333333333333333 D_{KL}(p||q_3) = 0.8926906350375459 D_{KL}(p||r_1) = 0.3333333333333333 D_{KL}(p||r_2) = 0.8926906350375459 end{matrix} $$ Theorem.2.2. . If $p = q$, . $$ D_{KL}(p||q) = H[p, q] - H[p] = 0 $$ Since both operation doesn&#39;t allow commutative property, remark that cross entropy and kl divergence can&#39;t be used to distance metric. .",
            "url": "https://ndo04343.github.io/blog/information-theory/2022/02/24/ch02-entropy-between-two-distributions.html",
            "relUrl": "/information-theory/2022/02/24/ch02-entropy-between-two-distributions.html",
            "date": " • Feb 24, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "[InformationTheory] CH01. Entropy",
            "content": "1.0. Overview . Information theory was developed to study the problem of sending messages by sending discrete alphabets over noised channel like wireless communication. In machine learning, its context lies in the application of information theory to continuous variables. _TM Cover, 2006, MacKay, 2003__ . The following are the core intuitions of information theory. . Learning an event that is less likely to occur is more informative than learning an event that is more likely to occur. . For example, &quot;there was a solar eclipse this morning&quot; than &quot;the sun rose this morning&quot; means that you have more information. That is, to quantify the amount of information, the following properties must be satisfied. . Events with a high probability of occurrence should have less information. | There is no information about an event that must occur. | Events with a low probability of occurrence should have more information. | The amount of information for individual events should be additive. Ex) In coin toss, $I([H, H]) = 2I([H])$ | . | . Definition.1.1. Self-information of Event x=$x$ . $$ I_p(x) = - log P(x). $$ Since $ log$ is a natural logarithm, the unit of $I(x)$ above is nat. nat is the amount of information obtained by observing an event with probability $ frac{1}{e}$. If $ log_2$ is used instead of the natural logarithm, the unit is called bit or shannon, which means the amount of information obtained by observing an event with probability $ frac{1}{2}$. . Consider the binomial distribution of the probability that a coin is tossed 3 times and heads are x. The expression is: . $$ p(x) = {3 choose x}p^x (1 - p)^{3 - x} quad text{for} , x=0,1,2,3. $$In the above equation, for each event $x=0, x=1, x=2, x=3$, the probability value is as follows. . $$ begin{matrix} p(x=0) = 0.125 p(x=1) = 0.375 p(x=2) = 0.375 p(x=3) = 0.125 end{matrix} $$ The self-information for it is as follows. . $$ begin{matrix} I_p(x=0) = 3 I_p(x=1) = 1.415037 cdots I_p(x=2) = 1.415037 cdots I_p(x=3) = 3 end{matrix} $$ The information amount value is a structure that receives the probability value as $ log$, so it is not negative and has a relatively large value at a low probability value. . 1.1. Shannon Entropy . The above case of self-information deals with only one event. The uncertainty of the entire probability distribution can be quantified with the Shannon entropy. . Definition.1.2. Shannon Entropy . $$ H[p] = mathbb{E}_{x sim p}[I(x)] = - mathbb{E}_{x sim p}[ log p(x)]. $$ This is the average amount of information for the events in the distribution. This value tells the lower bound of the average number of bits required to encode information drawn from the distribution $p$. In the example above, the Shannon entropy value is calculated as follows. . $$ H[p] = frac{1}{4} (0.125 cdot 3 + 0.375 cdot 1.415037 + 0.375 cdot 1.415037 + 0.125 cdot 3) = 0.4528194. $$ The value of Shannon&#39;s entropy is low in the deterministic case, and the closer it is to an even distribution(i.e., like uniform distribution), the higher the entropy. In particular, when $x$ is a continuous variable, the Shannon entropy is called differential entropy. . . Application.1.1. Maximization of Shannon Entropy . Let probability density function $p$ be for $x in [a, b]$. Then, . $$ begin{matrix} int_{a}^{b} p(x)dx = 1 end{matrix} $$ Consider following problem . $$ max_{p(x)} H[p(x)] = max_{p(x)} - mathbb{E}[ log p(x)] = max_{p(x)} - int_a^b p(x) log p(x) dx $$ Above problem is equality constraind optimization problem. Therefore we have to get lagrangian function $ mathcal{L}$. . $$ mathcal{L} = - int_a^b p(x) log p(x) dx + lambda_1 left( int_a^b p(x) dx - 1 right) $$ Above $ mathcal{L}$ is functional, then . $$ frac{ delta mathcal{L}}{ delta p(x)} = - log p(x) - 1 + lambda_1 = 0 $$$$ therefore , p(x) = exp(-1 + lambda_1) = c , text{for some constant} , c. $$ By equality constraint, $$ int_a^b c dx = c(b - a) = 1. $$ Then . $$ therefore , c = frac{1}{b - a} = p(x) sim U(a, b) $$ In above result, we can know that the uniform distribution is maximized shannon entropy distribution. . . Application.1.2. Maximization of Shannon Entropy with Fixed Variation . Let the expectation of probability density function $p$ be $ mu$ and variation be $ sigma^2$. Then, . $$ begin{matrix} int_{- infty}^{ infty} p(x)dx = 1 int_{- infty}^{ infty} xp(x)dx = mu int_{- infty}^{ infty} (x - mu)^2p(x)dx = sigma^2 end{matrix} $$ For maximization of $H[p]$, we have to use lagrange multiplier. Then, . $$ max mathcal{L}[p]( mathbf{ lambda}) = max - int_{- infty}^{ infty} p(x) log p(x) dx + mathbf{ lambda}^T begin{bmatrix} int_{- infty}^{ infty} p(x)dx - 1 int_{- infty}^{ infty} xp(x)dx - mu int_{- infty}^{ infty} (x- mu)^2 p(x)dx - sigma^2 end{bmatrix} $$ Then, . $$ frac{ delta mathcal{L}}{ delta p(x)} = - log p(x) - 1 + lambda_1 + lambda_2x + lambda_3(x- mu)^2 = 0 $$$$ therefore , p(x) = exp(-1 + lambda_1 + lambda_2x + lambda_3(x- mu)^2) $$ By first equality constraint, . $$ begin{matrix} int_{- infty}^{ infty} p(x) dx = int_{- infty}^{ infty} exp(-1 + lambda_1 + lambda_2x + lambda_3(x - mu)^2) dx = int_{- infty}^{ infty} exp( lambda_3x^2 + ( lambda_2 - 2 mu)x + mu^2 lambda_3 + lambda_1 - 1) dx quad ( lambda_3 &lt; 0) end{matrix} $$ $$ begin{matrix} - | lambda_3|(x^2 + frac{ lambda_2 - 2 mu}{| lambda_3|}x + frac{( lambda_2 - 2 mu)^2}{4 lambda_3^2} ) - frac{( lambda_2 - 2 mu)^2}{4| lambda_3|} + mu^2 lambda_3 + lambda_1 - 1 = - | lambda_3|( x + frac{ lambda_2 - 2 mu}{2 lambda_3} )^2 - frac{( lambda_2 - 2 mu)^2}{4| lambda_3|} + mu^2 lambda_3 + lambda_1 - 1 end{matrix} $$ $$ begin{matrix} int_{- infty}^{ infty} exp( lambda_3x^2 + ( lambda_2 - 2 mu)x + mu^2 lambda_3 + lambda_1 - 1) dx = int_{- infty}^{ infty} exp(- | lambda_3|( x + frac{ lambda_2 - 2 mu}{2 lambda_3} )^2 - frac{( lambda_2 - 2 mu)^2}{4| lambda_3|} + mu^2 lambda_3 + lambda_1 - 1) dx end{matrix} $$ Let $ sqrt{| lambda_3|}(x + frac{ lambda_2}{2 lambda_3}) = t$. Then . $$ begin{matrix} int_{- infty}^{ infty} exp(- | lambda_3|( x + frac{ lambda_2 - 2 mu}{2 lambda_3} )^2 - frac{( lambda_2 - 2 mu)^2}{4| lambda_3|} + mu^2 lambda_3 + lambda_1 - 1) dx = frac{1}{ sqrt{| lambda_3|}} exp left { - frac{( lambda_2 - 2 mu)^2}{4| lambda_3|} + mu^2 lambda_3 + lambda_1 - 1 right } int_{- infty}^{ infty} exp(-t^2) dt end{matrix} $$ $$ therefore , frac{ sqrt{ pi}}{ sqrt{| lambda_3|}} exp left { - frac{( lambda_2 - 2 mu)^2}{4| lambda_3|} + mu^2 lambda_3 + lambda_1 - 1 right } = 1 qquad cdots , (1) $$ In similar way by second equality constraint, . $$ - frac{( lambda_2 - 2 mu) sqrt{ pi}}{2 lambda_3 sqrt{| lambda_3|}} exp left { - frac{( lambda_2 - 2 mu)^2}{4| lambda_3|} + mu^2 lambda_3 + lambda_1 - 1 right } = mu qquad cdots , (2) $$ In similar way by third equality constraint, . $$ frac{ sqrt{ pi}}{ sqrt{| lambda_3|}} left( frac{1}{2| lambda_3|} + frac{( lambda_2 - 2 mu + 2 lambda_3 mu)^2}{4 lambda_3^2} right) = sigma^2 qquad cdots , (3) $$ By $(1), , (2), , text{and} , (3)$, . $$ therefore , p(x) = frac{1}{ sqrt{2 pi sigma^2}} exp left { - frac{(x - mu)^2}{2 sigma^2} right } $$ .",
            "url": "https://ndo04343.github.io/blog/information-theory/2022/02/24/ch01-entropy.html",
            "relUrl": "/information-theory/2022/02/24/ch01-entropy.html",
            "date": " • Feb 24, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "[NDIR] 연구 중간 보고",
            "content": "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 .",
            "url": "https://ndo04343.github.io/blog/report/encrypted/2022/02/16/NDIR-midterm-report-220218.html",
            "relUrl": "/report/encrypted/2022/02/16/NDIR-midterm-report-220218.html",
            "date": " • Feb 16, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "[OptimizationTheory] CH05. Constrained Optimizations",
            "content": "5.1. Introduction . Many practical optimization problems have constraints like equality and inequality constraint. Consider following problem: . $$ mathbf{w}^* = underset{ mathbf{w}}{ mathrm{argmin}} || mathbf{y} - X mathbf{w} ||_2^2 quad s.t. quad || mathbf{w}||_2^2 le 1. $$Above problem is inequality constrained optimization problem, and luckly $MSE$ cost function is convex function. Therefore, we can obtain optimal solution analytically. However, if obtained optimal solution doesn&#39;t satisfy the inequality condition, we have to find another solution. We can define general constained optimization problem like following. . Definition.5.1. Constrained Optimization Problem . Following problem that find optimal solution $ mathbf{x}$ are called constrained optimization problem. . $$ begin{matrix} mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , f( mathbf{x}) quad s.t. quad g_i( mathbf{x}) le 0, , i=1, cdots,m, h_j( mathbf{x}) = 0, , j=1, cdots,k end{matrix} $$where $f: mathbb{R}^n rightarrow mathbb{R}$ is a objective function, $ mathbf{g} : mathbb{R}^n rightarrow mathbb{R}^m$ is a inequality constraint function, and $ mathbf{h}: mathbb{R}^n rightarrow mathbb{R}^k $ is a equality constraint function. . 5.2. Lagrange Multiplier . Definition.5.2. Lagrangian Function . In above constrained optimization problem, there is corresponding Lagrangian function . $$ begin{matrix} mathcal{L}( mathbf{x}, mathbf{ lambda}, mathbf{ mu}) &amp;= f( mathbf{x}) + sum_{i = 1}^{m} lambda_ig_i( mathbf{x}) + sum_{j = 1}^{k} mu_j h_j( mathbf{x}) &amp;= f( mathbf{x}) + mathbf{ lambda}^T mathbf{g}( mathbf{x}) + mathbf{ mu}^T mathbf{h}( mathbf{x}) end{matrix} $$ where $ lambda_i, mu_j$ for $i=1, cdots,m, , j = 1, cdots,k$ are dual variables. . Theorem.5.1. Lagrangian Multiplier . In equality constrained optimization problem, if $ mathbf{x}^* in mathbb{R}^n $ is a local minimum, then there exist $ mathbf{ mu}^* in mathbb{R}^k $ such that . $$ begin{cases} frac{ partial}{ partial mathbf{x}} mathcal{L}( mathbf{x}, mathbf{ mu}) = mathbf{0} frac{ partial}{ partial mathbf{ mu}} mathcal{L}( mathbf{x}, mathbf{ mu}) = mathbf{0} end{cases}. $$And system of equations contain $n + k$ equation. . . Consider following problem from algebra. . Ex) . 2차원 평면상의 원 $x^2 + y^2 = k$과 직선 $y = sqrt{3}x + 4 sqrt{3}$을 지날때, k의 최소를 구하시오. . sol) . $ text{Let} , mathbf{x} = begin{bmatrix} x y end{bmatrix} = begin{bmatrix} x_1 x_2 end{bmatrix} in mathbb{R}^2. $ $ text{Then, this problem can be convert following constrained optimization problem.} $ . $$ mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , mathbf{x}^T mathbf{x} quad s.t. quad g( mathbf{x}) = [ sqrt{3} , - 1] mathbf{x} + 4 sqrt{3} = 0. $$$ text{Then the Lagrangian function of above constrained optimization problem is} $ . $$ mathcal{L}( mathbf{x}, mu) = mathbf{x}^T mathbf{x} + mu ([ sqrt{3} , - 1] mathbf{x} + 4 sqrt{3}). $$ $$ frac{ partial}{ partial mathbf{x}} mathcal{L}( mathbf{x}, mu) = 2 mathbf{x} + mu [ sqrt{3} , -1]^T = mathbf{0} qquad cdots , (1) $$$$ frac{ partial}{ partial mu} mathcal{L}( mathbf{x}, mu) = [ sqrt{3} , -1] mathbf{x} + 4 sqrt{3} = 0 qquad cdots , (2) $$ $ text{By theorem.5.1., solution of above equation is a local minimum of optimization problem.}$ $ text{Also, above Lagrangian function is convex.} quad ( because , text{Property.3.3})$ $ text{By, theorem.3.1.,}$ . $$ therefore quad mathbf{x}^* = begin{bmatrix} - frac{ sqrt{3} mu}{2} frac{ mu}{2} end{bmatrix}, , min k = 12 $$ Of course, it is reasonable to use gradient based optimization when we optimize the Lagrangian function. Above solution is obtained just analytically, not numerical method. . 5.3. Karush-Kuhn-Tucker(KKT) Conditions . Consider following problem. . $$ min x_1^2 + x_2^2 quad s.t. quad x_1 + x_2 = 0 $$In above problem, the value of $ mu$ is zero. This means that the equality constraint doesn&#39;t dependent on optimization problem. In this situation, above constrained optimization problem has equivalence relation with unconstrained optimization; $ min x_1^2 + x_2^2 $. . Theorem.5.2. Karush-Kuhn-Tucker(KKT) Conditions . If $ mathbf{x}^* in mathbb{R}^n $ is a local minimum, then there exist $ mathbf{ lambda}^* in mathbb{R}^m $ and $ mathbf{ mu}^* in mathbb{R}^k $ such that . $ (1) , text{Stationarity} quad frac{ partial}{ partial mathbf{x}} mathcal{L}( mathbf{x}, mathbf{ lambda}, mathbf{ mu}) = mathbf{0} $ . $ (2) , text{Primal feasibility} quad ^ forall i, , g_i( mathbf{x}^*) le 0, , mathbf{h}( mathbf{x}^*) = mathbf{0} $ . $ (3) , text{Complementary slackness} quad ^ forall i, , lambda_i^* g_i( mathbf{x}^*) = 0 $ . $ (4) , text{Dual feasibility} quad ^ forall i, , lambda_i^* ge 0 $ . Remark that if optimal solution isn&#39;t dependent with inequality constraint, then it is same with unconstrained optimization problem, and if optimal solution is dependent with inequality constraint, then optimal solution is on the boundary line of inequatlity constraint. Consider following problems. . $$ min x_1^2 + x_2^2 quad s.t. quad x_1 + x_2 - 1 le 0 qquad cdots , (1) $$$$ min x_1^2 + x_2^2 quad s.t. quad x_1 + x_2 - 1 ge 0 qquad cdots , (2) $$ All constraint conditions are divided into those that affect the result and those that do not. In $(1)$, that is same with . $$ min x_1^2 + x_2^2. $$ . Furthermore, since optimal solution can be in boundary line of inequality constraint, the inequality constraint that affect the result can be converted equality constraint. Therefore, $(2)$ is same with . $$ min x_1^2 + x_2^2 quad s.t. quad x_1 + x_2 - 1 = 0 $$Remark followings. Stationarity condition is because of Lagrange multiplier. Primal feasibility condition is trivial. Complementary slackness condition is derived from Lagrangian multiplier. We can understand that . $$ frac{ partial }{ partial lambda_i} mathcal{L}( mathbf{x}, mathbf{ lambda}, mathbf{ mu}) = g_i( mathbf{x}) $$, and since it can be constraint that doesn&#39;t affect the result, $ lambda_i$ can be zero or not. Therefore, . $$ ^ forall i, , lambda_i^* g_i( mathbf{x}^*) = 0. $$Dual feasibility condition is a condition that guarantees that the KKT condition is the same problem as the inequality constrained optimization problem. . 5.4. Duality . Consider following optimization problem . $$ begin{matrix} mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , f( mathbf{x}) quad s.t. quad g_i( mathbf{x}) le 0, , i=1, cdots,m, h_j( mathbf{x}) = 0, , j=1, cdots,k end{matrix} $$where $f: mathbb{R}^n rightarrow mathbb{R}$ is a convex function, $ mathbf{g} : mathbb{R}^n rightarrow mathbb{R}^m$ is a convex function for each $g_i$, and $ mathbf{h}: mathbb{R}^n rightarrow mathbb{R}^k $ is an affine function. Above optimization problem is called convex. For convex problems. KKT conditions becomes necessary and also sufficient for global optimality. . From now on, we consider above problem with duality. Duality means that the primal problem of optimization problem can view the dual problem. In Lagrangian method, it is called Lagrangian dual problem. . Definition.5.2. Lagrangian Dual Function . $$ mathcal{D}( mathbf{ lambda}, mathbf{ mu}) = underset{ mathbf{x}}{ min} mathcal{L}( mathbf{x}, mathbf{ lambda}, mathbf{ mu}) = underset{ mathbf{x}}{ min} left { f( mathbf{x} + mathbf{ lambda}^T mathbf{h}( mathbf{x}) + mathbf{ mu}^T mathbf{g}( mathbf{x}) right } $$ Theorem.5.3. Lagrange Dual Problem . The problem $$ begin{matrix} mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , f( mathbf{x}) quad s.t. quad g_i( mathbf{x}) le 0, , i=1, cdots,m, h_j( mathbf{x}) = 0, , j=1, cdots,k end{matrix} $$ where $f: mathbb{R}^n rightarrow mathbb{R}$ is a convex function, $ mathbf{g} : mathbb{R}^n rightarrow mathbb{R}^m$ is a convex function for each $g_i$, and $ mathbf{h}: mathbb{R}^n rightarrow mathbb{R}^k $ is an affine function, is equivalent with . $$ underset{ mathbf{ lambda}, mathbf{ mu}}{ max} mathcal{D}( mathbf{ lambda}, mathbf{ mu}) quad text{for} , lambda_i ge 0, , i=1, cdots,m. $$ . In above problem, $ mathcal{D}( mathbf{ lambda}, mathbf{ mu}) le mathcal{L}( mathbf{x}^*, mathbf{ lambda}, mathbf{ mu}) le f( mathbf{x}^*) $ where $ mathbf{x}^*$ is primal optimal. Therefore, $ mathcal{D}( mathbf{ lambda}, mathbf{ mu}) le underset{ lambda_i ge 0, mathbf{ mu}, , text{for} , i=1, cdots,m}{ max} mathcal{D}( mathbf{ lambda}, mathbf{ mu}) le f( mathbf{x}^*)$. $f( mathbf{x}^*)$ is called primal optimal, and $ underset{ lambda_i ge 0, mathbf{ mu}, , text{for} , i=1, cdots,m}{ max} mathcal{D}( mathbf{ lambda}, mathbf{ mu})$ is called dual optimal. . Definition.5.3. Duality . Let $p^*$ be primal optimal and $d^*$ be dual optimal of a dual problem. If $p^* ge d^*$, it is called weak duality, and if $p^* = d^*$, it is called strong duality. And $p^* - d^*$ is called duality gap. . Remark) . Dual function is a concave function(proved by definition of concave). | For a convex optimization problem, the strong duality usually holds (not always, i.e., When Slater’s condition is not satisfied.) | . . Ex) . $$ mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} mathbf{x}^T mathbf{x} quad s.t. quad A mathbf{x} = mathbf{b} $$",
            "url": "https://ndo04343.github.io/blog/optimization-theory/2022/02/11/ch05-constrained-optimizations.html",
            "relUrl": "/optimization-theory/2022/02/11/ch05-constrained-optimizations.html",
            "date": " • Feb 11, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "[NDIR] Identification of MIR-Flickr Near-Duplicate Images",
            "content": "1. Overview . They give a new test set based on a large, serendipitously selected collection(mir-flickr 1M) of high quality images. It disclose a set of 1,958 near-duplicate clusters from within the set. The main contribution of this is the identification of these images, which may then be used by other authors to make comparisons as they see fit. . 2. Problem . The problems of NDIR tasks are, . In near-duplicate image retrieval, it is still(2015) uncommon for methods to be objectively compared with each other, because of a lack of any good framework in which to do so. | Published sets of near-duplicate images exists, but are typically small, specialist, or generated. | How can we quantitative comparison of different similarity functions for the detection of near-duplicate images. | . To measure recall and precision for a given similarity function, . It requires very large sets of benchmark images which should be no bias as to the type of images in the collection, nor the method with which the near-duplicates have been formed. | It requires a known ground truth of near-duplicates. | . 3. Related Works . (Kim et al., 2010) : Near duplicate keyframes detecting, used TRECVID dataset. | (Chm et al., 2007; Jinda-Apiraksa et al., 2013) : Duplicate or near-duplicate of a query image detection. . &quot;We do not have access to ground-truth data for our experiments, since we are not aware of any large public corpus in which near duplicate images have been annotated.&quot; . &quot;Although the target application of this dataset is image retrieval, it was selected due to the lack of other appropriate datasets&quot; . | (Jegou et al., 2008) :Near-duplicate imate detection with using INRIA Holidays Dataset, there is no information about duplicate or near-duplicate images.4. (Nister and Stewenius, 2006) : Near-duplicate image detection with using composed of 10,200 images in sets of 4 images of one object/scene, there is no information about how similar two sets might be and whether or not they should be considered duplicate or near-duplicate. | (Jinda-Apiraksa et al., 2013) : Near-duplicate image detection, in this publication, the authors give a dataset specifically built for near-duplicate image detection. | 4. Contributions . 4.1. Basic Definitions . Definition(ground truth) of kinds of near duplicates are followings: . Duplicate : same pair | Identical near-duplicate(IND) : derived from the same digital source after applying some transformations transformations : any operation which has been performed using a standard image editor, with the intent of making cosmetic changes. | . | Non-identical near-duplicate(NIND) : share the same scenes and objects | . For the puposes of benchmarking, they choose to primarily use the IND definition for thw following reasons: . (almost) objective | such pairs are relatively common in the MIR Flickr set | the resulting relation is an equivalence relation | For some reason pairs of images were classified in three categories: . IND, as defined above | pairs of images which are strikingly visually similar, but are not IND as defined, also not NIND | Image(filename=&quot;figure2-1.png&quot;, width=224, height=224) . Image(filename=&quot;figure2-2.png&quot;, width=224, height=224) . Above pictures are strikingly similar, but not near-duplicate. . pairs which do not meet either criteria | 4.2. Methodology . Used characterisations : . Eh(MPEG-7 Edge Histograms) | Ht(MPEG-7 Heterogeneous Textures) | Cs(MPEG-7 Colour Structures) | pHash(Perceptual Hashing) | Used distance metrics : . Man($L_1$ distance) | Euc(euclidean distance) | Cos(cosine distance) | Sed(structural entropic distance) | Ham(hamming distance over bitmaps) | They recommend cosine distance as proper distance metric. . 4.2. Cluster Identification . Remove perfect duplicate images(in this procedure, 378 images removed) For each characterisations For each similarity function Do Threshold-limited Nearest-Neighbour Search(10^12 times comparison, use Chavez et al., 2001; Zezula et al., 2006) Each of the resulting image pairs was inspected by them . At point of publication, this has resulted, . 1,958 near-duplicate clusters within the set, containing a total of 4,071 images(the mean size of a cluster is 2.08) | 543 pairs of strikingly similar | . 5. Semantic Comparison . They already of course have results for the functions used to construct the set. Each x-axis of each graphs are threshold. Sensitivity and PPV mean just recall and precision. Here are brief: . $$ begin{matrix} text{Precision} = frac{ text{TP}}{ text{TP} + text{FP}} text{Recall} = frac{ text{TP}}{ text{TP} + text{FN}} end{matrix} $$ Image(filename=&quot;figure3.png&quot;, width=350) . 6. Conclusions . Using a number of different near-duplicate finders, they have found around 2,000 pairs of images conforming to an objective definition of near-duplicate, almost all the pairs that exist within the collection. Also, they said . The exhaustive search for near-duplicates within the set will of course never be finished:any updates will be gratefully received by the authors, and communicated onwards through our website. If we research on NDIR with MFND dataset, we must contact them. . 7. Comments . How can we benchmark ndir algorithm with this dataset? Is there a example? | .",
            "url": "https://ndo04343.github.io/blog/near-duplicate%20image%20detection/paper-review/2022/02/11/Identification-of-MIR-Flickr-Near-Duplicate-Images.html",
            "relUrl": "/near-duplicate%20image%20detection/paper-review/2022/02/11/Identification-of-MIR-Flickr-Near-Duplicate-Images.html",
            "date": " • Feb 11, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "[OptimizationTheory] CH04. Gradient based Optimizations",
            "content": "4.1. Introduction . Gradient descent is an iterative first-order optimisation algorithm used to find a local minimum of a given function. . 4.2. Gradient Descent . Algorithm.4.1. Gradient Descent . Gradient descent is an iterative method to find a stationary point of an unconstraint optimization problem : $$ theta^* = underset{ mathbf{ theta}} { arg min} L ( mathbf{ theta}) $$ . $$ L( mathbf{ theta} + eta mathbf{d}) approx L( mathbf{ theta}) + eta bigtriangledown _ mathbf{ theta} ^ T L( mathbf{ theta} ) mathbf{d} quad where quad eta &gt; 0, , left | mathbf{d} right | = 1 $$ $$ L( mathbf{ theta} + eta mathbf{d}) - L( mathbf{ theta}) approx eta bigtriangledown _ mathbf{ theta} ^ T L( mathbf{ theta} ) mathbf{d} = eta cos{( phi)} left | bigtriangledown _ mathbf{ theta} ^ T L( mathbf{ theta} ) right | $$ Find the directional vector $ mathbf{d}$ that minimizes $ L( mathbf{ theta} + eta mathbf{d} ) - L( mathbf{ theta}) le 0 $ . $$ cos{( phi)} = -1 , rightarrow , mathbf{d} = - frac{ bigtriangledown_ mathbf{ theta} L( mathbf{ theta} ) }{ left | bigtriangledown _ mathbf{ theta} L( mathbf{ theta} ) right | } $$ $$ therefore mathbf{ theta} + eta mathbf{d} = mathbf{ theta} - eta frac{ bigtriangledown_ mathbf{ theta} L( mathbf{ theta} ) }{ left | bigtriangledown _ mathbf{ theta} L( mathbf{ theta} ) right | } = mathbf{ theta} - alpha bigtriangledown_ mathbf{ theta} L( mathbf{ theta} ) $$ 4.3. 4 Types of Gradient Descent . $ (i) , text{Standard (or steepest) Gradient Descent} $ $$ mathbf{w} leftarrow mathbf{w} - eta bigtriangledown mathbb{E}[J( mathbf{w})] $$ . Practically infeasible | Thus, we need distribution about data $ mathbf{x}$ (Contradiction) | So, We can use sample mean | . $ (ii) , text{Stochastic(online) Gradient Descent} $ $$ mathbf{w} leftarrow mathbf{w} - eta bigtriangledown J_i( mathbf{w}) $$ . Simple to implement | Effective for large-scale problem | Much less memory | Unstable(zigzaging) | Purpose : We just consider one of data | It can be convergent. But there is little unstable. | . $ (iii) , text{Batch gradient Descent} $ $$ mathbf{w} leftarrow eta bigtriangledown sum_{i=1}^{N} J_i ( mathbf{w}) $$ . Accurate estimation of gradients | Parallelization of learning | Large memory | Big time-complexity can be problem in this method.(So slow) | But, there isn&#39;t problem in convergence. | Purpose : We consider all of data! | . $ (vi) , text{Mini-Batch Gradient Descent} $ $$ mathbf{w} leftarrow mathbf{w} - eta bigtriangledown sum_{i in mathfrak{I}}^{N} J_i ( mathbf{w}), quad 1 le left | mathfrak{I} right | le N $$ . Most generalized version | Effective to deal with large | Amount of training data | Purpose : We just consider seveal datas. | . 4.4. Newton&#39;s Method . Newton&#39;s method is zero finding algorithm. Many equations can be solved by this algorithm and bisection search algorithm in numerical analysis. We use this method too because of gradient necessary condition, which is $ nabla L = mathbf{0}$. . Algorithm.4.2. Newton-Rapson Method in Multivariate Function . In gradient updating context, we can find hyperplane of $L$ at $ mathbf{w}_0$ . $$ mathbf{y} = nabla^2 L( mathbf{w})^T ( mathbf{w} - mathbf{w}_0) + nabla L( mathbf{w}_0) $$ And we have to find next $ mathbf{w}$ by obtaining solution of following eqation: . $$ nabla^2 L( mathbf{w})^T ( mathbf{w} - mathbf{w}_0) + nabla L( mathbf{w}_0) = mathbf{0} $$ Therefore, . $$ mathbf{w}_1 = mathbf{w}_0 - H( mathbf{w}_0)^{-1} nabla L( mathbf{w}_0) $$ Actually, we can consider too polynomial approximation like Taylor series expansion. The result is surprising. . $$ L( mathbf{w} + Delta mathbf{w}) approx L( mathbf{w}) + nabla L ( mathbf{w})^T Delta mathbf{w} + frac{1}{2} Delta mathbf{w}^T H( mathbf{w}) Delta mathbf{w} $$ $$ frac{ partial}{ partial Delta mathbf{w}} L( mathbf{w} + Delta mathbf{w}) approx nabla L( mathbf{w}) + H( mathbf{w}) Delta mathbf{w} = mathbf{0} $$ $$ therefore , Delta mathbf{w} = H( mathbf{w})^{-1} nabla L( mathbf{w}) $$ The above result is the same as the result of the Newton Method. . 4.4. Quasi-Newton Method . The inverse of the Hessian matrix appearing in Newton&#39;s method is difficult to use because of its too much computation. By replacing this with an average gradient, the amount of computation can be reduced. Explore the BFGS method. . 4.5. Update Rule with Momentum . We can add a momentum term to the update equation to prevent slowing down of learning or reduce instability of learning. Basic update rule is following: . $$ mathbf{w}_{k + 1} = mathbf{w}_k - eta nabla_ mathbf{w} L( mathbf{w}_k) + gamma mathbf{w}_{k - 1} , text{for} , k ge 2. $$There are various variants of the gradient update algorithm using momentum. . Algorithm.4.4. Nesterov Accelerated Gradient(NAG) . When using Momentum, the direction of the gradient is also slightly shifted in the previous direction. . $$ mathbf{w}_{k + 1} = mathbf{w}_k - eta nabla_ mathbf{w} L( mathbf{w}_k + gamma mathbf{w}_{k - 1}) + gamma mathbf{w}_{k - 1} , text{for} , k ge 2. $$ 4.6. Update Rule with Adaptive Leaning Rate . If the learning rate is too small, the learning time is too long, and if the learning rate is too large, it diverges(zigzagging) and learning is not performed properly. AdaGrad solves this problem through learning rate decay. However, this also has a problem (zero convergence problem), so the following methods are used. . Algorithm.4.5. Adaptive Gradient(AdaGrad) . $$ mathbf{w}_{k + 1} = mathbf{w}_k - frac{ eta}{ sqrt{ epsilon + mathbf{d}_k}} odot nabla_ mathbf{w} L( mathbf{w}_k), , mathbf{d}_k = mathbf{d}_{k - 1} + nabla_ mathbf{w} L( mathbf{w}_k) odot nabla_ mathbf{w} L( mathbf{w}_k) $$ The above algorithm has a fatal flaw. Since $d$ is infinitely increasing, the amount of change in the gradient will converge to zero. . Algorithm.4.6. Root Mean Square Propagation(RMSProp) . $$ mathbf{w}_{k + 1} = mathbf{w}_k - frac{ eta}{ sqrt{ epsilon + mathbf{d}_k}} odot nabla_ mathbf{w} L( mathbf{w}_k), , mathbf{d}_k = gamma mathbf{d}_{k - 1} + (1 - gamma) nabla_ mathbf{w} L( mathbf{w}_k) odot nabla_ mathbf{w} L( mathbf{w}_k) $$ Algorithm.4.7. Adaptive Delta(AdaDelta) . $$ mathbf{w}_{k + 1} = mathbf{w}_k - frac{ sqrt{ epsilon + mathbf{u}_k}}{ sqrt{ epsilon + mathbf{d}_k}} odot nabla_ mathbf{w} L( mathbf{w}_k), , mathbf{d}_k = gamma mathbf{d}_{k - 1} + (1 - gamma) nabla_ mathbf{w} L( mathbf{w}_k) odot nabla_ mathbf{w} L( mathbf{w}_k), , mathbf{u}_k = gamma mathbf{u}_{k - 1} - (1 - gamma) frac{ sqrt{ epsilon + mathbf{u}_{k-1}}}{ sqrt{ epsilon + mathbf{d}_{k-1}}} odot nabla_ mathbf{w} L( mathbf{w}_{k-1}) , text{for} , k ge 2. $$ 4.6. Hybrid Update Rule with Momentum and Adaptive Learning Rate . Adaptive Moment Estimation(Adam) : Momentum + RMSProp | Nesterov-accelerated Adaptive Moment Estimation(NAdam) : NAG + Adam | . 4.7. Laerning Rate Scheduler . In implementations of neural network, the optimizer is important, but the learning rate scheduler is also important. In pytorch implementation, the followings are a commonly used learning rate scheduler. . Constant Learning Rate | LambdaLR | scheduler = LambdaLR(optimizer, lr_lambda = lambda epoch: 0.95 ** epoch) . def func(epoch): if epoch &lt; 40: return 0.5 elif epoch &lt; 70: return 0.5 ** 2 elif epoch &lt; 90: return 0.5 ** 3 else: return 0.5 ** 4 scheduler = LambdaLR(optimizer, lr_lambda = func . StepLR | scheduler = StepLR(optimizer, step_size=200, gamma=0.5) . MultiStepLR | scheduler = MultiStepLR(optimizer, milestones=[200, 350], gamma=0.5) . ExponentialLR | scheduler = ExponentialLR(optimizer, gamma=0.95) . CosineAnnealingLR | scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=0.001) . CosineAnnealingWarmRestarts | scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2, eta_min=0.001) . Custom CosineAnnealingWarmRestarts | import math from torch.optim.lr_scheduler import _LRScheduler class CosineAnnealingWarmUpRestarts(_LRScheduler): def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1): if T_0 &lt;= 0 or not isinstance(T_0, int): raise ValueError(&quot;Expected positive integer T_0, but got {}&quot;.format(T_0)) if T_mult &lt; 1 or not isinstance(T_mult, int): raise ValueError(&quot;Expected integer T_mult &gt;= 1, but got {}&quot;.format(T_mult)) if T_up &lt; 0 or not isinstance(T_up, int): raise ValueError(&quot;Expected positive integer T_up, but got {}&quot;.format(T_up)) self.T_0 = T_0 self.T_mult = T_mult self.base_eta_max = eta_max self.eta_max = eta_max self.T_up = T_up self.T_i = T_0 self.gamma = gamma self.cycle = 0 self.T_cur = last_epoch super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch) def get_lr(self): if self.T_cur == -1: return self.base_lrs elif self.T_cur &lt; self.T_up: return [(self.eta_max - base_lr)*self.T_cur / self.T_up + base_lr for base_lr in self.base_lrs] else: return [base_lr + (self.eta_max - base_lr) * (1 + math.cos(math.pi * (self.T_cur-self.T_up) / (self.T_i - self.T_up))) / 2 for base_lr in self.base_lrs] def step(self, epoch=None): if epoch is None: epoch = self.last_epoch + 1 self.T_cur = self.T_cur + 1 if self.T_cur &gt;= self.T_i: self.cycle += 1 self.T_cur = self.T_cur - self.T_i self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up else: if epoch &gt;= self.T_0: if self.T_mult == 1: self.T_cur = epoch % self.T_0 self.cycle = epoch // self.T_0 else: n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult)) self.cycle = n self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1) self.T_i = self.T_0 * self.T_mult ** (n) else: self.T_i = self.T_0 self.T_cur = epoch self.eta_max = self.base_eta_max * (self.gamma**self.cycle) self.last_epoch = math.floor(epoch) for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()): param_group[&#39;lr&#39;] = lr optimizer = optim.Adam(model.parameters(), lr = 0) scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0=150, T_mult=1, eta_max=0.1, T_up=10, gamma=0.5) .",
            "url": "https://ndo04343.github.io/blog/optimization-theory/2022/02/03/ch04-gradient-based-optimizations.html",
            "relUrl": "/optimization-theory/2022/02/03/ch04-gradient-based-optimizations.html",
            "date": " • Feb 3, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "[Calculus] CH03. Taylor Expansion",
            "content": "3.1. Mutivariate Taylor Polynomial . Definition.3.1. First Order Taylor Polynomial(Linear Approximation) . Let $f: mathbb{R}^n rightarrow mathbb{R}$ be a differentiable function. Then first order taylor polynomial of $f$ at $ mathbf{x}_0$ is . $$ P_1( mathbf{ mathbf{x}}) = f( mathbf{x}_0) + ( mathbf{x} - mathbf{x}_0)^T nabla_ mathbf{x} f( mathbf{x}_0). $$ Definition.3.2. Second Order Taylor Polynomial . Let $f: mathbb{R}^n rightarrow mathbb{R}$ be a twice differentiable function. Then second order taylor polynomial of $f$ at $ mathbf{x}_0$ is . $$ P_2( mathbf{ mathbf{x}}) = f( mathbf{x}_0) + ( mathbf{x} - mathbf{x}_0)^T nabla_ mathbf{x} f( mathbf{x}_0) + frac{1}{2!} ( mathbf{x} - mathbf{x}_0)^T nabla_ mathbf{x}^2 f( mathbf{x}_0) ( mathbf{x} - mathbf{x}_0) $$ We don&#39;t need more than this. .",
            "url": "https://ndo04343.github.io/blog/calculus/2022/02/03/ch03-taylor-expansion.html",
            "relUrl": "/calculus/2022/02/03/ch03-taylor-expansion.html",
            "date": " • Feb 3, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "[OptimizationTheory] CH03. Convex Optimization",
            "content": "3.1. Introduction . Consider the objective function that is differentiable in optimization problem. If $ mathbf{w}^*$ is optimal, $ nabla_ mathbf{w} L$ must be zero vector. This condition is called gradient necessary condition. In optimization problem, we have to find global optimal. However, we only find local optimal everytime. Also, if we find the solution of $ nabla_ mathbf{w} L = mathbf{0}$, there is no guarantee that the solution is a global optimal. . Here is something to think about. . [Dauphin14] Y. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, Y. Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. . Above paper suggest that local minima problem is actually a very rare case that does not occur in a high dimensional space. First reason is gradient necessary condition. Every elements in gradient must be a zero at some $ mathbf{w}^*$. But this is very rare case. Also, in every direction, gradient must form a convex shape in a high-dimensional space. But, the probability of that happening is close to zero. . Intuitively, in high dimensions, the chance that all the directions around a critical point lead upward is exponentially small w.r.t. the number of dimensions, unless the critical point is the global minimum or stands at an error level close to it, i.e., it is unlikely one can find a way to go further down. . In this context, since both local minima and the global minima are same, it can be seen that a convex function can be good loss function. . 3.2. Convex Optimization . Definition.3.1. Convex Set . A set $S$ is said to be convex if . $$ mathbf{x}, mathbf{y} in S, , text{then} , t mathbf{x} + (1 - t) mathbf{y} in S, , text{for} , t in [0, 1] $$ Definition.3.2. Convex Function . A function $f: S rightarrow mathbb{R}$ is said to be convex if . $$ ^ forall mathbf{x}_1, mathbf{x}_2 in S, , f(t mathbf{x}_1 + (1 - t) mathbf{x}_2) le tf( mathbf{x}_1) + (1 - t)f( mathbf{x}_2) , text{for} , t in [0, 1] $$where $S$ is convex subset of a real vector space. . Theorem.3.1. . In convex function, some local minimum is a global minimum. . Proof. Trivial(proof by contradiction). . Theorem.3.2. . A twice differentiable function $f: mathbb{R}^n rightarrow mathbb{R}$ is a convex function if and only if $ nabla^2 f succeq 0$. . Proof. Suppose $f$ has a positive semidefinite hessian matrix. Then for some $ mathbf{x}_0, mathbf{x}_1$ in the domain, and $t in [0, 1]$, we have . $$ g(t) = f(t mathbf{x}_0 + (1- t) mathbf{x}_1) $$which have the first and second derivative . $$ begin{matrix} frac{dg}{dt} = ( mathbf{x}_0 - mathbf{x}_1)^T nabla_ mathbf{x} f(t mathbf{x}_0 + (1 - t) mathbf{x}_1) frac{d^2g}{dt^2} = ( mathbf{x}_0 - mathbf{x}_1)^T nabla_ mathbf{x}^2 f(t mathbf{x}_0 + (1 - t) mathbf{x}_1)( mathbf{x}_0 - mathbf{x}_1) end{matrix} $$ Since the hessian matrix of $f$ is positive semidefinite, $ frac{d^2g}{dt^2} ge 0$ for $t in [0, 1]$. Then we can get . $$ begin{matrix} g(0) ge g(t) + g^ prime(t)(-t) g(1) ge g(t) + g^ prime(t)(1 - t) end{matrix} quad ( because , text{Taylor&#39;s theorem}) $$ Then $$ g(t) le tg(1) + (1 - t)g(0) $$ . $$ therefore , ^ forall mathbf{x}_0, mathbf{x}_1 in D, , f(t mathbf{x}_0 + (1 - t) mathbf{x}_1) le tf( mathbf{x}_0) + (1 - t) mathbf{x}_1 quad blacksquare $$ Definition.3.3. Convex Optimization Problem . A convex optimization problem is an optimization problem in which the objective function is a convex function and the feasible set is a convex set. . 3.3. Properties . Followings are convex function: . Exponential function | Power function(in some case) | Logarithmic function | Affine function | Quadratic function | Mean square error | Max function | Norm function | Softmax function | . Theorem.3.3. . Let $f( mathbf{x}) = h(g( mathbf{x})) = h(g_1( mathbf{x}), cdots, g_k( mathbf{x}))$ where $g: mathbb{R}^n rightarrow mathbb{R}^k, , h : mathbb{R}^k rightarrow mathbb{R}, , text{and} , f : mathbb{R}^n rightarrow mathbb{R}$. Then . $f$ is convex if $h$ is convex and nondecreasing in each argument, g is convex. | $f$ is convex if $h$ is convex and nonincreasing in each argument, g is concave. | . Proof. Trivial(by chain rule and above theorems). .",
            "url": "https://ndo04343.github.io/blog/optimization-theory/2022/02/03/ch03-convex-optimization.html",
            "relUrl": "/optimization-theory/2022/02/03/ch03-convex-optimization.html",
            "date": " • Feb 3, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "[LinearAlgebra] CH03. Quadratic Form",
            "content": "3.1. Quadratic Form . Definition.3.1. . $n times n$ 대칭행렬 $A = [a_{ij}]$와 벡터 $ mathbf{x} = [x_1 , x_2 , cdots , x_n]^T in mathbb{R}^n$에 대해서 . $$ q( mathbf{x}) = mathbf{x}^T A mathbf{x} = sum_{i = 1}^{n} sum_{j = 1}^{n} a_{ij} x_i x_j $$일 때, $A$를 $q( mathbf{x})$의 계수행렬(coefficient matrix)이라고 하며 $q( mathbf{x})$를 계수행렬이 $A$인 이차형식(quadratic form)이라고 하고, $x_1, , x_2, , cdots , , x_n $을 $q( mathbf{x})$의 변수(variable)라고 하며 $ mathbf{x}$를 변수 벡터라고 한다. . Remark . $q( mathbf{x}) = mathbf{x}^ mathbf{x} A mathbf{x} = &lt; mathbf{x}, A mathbf{x} &gt; = sum_{i = 1}^n a_{ii}x_i^2 + 2 sum_{i neq j} a_{ij} x_i x_j$ | $n$개의 실변수를 가지는 이차방정식은 다음과 같다. | $$ begin{matrix} f( mathbf{x}) = sum_{i = 1}^n sum_{j = 1}^n a_{ij}x_ix_j + sum_{i = 1}^n b_ix_i + c = 0 quad text{or} f( mathbf{x}) = mathbf{x}^TA mathbf{x} + mathbf{b}^T mathbf{x} + c = 0 end{matrix} $$위의 식에서 $ mathbf{b}^T mathbf{x} = sum_{i = 1}^n b_ix_i$를 $f( mathbf{x})$의 일차형식(linear form)이라고 한다. . $A$가 대칭행렬로 규정된 이유는 다음과 같다. | 만약 $A$가 정방행렬이라면, $A = S + T quad text{where} , S , text{is a symetric matrix, and} , T , text{is a skew-symetric matrix.}$ $$ begin{matrix} mathbf{x}^TT mathbf{x} &amp;= ( mathbf{x}^TT mathbf{x})^T quad ( because , mathbf{x}^TT mathbf{x} in R) &amp;= mathbf{x}^TT^T mathbf{x} &amp;= - mathbf{x}^TT mathbf{x} end{matrix} $$ . $$ therefore quad q( mathbf{x}) = mathbf{x}^TA mathbf{x} = mathbf{x}^TS mathbf{x} $$ Theorem.3.1. . $n times n$ 대칭행렬 $A$에 대한 이차형식 $q( mathbf{x}) = mathbf{x}^T A mathbf{x}$는 적당한 직교행렬 $P$를 이용하여 변수벡터를 $ mathbf{x} = P mathbf{y}$로 변환하여, . $$ g( mathbf{y}) = lambda_1 y_1^2 + lambda_2 y_2^2 + cdots + lambda_n y_n^2 $$로 고칠 수 있다. 단, $ mathbf{y} = [y_1 , y_2 , cdots , y_n]^T in mathbb{R}^n$이고, $ lambda_1, , lambda_2, , cdots, , lambda_n$은 $q( mathbf{x})$의 계수행렬 $A$의 고유치이다. . Proof. Trivial. . Definition.3.2. Problem of Principal Axes and Standard Form . Theorem.3.1. 에서 이차형식 . $$ q( mathbf{x}) = mathbf{x}^T A mathbf{x} quad cdots (1) $$을 $ mathbf{x}$의 좌표 $(x_1, , x_2, , cdots, , x_n)$을 적당히 변환하여 . $$ g( mathbf{y}) = lambda_1 y_1^2 + lambda_2 y_2^2 + cdots + lambda_n y_n^2 quad cdots (2) $$로 고치는 문제는 응용상 매우 중요하다. 이 문제를 이차형식의 주축문제(problem of principal axes)라고 한다. 그리고 이차형식 $(2)$를 이차형식 $(1)$의 표준형(standard form)이라고 한다. . Theorem.3.2. . 계수행렬이 $n times n$ 대칭행렬 $A$인 이차형식 $f( mathbf{x}) = mathbf{x}^T A mathbf{x}$의 변수벡터 $ mathbf{x}$를 직교행렬 $P$로 변환하여 표준형으로 고쳤을 때, 그 계수들 중에서 양인 것의 개수 $p$와 음인 것의 개수 $q$는 $f( mathbf{x})$를 표준형으로 고치는 방법에 관계없이 일정하다. . Proof. Trivial. . Definition.3.3. Positive Definite Quadratic Form and Matrix . 계수행렬이 대칭행렬 $A$인 이차형식 $f( mathbf{x}) = mathbf{x}^T A mathbf{x}$를 표준형으로 고쳐서, 그 계수들 중에서 양인 것의 개수를 $p$, 음인 것의 개수를 $q$라고 할 때, $(p, q)$를 $f( mathbf{x})$ 또는 $A$의 부호수(number of sign)라고 하고, . $$ sgn(A) = (p, q) $$와 같이 나타낸다. 또, 모든 $ mathbf{x} neq mathbf{0}$에 대하여 $f( mathbf{x}) = mathbf{x}^TA mathbf{x} &gt; 0$일 때, $f( mathbf{x})$를 양정치이차형식(positive definite quadratic form)이라고 하고, $A$를 양정치행렬(positive definite matrix)이라고 한다. . Theorem.3.3. . $n times n$ 대칭행렬을 $A$의 고유치가 $ lambda_1, , lambda_2, , cdots, , lambda_n( lambda_1 ge lambda_2 ge cdots ge lambda_n)$이라고 하고, $ mathbf{x} in mathbb{R}^n$를 임의의 단위벡터($|| mathbf{x}|| = 1)$라고 하면 다음 사항들이 성립한다. . $ lambda_n le mathbf{x}^T A mathbf{x} le lambda_1$ | 단위벡터가 $ mathbf{x}$가 $ lambda_i$에 대응되는 $A$의 고유벡터이면 $ mathbf{x}^T A mathbf{x} = lambda_i$ | . Proof. Trivial. . Theorem.3.4. . $n$차 정방행렬에 대하여 다음은 동치이다. . $A$는 양정치행렬이다. | $A$의 모든 고유치가 양수이다. | $A = S^2$($S$는 양정치행렬)으로 나타난다. | $A = B^TB$($B$는 정칙행렬)으로 나타난다. | Proof. Trivial. . 3.2. Classification of Quadratic Lines and Plane . 3.1.에서 다룬 내용은 이차곡선의 분류 문제로 연결된다. 쓸 일이 없으니까 간단하게 종합정리만하면 다음과 같다. . 이차곡선 . $|A| &gt; 0$ 일 때, $| bar{A}| &gt; 0$ : 허타원(Imaginary ellipse) | $| bar{A}| &lt; 0$ : 타원(Ellipse) | $| bar{A}| = 0$ : 점타원 | | $|A| &lt; 0$ 일 때, $| bar{A}| neq 0$ : 쌍곡선(Hyperbola) | $| bar{A}| = 0$ : 서로 만나는 두 직선 | | $|A| = 0$ 일 때, $| bar{A}| neq 0$ : 포물선(Parabola) | $| bar{A}| = 0$ 일 때, $rank( bar{A}) = 2$ 일 때, $sgn( bar{A}) = (2, 0)$ : $ emptyset$ | $sgn( bar{A}) = (1, 1)$ : 나란한 두 직선 | | $rank( bar{A}) = 1$ : 일치하는 두 직선 | | | 이차곡면 . $|A| &gt; 0$ $| bar{A}| &gt; 0$ : 허타원면(Imaginary ellipsoid) | $| bar{A}| &lt; 0$ : 타원면(Ellipsoid) | $| bar{A}| = 0$ : 점타원면 | | $|A| &lt; 0$ $| bar{A}| &gt; 0$ : 일엽쌍곡선(Hyperboloid of one sheet) | $| bar{A}| &lt; 0$ : 이엽쌍곡선(Hyperboloid of two sheets) | | $|A| = 0$ 일 때, $| bar{A}| neq 0$ : 쌍곡포물면 | $| bar{A}| = 0$ 일 때, $rank(A) = 3$ : 이차추면 | $rank(A) = 2$ 일 때, $rank( bar{A}) = 4$ : 타원포물선 | $rank( bar{A}) = 3$ 일 때, $sgn(A) = (2, 0)$ : 타원주면 | $sgn(A) = (1, 1)$ : 쌍곡주면 | | $rank( bar{A}) = 2$ : 서로 만나는 두 평면 | | $rank(A) = 1$ 일 때, $rank( bar{A}) = 3$ : 포물주면 | $rank( bar{A}) = 2$ 일 때, $sgn(A) = (1, 1)$ : 나란한 두 평면 | $sgn(A) = (2, 0)$ : $ emptyset$ | | $rank( bar{A}) = 1$ : 하나의 평면 | | | |",
            "url": "https://ndo04343.github.io/blog/linear-algebra/2022/02/02/ch03-quadratic-form.html",
            "relUrl": "/linear-algebra/2022/02/02/ch03-quadratic-form.html",
            "date": " • Feb 2, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "[Calculus] CH02. Vector Calculus",
            "content": "2.0. Overview . For $f: mathbb{R}^n rightarrow mathbb{R}$, we know that $ nabla f : mathbb{R}^n rightarrow mathbb{R}^n$. For example, . $$ begin{matrix} nabla_ mathbf{w} ( mathbf{x}^T mathbf{w}) = mathbf{x} nabla_ mathbf{w} ( mathbf{w}^T R mathbf{w}) = (R + R^T) mathbf{w} end{matrix} $$In this time, we consider about derivative of vector functions, matrix functions, and second derivative of multi-variate functions. . 2.1. Gradient of Various Functions . Definition.2.1. Gradient of a Scalar Function with Respect to a Vector . Let the function $ f : mathbb{R}^m rightarrow mathbb{R} $ be. $$ frac{ partial f( mathbf{w})}{ partial mathbf{w}} = nabla_ mathbf{w} f( mathbf{w}) = begin{bmatrix} frac{ partial f( mathbf{w})}{ partial w_1} frac{ partial f( mathbf{w})}{ partial w_2} cdots frac{ partial f( mathbf{w})}{ partial w_m} end{bmatrix}_{m times 1}. $$ . Example.2.1. . $ nabla_ mathbf{w} ( mathbf{x}^T mathbf{w}) = mathbf{x}$ | $ nabla_ mathbf{w} ( mathbf{w}^T R mathbf{w}) = (R + R^T) mathbf{w}$ | . Definition.2.2. Gradient of a Vector Function with Respect to a Vector . Let the function $ mathbf{g} : mathbb{R}^m rightarrow mathbb{R}^n $ be. If $ mathbf{g}( mathbf{w}) = $ $ begin{bmatrix} g_1( mathbf{w}) g_2( mathbf{w}) vdots g_n( mathbf{w}) end{bmatrix} $ and $ w , text{be a} , m times 1 , text{vector}$, . $$ frac{ partial mathbf{g}( mathbf{w})}{ partial mathbf{w}} = nabla_ mathbf{w} mathbf{g}( mathbf{w}) = begin{bmatrix} nabla_ mathbf{w} g_1( mathbf{w})^T nabla_ mathbf{w} g_2( mathbf{w})^T vdots nabla_ mathbf{w} g_n( mathbf{w})^T end{bmatrix} = begin{bmatrix} frac{ partial g_1( mathbf{w})}{ partial w_1} &amp; frac{ partial g_1( mathbf{w})}{ partial w_2} &amp; cdots &amp; frac{ partial g_1( mathbf{w})}{ partial w_m} frac{ partial g_2( mathbf{w})}{ partial w_1} &amp; frac{ partial g_2( mathbf{w})}{ partial w_2} &amp; cdots &amp; frac{ partial g_2( mathbf{w})}{ partial w_m} vdots &amp; vdots &amp; ddots &amp; vdots frac{ partial g_n( mathbf{w})}{ partial w_1} &amp; frac{ partial g_n( mathbf{w})}{ partial w_2} &amp; cdots &amp; frac{ partial g_n( mathbf{w})}{ partial w_m} end{bmatrix}_{n times m}, , text{and it is called Jacobian.} $$ . Example.2.2. . $ nabla_ mathbf{w} (A mathbf{w}) = A$ | . Definition.2.3. Hessian Matrix of a Scalar Function with Respect to a Vector . Let the function $ f : mathbb{R}^m rightarrow mathbb{R} $ and $ mathbf{w} , text{be a} , m times 1 $ vector. . $$ H = frac{ partial}{ partial mathbf{w}} nabla_ mathbf{w}f( mathbf{w}) = nabla_ mathbf{w}^2 f( mathbf{w}) = begin{bmatrix} frac{ partial ^2f( mathbf{w})}{ partial w_1^2} &amp; frac{ partial ^2f( mathbf{w})}{ partial w_1 partial w_2} &amp; cdots &amp; frac{ partial ^2f( mathbf{w})}{ partial w_1 partial w_m} frac{ partial ^2f( mathbf{w})}{ partial w_2 partial w_1} &amp; frac{ partial ^2f( mathbf{w})}{ partial w_2^2} &amp; cdots &amp; frac{ partial ^2f( mathbf{w})}{ partial w_2 partial w_m} vdots &amp; vdots &amp; ddots &amp; vdots frac{ partial ^2f( mathbf{w})}{ partial w_m partial w_1} &amp; frac{ partial ^2f( mathbf{w})}{ partial w_m partial w_2} &amp; cdots &amp; frac{ partial ^2f( mathbf{w})}{ partial w_m^2} end{bmatrix}_{m times m} $$ Hassian matrix is symmetric. | . Definition.2.4. Gradient of a Scalar Function with Respect to a Matrix . Let the function $ f : mathbb{R}^{m times n} rightarrow mathbb{R} $ be a function. . $$ frac{ partial f(W)}{ partial W} = nabla_W f(W) = begin{bmatrix} frac{ partial f(W)}{ partial w_{11}} &amp; frac{ partial f(W)}{ partial w_{12}} &amp; cdots &amp; frac{ partial f(W)}{ partial w_{1n}} frac{ partial f(W)}{ partial w_{21}} &amp; frac{ partial f(W)}{ partial w_{22}} &amp; cdots &amp; frac{ partial f(W)}{ partial w_{2n}} vdots &amp; vdots &amp; ddots &amp; vdots frac{ partial f(W)}{ partial w_{m1}} &amp; frac{ partial f(W)}{ partial w_{m2}} &amp; cdots &amp; frac{ partial f(W)}{ partial w_{mn}} end{bmatrix}_{m times n} $$ Example.2.3. . $ nabla_ mathbf{W} (trace(XW)) = X^T$ | Let $W$ be a $n times m$ matrix and $R$ be a $n times n$ matrix. $$ nabla_W (W^TRW) = (R + R^T)W $$ | $ nabla_W ( log |W|) = (W^{-1})^T$ | .",
            "url": "https://ndo04343.github.io/blog/calculus/2022/02/02/ch02-vector-calculus.html",
            "relUrl": "/calculus/2022/02/02/ch02-vector-calculus.html",
            "date": " • Feb 2, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "[Calculus] CH01. Basic Calculus",
            "content": "1.0. Functions . We&#39;ll use numerator-layout notation. . Definition.1.1. A Function $f$ of $n$ Variables . A function $f$ of $n$ variables is a rule that assigns a number $z = f(x_1, x_2, cdots, x_n)$ to an $n$-tuple $(x_1, x_2, cdots, x_n)$ of real numbers. . $$ f : mathbb{R}^n rightarrow mathbb{R} $$ Definition.1.2. Limits of a Function of $n$ Variables . Let $f$ be a function of $n$ variables whose domain $D$ includes points arbitrarily close to $ mathbf{a}$. Then we say that the limit of $f( mathbf{x})$ as $ mathbf{x}$ approaches $ mathbf{a}$ is $L$ and we write . $$ lim_{ mathbf{x} rightarrow mathbf{a}} f( mathbf{x}) = L $$if for every number $ epsilon &gt; 0$ there is a corresponding number $ delta &gt; 0$ such that if $ mathbf{x} in D$ and $0 &lt; || mathbf{x} - mathbf{a}|| &lt; delta$ then $|f( mathbf{x}) - L| &lt; epsilon $. . Definition.1.3. Continuity . A function $f$ of two variables is called continuous at $ mathbf{a}$ if . $$ lim_{ mathbf{x} rightarrow mathbf{a}} f( mathbf{x}) = f( mathbf{a}). $$We say $f$ is continuous on $D$ if $f$ is continuous at every point $ mathbf{a}$ in $D$. . 1.1. Partial Derivatives and Gradients . Definition.1.4. Partial Derivative of a Function of $n$ Variables . If $f$ is a function of $n$ variables, its partial derivatives are . $$ frac{ partial}{ partial x_i} f( mathbf{x}) = lim_{h rightarrow 0} frac{f(x_1, cdots, x_i + h, cdots, x_n) - f(x_1, cdots, x_i, cdots, x_n)}{h} quad text{for} , i=1,2, cdots,n $$Also, a vector of partial derivatives as above is called gradient; . $$ nabla_ mathbf{x} f( mathbf{x}) = ( frac{ partial}{ partial x_1} f( mathbf{x}), cdots, frac{ partial}{ partial x_n} f( mathbf{x})) $$ Theorem.1.1. . Let $V = {f | f: mathbb{R}^n rightarrow mathbb{R} , text{and} , f , text{is differentiable} }, , W = {f | f: mathbb{R}^n rightarrow mathbb{R}^n }$, and $D: V rightarrow W, , D(f) = nabla f$. Then the differential transformation $D$ is a linear transformation and it is called differential operator $D$. . Proof. $$ text{For} , 0 in V, , D(0) = mathbf{0} in W. qquad cdots , (1) $$ . $$ begin{aligned} text{For} , f, g in V, , D(f + g) &amp;= nabla_ mathbf{x} (f + g) &amp;= nabla_ mathbf{x} f + nabla_ mathbf{x} g &amp;= D(f) + D(g) qquad cdots , (2) end{aligned} $$ $$ begin{aligned} text{For} , c in mathbb{R}, , f in V, , D(cf) &amp;= nabla_ mathbf{x} f &amp;= c nabla_ mathbf{x} f &amp;= c D(f) qquad cdots , (3) end{aligned} $$ $$ therefore , D , text{is a linear transformation} quad ( because , (1), , (2), , text{and} , (3)) quad blacksquare $$ Definition.1.5. Linear Approximation . The linear approximation of $f$ at $ mathbf{x}_0$ is given as . $$ f( mathbf{x}) approx f( mathbf{x}_0) + nabla_ mathbf{x} f( mathbf{x}_0)^T ( mathbf{x} - mathbf{x}_0). $$ Definition.1.6. Differentiable . If $y = f( mathbf{x})$, $f$ is differentiable at $ mathbf{x}_0$ if $ Delta u$ can be expressed in the form . $$ Delta y = nabla_ mathbf{x} f( mathbf{x}_0)^T begin{bmatrix} Delta x_1 Delta x_2 vdots Delta x_n end{bmatrix} + mathbf{ epsilon}^T begin{bmatrix} Delta x_1 Delta x_2 vdots Delta x_n end{bmatrix} , text{where} , mathbf{ epsilon} rightarrow mathbf{0} , text{as} , begin{bmatrix} Delta x_1 Delta x_2 vdots Delta x_n end{bmatrix} rightarrow mathbf{0}. $$ Theorem.1.2. . If the gradient $ nabla_ mathbf{x} f$ exist near $ mathbf{x}_0$ and are continuous at $ mathbf{x}_0$, then $f$ is differentiable at $ mathbf{x}_0$. . Proof. . Trivial. . Definition.1.7. Differentials . For a differentiable function $y = f( mathbf{x})$, let the differentials $dx_i$ for $i=1,2, cdots,n$ be independent variables. Then the total differential $dy$ is defined by . $$ dy = nabla_ mathbf{x} f^T begin{bmatrix} dx_1 dx_2 vdots dx_n end{bmatrix}. $$ 1.2. Properties of Partial Derivatives . Theorem.1.3. . Let $f: mathbb{R}^n rightarrow mathbb{R}, , g: mathbb{R}^n rightarrow mathbb{R},$ and $c in mathbf{R}$. . Constant Multiple Rule: $ nabla_ mathbf{x} f(c mathbf{x}) = c nabla_ mathbf{x} f( mathbf{x})$ | Sum Rule: $ nabla_ mathbf{x} { f( mathbf{x}) + g( mathbf{x}) } = nabla_ mathbf{x} f( mathbf{x}) + nabla_ mathbf{x} g( mathbf{x})$ | Product Rule: $ nabla_ mathbf{x} { f( mathbf{x})g( mathbf{x}) } = ( nabla_ mathbf{x} f( mathbf{x}))g( mathbf{x}) + f( mathbf{x}) ( nabla_ mathbf{x} g( mathbf{x}))$ | Quotient Rule: $ nabla_ mathbf{x} left { frac{f( mathbf{x})}{g( mathbf{x})} right } = frac{( nabla_ mathbf{x} f( mathbf{x}))g( mathbf{x}) - f( mathbf{x}) ( nabla_ mathbf{x} g( mathbf{x}))}{g( mathbf{x})^2}$ | Proof. Trivials. . Theorem.1.4. (Chain Rule) . If $y = f( mathbf{x})$ and $ mathbf{x} = mathbf{g}(t)$ are differentiable functions, then . $$ frac{dy}{dt} = nabla_ mathbf{x} f^T begin{bmatrix} frac{dx_1}{dt} frac{dx_2}{dt} vdots frac{dx_n}{dt} end{bmatrix} $$Proof. . $$ begin{matrix} Delta y = nabla_ mathbf{x} f( mathbf{x})^T begin{bmatrix} Delta x_1 Delta x_2 vdots Delta x_n end{bmatrix} + mathbf{ epsilon}^T begin{bmatrix} Delta x_1 Delta x_2 vdots Delta x_n end{bmatrix} frac{ Delta y}{ Delta t} = nabla_ mathbf{x} f( mathbf{x})^T begin{bmatrix} frac{ Delta x_1}{ Delta t} frac{ Delta x_2}{ Delta t} vdots frac{ Delta x_n}{ Delta t} end{bmatrix} + mathbf{ epsilon}^T begin{bmatrix} frac{ Delta x_1}{ Delta t} frac{ Delta x_2}{ Delta t} vdots frac{ Delta x_n}{ Delta t} end{bmatrix} lim_{ Delta t rightarrow 0} frac{ Delta y}{ Delta t} = lim_{ Delta t rightarrow 0} left { nabla_ mathbf{x} f( mathbf{x})^T begin{bmatrix} frac{ Delta x_1}{ Delta t} frac{ Delta x_2}{ Delta t} vdots frac{ Delta x_n}{ Delta t} end{bmatrix} + mathbf{ epsilon}^T begin{bmatrix} frac{ Delta x_1}{ Delta t} frac{ Delta x_2}{ Delta t} vdots frac{ Delta x_n}{ Delta t} end{bmatrix} right } therefore quad frac{dy}{dt} = nabla_ mathbf{x} f( mathbf{x})^T begin{bmatrix} frac{dx_1}{dt} frac{dx_2}{dt} vdots frac{dx_n}{dt} end{bmatrix} qquad ( therefore , mathbf{ epsilon}, begin{bmatrix} Delta x_1 Delta x_2 vdots Delta x_n end{bmatrix} rightarrow mathbf{0} , text{as} , Delta t rightarrow 0) quad blacksquare end{matrix} $$ Theorem.1.5. (Chain Rule) . If $y = f( mathbf{x})$ and $ mathbf{x} = mathbf{g}( mathbf{t})$ are differentiable functions that $f: mathbb{R}^n rightarrow mathbb{R}, , mathbf{g}: mathbb{R}^m rightarrow mathbb{R}^n $, then . $$ nabla_ mathbf{t} f( mathbf{x}) = begin{bmatrix} frac{ partial}{ partial t_1}f( mathbf{x}) frac{ partial}{ partial t_t}f( mathbf{x}) vdots frac{ partial}{ partial t_m}f( mathbf{x}) end{bmatrix}_{m times 1}, quad frac{ partial}{ partial t_i}f( mathbf{x}) = nabla_ mathbf{x} f( mathbf{x})^T begin{bmatrix} frac{ partial x_1}{ partial t_i} frac{ partial x_2}{ partial t_i} vdots frac{ partial x_n}{ partial t_i} end{bmatrix} $$Proof. . Trivials. . Theorem.1.6. . Followings are true. . $ nabla_ mathbf{w} ( mathbf{x}^T mathbf{w}) = mathbf{x} $ | $ nabla_ mathbf{w} ( mathbf{w}^T R mathbf{w}) = (R + R^T) mathbf{w} $ | Proof. . Trivials. . . Application.1.1. . Solve a equation $ nabla_ mathbf{w} || mathbf{X}_{m times n} mathbf{w}_{n times 1} - hat{ mathbf{y}}_{m times 1} ||^2 = mathbf{0} $ . Solve. . $$ begin{aligned} &amp; nabla_ mathbf{w} (X mathbf{w} - hat{ mathbf{y}})^T (X mathbf{w} - hat{ mathbf{y}}) &amp;= nabla_ mathbf{w} ( mathbf{w}^T X^T - hat{ mathbf{y}}^T)^T (X mathbf{w} - hat{ mathbf{y}}) &amp;= nabla_ mathbf{w} ( mathbf{w}^T X^T X mathbf{w} - mathbf{w}^T X^T hat{ mathbf{y}} - hat{ mathbf{y}}^T X mathbf{w} + hat{ mathbf{y}}^T hat{ mathbf{y}}) &amp;= nabla_ mathbf{w} ( mathbf{w}^T X^T X mathbf{w} - 2 hat{ mathbf{y}}^T X mathbf{w} + hat{ mathbf{y}}^T hat{ mathbf{y}}) &amp;= 2X^TX mathbf{w} - 2X^T hat{ mathbf{y}} = mathbf{0} , &amp; qquad X^TX mathbf{w} = X^T hat{ mathbf{y}} &amp; therefore , mathbf{w} = (X^TX)^{-1}X^T hat{ mathbf{y}} end{aligned} $$ . 1.3. Direction of Gradients . Consider the directional vector $ mathbf{d}$ that maximizes $ mathcal{L}( mathbf{w} + eta mathbf{d}) - mathcal{L}( mathbf{w}) ( le 0)$. Geometically, $ mathbf{d}$ must be same direction of $ mathcal{L}( mathbf{w})$, therefore $ cos( phi) = 1$. . $$ therefore , mathbf{d} = frac{ nabla_ mathbf{w} mathcal{L}( mathbf{w})}{|| nabla_ mathbf{w} mathcal{L}( mathbf{w})||} $$that is, the direction of the gradient is the direction in which the function value increases. .",
            "url": "https://ndo04343.github.io/blog/calculus/2022/01/27/ch01-basic-calculus.html",
            "relUrl": "/calculus/2022/01/27/ch01-basic-calculus.html",
            "date": " • Jan 27, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "[OptimizationTheory] CH02. Search based Optimization",
            "content": "2.1. Grid Search . $$ mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , f( mathbf{x}) , text{where} , f:(0,1)^n rightarrow mathbb{R} , text{and} , mathbf{x} in {(a_1, a_2, cdots, a_n) : a_1, a_2, cdots, a_n in {0, frac{1}{m}, frac{2}{m}, cdots, frac{m - 1}{m} } }, , m in mathbb{N} $$Example) . In KNN classification, we can obtain $k$ with grid search. | In SVM classification with RBF kernel, we can obtain $c$ and $ gamma$. | . from sklearn.model_selection import GridSearchCV from sklearn import svm, datasets iris = datasets.load_iris() params = { &#39;kernel&#39;: (&#39;linear&#39;, &#39;rbf&#39;), &#39;C&#39;: [1, 10] } svc = svm.SVC() clf = GridSearchCV(svc, params) clf.fit(iris.data, iris.target) clf.cv_results_ . {&#39;mean_fit_time&#39;: array([0.00038528, 0.00042386, 0.00036063, 0.00038018]), &#39;std_fit_time&#39;: array([4.15368686e-05, 6.48989211e-06, 1.46394444e-05, 7.80994636e-06]), &#39;mean_score_time&#39;: array([0.00018463, 0.00020938, 0.00017271, 0.00018597]), &#39;std_score_time&#39;: array([1.26002358e-05, 1.30935003e-06, 6.32595976e-07, 3.16657214e-06]), &#39;param_C&#39;: masked_array(data=[1, 1, 10, 10], mask=[False, False, False, False], fill_value=&#39;?&#39;, dtype=object), &#39;param_kernel&#39;: masked_array(data=[&#39;linear&#39;, &#39;rbf&#39;, &#39;linear&#39;, &#39;rbf&#39;], mask=[False, False, False, False], fill_value=&#39;?&#39;, dtype=object), &#39;params&#39;: [{&#39;C&#39;: 1, &#39;kernel&#39;: &#39;linear&#39;}, {&#39;C&#39;: 1, &#39;kernel&#39;: &#39;rbf&#39;}, {&#39;C&#39;: 10, &#39;kernel&#39;: &#39;linear&#39;}, {&#39;C&#39;: 10, &#39;kernel&#39;: &#39;rbf&#39;}], &#39;split0_test_score&#39;: array([0.96666667, 0.96666667, 1. , 0.96666667]), &#39;split1_test_score&#39;: array([1. , 0.96666667, 1. , 1. ]), &#39;split2_test_score&#39;: array([0.96666667, 0.96666667, 0.9 , 0.96666667]), &#39;split3_test_score&#39;: array([0.96666667, 0.93333333, 0.96666667, 0.96666667]), &#39;split4_test_score&#39;: array([1., 1., 1., 1.]), &#39;mean_test_score&#39;: array([0.98 , 0.96666667, 0.97333333, 0.98 ]), &#39;std_test_score&#39;: array([0.01632993, 0.02108185, 0.03887301, 0.01632993]), &#39;rank_test_score&#39;: array([1, 4, 3, 1], dtype=int32)} . clf.best_params_ . {&#39;C&#39;: 1, &#39;kernel&#39;: &#39;linear&#39;} . 2.2. Random Search . Random search randomly selects a point to search for. It is generally faster than grid search. . from sklearn import svm, datasets from sklearn.model_selection import RandomizedSearchCV iris = datasets.load_iris() params = { &#39;kernel&#39;: (&#39;linear&#39;, &#39;rbf&#39;), &#39;C&#39;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] } svc = svm.SVC() clf = RandomizedSearchCV(svc, params, n_iter=5) clf.fit(iris.data, iris.target) clf.cv_results_ . {&#39;mean_fit_time&#39;: array([0.0004076 , 0.00036163, 0.00039191, 0.00040193, 0.00035834]), &#39;std_fit_time&#39;: array([2.62769725e-05, 5.79195235e-06, 4.55473096e-06, 1.88120569e-05, 1.21943587e-05]), &#39;mean_score_time&#39;: array([0.00020247, 0.00017519, 0.00019407, 0.00019431, 0.00019393]), &#39;std_score_time&#39;: array([1.38532479e-05, 6.97552626e-07, 2.92001932e-06, 2.64633883e-06, 3.75817868e-05]), &#39;param_kernel&#39;: masked_array(data=[&#39;rbf&#39;, &#39;linear&#39;, &#39;rbf&#39;, &#39;rbf&#39;, &#39;linear&#39;], mask=[False, False, False, False, False], fill_value=&#39;?&#39;, dtype=object), &#39;param_C&#39;: masked_array(data=[8, 4, 4, 5, 6], mask=[False, False, False, False, False], fill_value=&#39;?&#39;, dtype=object), &#39;params&#39;: [{&#39;kernel&#39;: &#39;rbf&#39;, &#39;C&#39;: 8}, {&#39;kernel&#39;: &#39;linear&#39;, &#39;C&#39;: 4}, {&#39;kernel&#39;: &#39;rbf&#39;, &#39;C&#39;: 4}, {&#39;kernel&#39;: &#39;rbf&#39;, &#39;C&#39;: 5}, {&#39;kernel&#39;: &#39;linear&#39;, &#39;C&#39;: 6}], &#39;split0_test_score&#39;: array([0.96666667, 0.96666667, 0.96666667, 0.96666667, 1. ]), &#39;split1_test_score&#39;: array([1., 1., 1., 1., 1.]), &#39;split2_test_score&#39;: array([1. , 0.93333333, 0.96666667, 1. , 0.9 ]), &#39;split3_test_score&#39;: array([0.96666667, 0.96666667, 0.96666667, 0.96666667, 0.96666667]), &#39;split4_test_score&#39;: array([1., 1., 1., 1., 1.]), &#39;mean_test_score&#39;: array([0.98666667, 0.97333333, 0.98 , 0.98666667, 0.97333333]), &#39;std_test_score&#39;: array([0.01632993, 0.02494438, 0.01632993, 0.01632993, 0.03887301]), &#39;rank_test_score&#39;: array([1, 4, 3, 1, 4], dtype=int32)} . clf.best_params_ . {&#39;kernel&#39;: &#39;rbf&#39;, &#39;C&#39;: 8} . 2.3. Baysian Optimization . Bayesian optimization is a sequential design strategy for global optimization of black-box functions that does not assume any functional forms. The above methods assume that the results of each trial are independent of each other. However, bayesian optimization optimizes by selecting the next candidate point using the results of each trial. . def black_box_function(x, y): return -x ** 2 - (y - 1) ** 2 + 1 . from bayes_opt import BayesianOptimization # Bounded region of parameter space pbounds = {&#39;x&#39;: (2, 4), &#39;y&#39;: (-3, 3)} optimizer = BayesianOptimization( f=black_box_function, pbounds=pbounds, random_state=1, ) . optimizer.maximize( init_points=2, n_iter=3, ) . | iter | target | x | y | - | 1 | -7.135 | 2.834 | 1.322 | | 2 | -7.78 | 2.0 | -1.186 | | 3 | -7.11 | 2.218 | -0.7867 | | 4 | -12.4 | 3.66 | 0.9608 | | 5 | -6.999 | 2.23 | -0.7392 | ================================================= . optimizer.res . [{&#39;target&#39;: -7.135455292718879, &#39;params&#39;: {&#39;x&#39;: 2.8340440094051482, &#39;y&#39;: 1.3219469606529488}}, {&#39;target&#39;: -7.779531005607566, &#39;params&#39;: {&#39;x&#39;: 2.0002287496346898, &#39;y&#39;: -1.1860045642089614}}, {&#39;target&#39;: -7.109925819441113, &#39;params&#39;: {&#39;x&#39;: 2.2175526295255183, &#39;y&#39;: -0.7867249801593896}}, {&#39;target&#39;: -12.397162416009818, &#39;params&#39;: {&#39;x&#39;: 3.660003815774634, &#39;y&#39;: 0.9608275029525108}}, {&#39;target&#39;: -6.999472814518675, &#39;params&#39;: {&#39;x&#39;: 2.2303920156083024, &#39;y&#39;: -0.7392021938893159}}] . optimizer.max . {&#39;target&#39;: -6.999472814518675, &#39;params&#39;: {&#39;x&#39;: 2.2303920156083024, &#39;y&#39;: -0.7392021938893159}} . 2.4. Golden-Section Search . Golden-section search algorithm is search algorithm for finding a minumum on an interval $[x_l, x_u]$ with a single minimum(unimodal interval). It uses the golden ratio $ phi = 1.6180 cdots $ to determine location of two interior points $x_1$ and $x_2$. By using the golden ratio, one of the interior points can be re-used in the next iteration. . $$ begin{matrix} text{Let} , d = ( phi - 1)(x_u - x_l) x_1 = x_l + d, , x_2 = x_u - d end{matrix} $$ Similarily, compute new $d$ about $x_1$ and $x_2$. Afterwards, it repeats the specified number of times or until the relative error is lower than the specified threshold. . from scipy import optimize def f(x): return (x - 1)**2 minimum = optimize.golden(f, brack=(0, 5)) minimum . 1.000000003917054 .",
            "url": "https://ndo04343.github.io/blog/optimization-theory/2022/01/26/ch02-search-based-optimization.html",
            "relUrl": "/optimization-theory/2022/01/26/ch02-search-based-optimization.html",
            "date": " • Jan 26, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "[OptimizationTheory] CH01. Introduction",
            "content": "1.0. Optimization . Optimization is the process of creating something that is as effective as possible. From a mathematical perspective, optimization deals with finding the maxima and minima of a function that depends on one or more variables. . For example, determin the optimum analytically . $$ z = z_0 + frac{m}{c}(v_0 + frac{mg}{c})(- exp(-(c/m)t)) - frac{mg}{c}t quad text{where} , g = 9.81, , z_0 = 100, , v_0 = 55, , m = 80, , c = 15 $$ Definition.1.1. Optimization Problem . Following problem that find optimal solution $ mathbf{x}$ are called optimization problem. . $$ mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , f( mathbf{x}) $$where $ mathbf{x}$ is optimization variable, $f : mathbb{R}^n rightarrow mathbb{R}$ is objective function. In this situation, $ mathbf{x}^*$ is called optimal solution. . It is very difficult to solve general optimization problem. Many optimization method involve some compromise, e.g., very long computation time, or not always finding the solution. However, certain problems can be solved efficiently and reliably. For example, . Least-squares problems | Linaer programming problems | Convex optimization problems | . 1.1. Classification of Optimization Method . There are some kinds of optimization method like . Search | Least-Squares | Linear Programming/Nonlinear Optimization | Convex Optimization | Gradient based Optimization | . Above methods are not separated by analytical method and numerical method. . 1.2. Search based Algorithm . Search based algorithms are simplest method in optimization theory. It just computes the objective function value for many $x$ candidates and finds the minimum point. Examples of algorithms are as follows. . Grid Search | Golden-Section Search | . These algorithms can be used effectively for single variable functions, but for high-dimensional multivariate functions, the amount of computation increases exponentially. And also, the solution does not guarantee a global optimum. . 1.3. Least-Squares(Mature technology) . Form like . $$ mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , || A mathbf{x} - mathbf{b} ||_2^2 $$are called least-squares problem. Optimal solution can be obtained analytically e.g., $ mathbf{x}^* = (A^T A)^{-1} A^T mathbf{b}$. There are reliable and efficient algorithms and software that have $n^2k , (A in mathbb{R}^{k times n})$ time complexity. It can increase flexibility by few standard techniques like including weights, adding regularization terms. . 1.4. Linear Programming(Mature technology)/Nonlinear Programming . Form like . $$ mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , mathbf{c}^T mathbf{x} quad s.t. quad mathbf{a}_i^T mathbf{x} le mathbf{b}_i, , i = 1, cdots,m $$are called linear programming. There is no analytical formula for solution, but there are reliable and efficient algorithms and software that have $n^2m$ time complexity. . Form like . $$ mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , f( mathbf{x}) quad s.t. quad g_i( mathbf{x}) le 0, , h_j( mathbf{x}) = 0, , i=1, cdots,p, , j=1, cdots,q $$are called nonlinear programming. There are local optimization methods and global optimization methods. . 1.5. Convex Optimization . Form like . $$ mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , f( mathbf{x}) quad s.t. quad g_i( mathbf{x}) le 0, , h_j( mathbf{x}) = 0, , i=1, cdots,p, , j=1, cdots,q, , f , text{is a convex function.} $$This problem includes least-squares problems and linear programming as special cases. $g_i$ is called an inequality constraint function and $h_j$ is called an equality constraint function. There are no analytical solutions, but reliable and efficient algorithms that have $ text{max} {n^3, n^2p, n^2q }$ time complexity roughly. . Using convex optimization often difficult to recognize. However, there are many tricks for transforming problems into convex form. Many problems can be solved via convex optimization. . 1.6. Gradient based Optimization . A gradient based optimization is an algorithm to solve problems of the form $$ mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , f( mathbf{x}) $$ with the search directions defined by the gradient of the function at the current point. .",
            "url": "https://ndo04343.github.io/blog/optimization-theory/2022/01/26/ch01-introduction.html",
            "relUrl": "/optimization-theory/2022/01/26/ch01-introduction.html",
            "date": " • Jan 26, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "[LinearAlgebra] CH02. Linear Transformation",
            "content": "2.0. Elementary Definitions and Theorems . Definition.2.1. Linear Transformation . $ mathbb{K} $상의 벡터공간 $V$와 $W$에 대해서, 다음 두 조건을 만족하는 함수 $T , : , V rightarrow W$를 linear transformation이라고 한다. . $ ^ forall mathbf{x} mathbf{y} in V, , ^ forall alpha in mathbb{K}, $ . $ T( mathbf{x} + mathbf{y}) = T( mathbf{x}) + T( mathbf{y}) $ | $ T( alpha mathbf{x}) = alpha T( mathbf{x}) $ | 특히, 일차변환 $T , : , V rightarrow W$가 전단사일때, 이를 linear isomorphism이라 한다. 그리고 이 때의 벡터공간 $V$와 $W$는 linear isomorphic이라고 하고, $ V cong W $로 나타낸다. . EX) . Zero transformation | Identity transformation | Inverse transformation of linear transformation $ T $(Not always). | Matrix transformation | . $ text{Let} , A = [a_{ij}]_{m times n} , text{for} , a_{ij} in mathbb{K} $. $ text{If} , ^ forall mathbf{x} in mathbb{K}^n, , T_A( mathbf{x}) = A mathbf{x} $, . $$ T_A , : , mathbb{K}^n rightarrow mathbb{K}^m $$ Differential transformation | Definite integral transformation | . 2.1. Symmetric Matrix and Orthogonal Matrix . Definition.2.2. Symmetric Matrix and Orthogonal Matrix . $$ text{If} , A^T = A , text{, then} , A , text{is a symetric matrix.} $$$$ text{If} , A^T = A^{-1} , text{, then} , A , text{is an orthogonal matrix.} $$ Theorem.2.1. . $ text{Let} , Q_1 = [q_{ij}^{(1)}]_{n times n} , text{and} , Q_2 = [q_{ij}^{(2)}]_{n times n}$. . $$ text{If} , Q_1, , Q_2 , text{are orthogonal matrices, then the followings are true.} $$ $ text{Every pairs of columns are orthogonal.} $ | $ text{Every columns are unit vector.} $ | $ Q_1Q_2 , text{is also orthogonal matrix.} $ | $ |Q| = 1 , text{or} , |Q| = -1 $ | . Proof. Trivial. . Theorem.2.2. . $ text{Let} , A , text{be a matrix.}$. . $$ text{Then,} , A^TA , text{is a symmetric matrix.} $$ $ (i, i) text{-element of} , A^TA = A_{C_i}^TA_{C_i} ge 0 $ | . Proof. Trivial. . 2.2. Eigenvalue and Eigenvector . Consider $T_A , : , V rightarrow V$ and following vector equation. . $$ T_A( mathbf{x}) = A mathbf{x} = lambda mathbf{x} quad text{for} , lambda in mathbb{R}. $$$ text{Since} , (A - lambda I_n) mathbf{x} = mathbf{0} Leftrightarrow A mathbf{x} = lambda mathbf{x}, $ $ text{by Basic Theorem of Algebra, above equation have} , n , text{complex solutions in} , V. $ . Definition.2.3. Eigenvalue and Eigenvector . $ text{Let} , T_A , : , V rightarrow V , text{and} , text{correspond with matrix} , A$. . For $ lambda in mathbb{K}, , T( mathbf{x}) = lambda mathbf{x}$ is called characteristic equation of linear transform $T$ . | In $T( mathbf{x}) = lambda mathbf{x}$, $ lambda$ is called eigenvalue, and corresponding $ mathbf{x}( neq mathbf{0}, in V)$ is called eigenvector . | Theorem.2.3. . $n$차 정방행렬 A와 정칙행렬 $N$에 대해서 $A, A^T, N^{-1}AN$의 고유치는 일치한다. . Proof. Trivial. . Theorem.2.4. . $ text{Let} , A = [a_{ij}]_{n times n}.$ . $$ begin{matrix} prod_{k = 1}^{n} lambda_k = |A| sum_{k = 1}^{n} lambda_k = sum_{k = 1}^{n} a_{kk} end{matrix} $$Proof. Trivial. . 2.3. Diagonalization . 유한차원 벡터공간 $V$의 일차변환 $T:V rightarrow V$에 대해서, $T$의 행렬을 대각행렬로 만드는 $V$의 기저가 존재할까? . 위의 문제는 theorem.2.3. 에 의해서 다음 문제와 동치이다. . 주어진 실정방행렬 $A$에 대해서, $N^{-1}AN$이 대각행렬이 되는 정칙행렬 $N$이 존재하는가? (복소정방행렬 $A$에 대해서는 $ bar{N}^{-1}AN$에 대해서 따진다.) . Definition.2.3. Diagonalizable . $ text{Let} , A = [a_{ij}]_{n times n} , text{for} , a_{ij} in mathbb{R}$. $ text{If} , exists , text{invertible matrix} , N in mathbb{M}_{n times n}( mathbb{R}) quad s.t. quad N^{-1}AN = diag(d_1, d_2, cdots, d_n),$ $ text{then} , A , text{is diagonalizable by} , N$. . Theorem.2.5. . $ text{Let} , A = [a_{ij}]_{n times n} , text{for} , a_{ij} in mathbb{R}, , lambda_1, lambda_2, cdots, lambda_n , text{are eigenvalues of matrix} , A, , text{and} , mathbf{x}_1, mathbf{x}_2, cdots, mathbf{x}_n , text{are corresponding eigenvectors of eigenvalues}$. $ text{Assume that} , { mathbf{x}_1, mathbf{x}_2, cdots, mathbf{x}_n} , text{are ordered basis of} , mathbb{R}^n$. $ text{Let} , N = [ mathbf{x}_1 , mathbf{x}_2 , cdots , mathbf{x}_n]$. $ text{Then} , N , text{is invertible and} , A , text{is diagonalizable by} , N$. $ text{That is}$ . $$ N^{-1}AN = diag( lambda_1, lambda_2, cdots, lambda_n) $$. . $ text{If} , lambda_1, lambda_2, cdots, lambda_n , text{are different with each other, then the eigenvectors} , mathbf{x}_1, mathbf{x}_2, cdots, mathbf{x}_n , text{are linearly independent and diagonalizable.} $ | $ text{Eigenvector can be multiplied any scalar except zero.} $ | . Proof. Trivial. . Theorem.2.6. . $ text{Let} , A , text{be a diagonalizable real matrix.}$ $ text{Then}$ . $$ A^k = N^{-1}D^kN quad text{for} , k in mathbb{Z}. $$Proof. Trivial. . 유한차원 내적공간 $V$의 일차변환 $T:V rightarrow V$에 대해서, $T$의 행렬을 대각행렬로 만드는 $V$의 정규직교기저가 존재할까? . 위의 문제는 theorem.2.3. 에 의해서 다음 문제와 동치이다. . 주어진 실정방행렬 $A$에 대해서, $P^{-1}AP$이 대각행렬이 되는 직교행렬 $P$가 존재하는가? (복소정방행렬 $A$에 대해서는 $ bar{P}^{-1}AP$에 대해서 따진다.) . Definition.2.4. Orthogonally Diagonalizable . 실정방행렬 $A$가 직교행렬 $P$에 의해서 대각화되면 A는 orthogonally diagonalizable이라고 한다. . Theorem.2.7. . 실대칭행렬 $A$에 대해서, 서로 다른 고유치에 대응되는 고유벡터는 직교한다. . Proof. $ text{Let} , A , text{be orthogonally diagonalizable and} , lambda_1, lambda_2, cdots, lambda_n, mathbf{x}_1, mathbf{x}_2, cdots, mathbf{x}_n , text{are eigenvalues and corresponding eigenvectors.} $ $ text{For} , i neq j, $ $$ begin{matrix} lambda_i mathbf{x}_i^T mathbf{x}_j &amp;= (A mathbf{x}_i)^T mathbf{x}_j &amp;= mathbf{x}_i^T A^T mathbf{x}_j &amp;= mathbf{x}_i(A mathbf{x}_j) &amp;= mathbf{x}_i^T ( lambda_j mathbf{x}_j) end{matrix} $$ . $ text{Therefore,} , ( lambda_i - lambda_j) mathbf{x}_i^T mathbf{x}_j = 0. $ $ text{Since} , lambda_i - lambda_j neq 0, mathbf{x}_i^T mathbf{x}_j = 0.$ . $$ therefore quad mathbf{x}_i perp mathbf{x}_j , text{for} , i neq j quad blacksquare$$ . Theorem.2.8. . $n$차 실정방행렬 $A$에 대해서, $A$가 직교대각화가능일 필요충분조건은 $A$가 대칭행렬이다. . Proof. Trivial. . 2.3. Singular Value Decomposition . Definition.2.5. Positive Definite . $ text{Let} , A , text{be a symmetric matrix.} $ . $$ text{If} , ^ forall mathbf{x} neq mathbf{0}, , mathbf{x}^T A mathbf{x} &gt; 0, , text{then} , A , text{is called positive definite.} $$$$ text{If} , ^ forall mathbf{x} neq mathbf{0}, , mathbf{x}^T A mathbf{x} ge 0, , text{then} , A , text{is called positive semidefinite.} $$$$ text{If} , ^ forall mathbf{x} neq mathbf{0}, , mathbf{x}^T A mathbf{x} &lt; 0, , text{then} , A , text{is called negative definite.} $$$$ text{If} , ^ forall mathbf{x} neq mathbf{0}, , mathbf{x}^T A mathbf{x} le 0, , text{then} , A , text{is called negative semidefinite.} $$ Theorem.2.9. . 대칭행렬 $A$에 대하여 다음이 성립한다. . $A$가 Positive definite면 모든 $A$의 모든 고윳값은 양수이다. | $A$가 Positive semidefinite면 모든 $A$의 모든 고윳값은 음이 아닌 수수이다. | $A$가 Negative definite면 모든 $A$의 모든 고윳값은 음수이다. | $A$가 Negative semidefinite면 모든 $A$의 모든 고윳값은 양이 아닌 실수이다. | . Proof. Chapter03 이후 . . Application.2.1. Eigenvalue Decomposition(EVD) . By above theorems. . Application.2.2. Singular Value Decomposition(SVD) . Any matrix $A_{m times n}$ can be decomposed as $ A = U_{m times m} Sigma_{m times n} {V_{n times n}}^T $ where $ AA^T = U Sigma Sigma^T U^T, , A^TA = V Sigma^T Sigma V^T $ . .",
            "url": "https://ndo04343.github.io/blog/linear-algebra/2022/01/25/ch02-linear-transformation.html",
            "relUrl": "/linear-algebra/2022/01/25/ch02-linear-transformation.html",
            "date": " • Jan 25, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "[LinearAlgebra] CH01. Linear Equations and Inverse Matrices",
            "content": "1.0. Elementary Definitions and Theorems . 이 노트에서 사용하는 수학 표기는 표기법을 따른다. | 기본적인 행렬의 연산과 성질들은 생략한다. | . Definition.1.1. Matrix . 임의의 자연수 $m$과 $n$에 대하여 $mn$개의 수 $(a_{ij} in mathbb{K}, , i = 1, 2, cdots, m, , j = 1, 2, cdots, n)$를 다음과 같이 배열한 도식(diagram) $A$를 $m times n$ 행렬(matrix)이라고 한다. . $$ A = begin{bmatrix} a_{11} &amp; a_{12} &amp; cdots &amp; a_{1j} &amp; cdots &amp; a_{1n} a_{21} &amp; a_{22} &amp; cdots &amp; a_{2j} &amp; cdots &amp; a_{2n} vdots &amp; vdots &amp; ddots &amp; vdots &amp; &amp; vdots a_{i1} &amp; a_{i2} &amp; cdots &amp; a_{ij} &amp; cdots &amp; a_{in} vdots &amp; vdots &amp; &amp; vdots &amp; ddots &amp; vdots a_{m1} &amp; a_{m2} &amp; cdots &amp; a_{mj} &amp; cdots &amp; a_{mn} end{bmatrix} $$행렬 $A$를 간단히 $A = [a_{ij}]$ 또는 $A = [a_{ij}]_{m times n}$과 같이 나타낸다. . 이 글에서 행렬과 관련된 기본 정의와 정리들은 위의 표기를 따른다. . 1.1. System of Linear Equations . Definition.1.2. Augmented Matrices and Coefficient Matrices . $n$개의 미지수 $x_1, , x_2, , cdots, , x_n$에 관한 연립일차방정식 . $$ begin{cases} a_{11}x_1 + a_{12}x_2 + cdots + a_{1n}x_n = b_1 a_{21}x_1 + a_{22}x_2 + cdots + a_{2n}x_n = b_2 ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; vdots a_{m1}x_1 + a_{m2}x_2 + cdots + a_{mn}x_n = b_m end{cases}, $$에 대하여 . $$ A = begin{bmatrix} a_{11} &amp; a_{12} &amp; cdots &amp; a_{1n} a_{21} &amp; a_{22} &amp; cdots &amp; a_{2n} vdots &amp; vdots &amp; ddots &amp; vdots a_{m1} &amp; a_{m2} &amp; cdots &amp; a_{mn} end{bmatrix}_{m times n}, mathbf{x} = begin{bmatrix} x_{1} x_{2} vdots x_{n} end{bmatrix}_{n times 1}, text{and} , mathbf{b} = begin{bmatrix} b_{1} b_{2} vdots b_{m} end{bmatrix}_{m times 1} $$로 둘때, 연립일차방정식을 행렬을 써서 나타내면 . $$ A mathbf{x} = mathbf{b} $$이다. 이때, 연립일차방정식에 대응되는 행렬 $[A quad mathbf{b}]_{m times (n + 1)}$를 주어진 연립일차방정식의 첨가행렬(Augmented matrix)이라고 하고 $A$를 연립일차방정식의 계수행렬(Coeffficient matrix)이라고 한다. . Theorem.1.1. . $A = [a_{ij}]_{n times n}, text{and} , B = [b_{ij}]_{n times n}$에 대하여 다음이 성립한다. . $$ qquad (AB)^ top = B^ top A^ top $$Proof. Trivial. . Theorem.1.2. . Let $A$ is invertible and have $n$-degree. . $$ qquad (A^ top)^{-1} = (A^{-1})^ top $$Proof. Trivial. . Theorem.1.3. . Square matrix $A$에 대하여, 다음이 성립한다. . $$ text{(1)} quad text{If} , [A quad I] cong_R [I quad P], , text{then} , P = A^{-1} text{(2)} quad text{If} , begin{bmatrix} A I end{bmatrix} cong_C begin{bmatrix} I P end{bmatrix}, , text{then} , P = A^{-1} $$Proof. Trivial. . Theorem.1.4. . Square matrix $A, , B, , C$에 대하여, $A$가 가역행렬일때, . $$ text{If} , [A quad B] cong_R [I quad C], , text{then} , C = A^{-1}B $$Proof. Trivial. . Theorem.1.5. . $ text{If} , A , text{and} , B , text{are} , n text{-degree square matrices, then followings are true.} $ . $$ text{(1)} quad Tr(AB) = Tr(BA) text{(2)} quad Tr(PAP^{-1}) = Tr(A) $$Proof. Trivial. . Definition.1.3. Gauss-Jordan Elimination and Gauss Elimination . Forward substitution을 통해서 어떤 행렬을 ref(row echelon form)으로 변환하는 과정을 Gauss Elimination이라고 하고, Back substitution까지 진행하여 어떤 행렬을 rref(reduced row echelon form으로 변환하는 과정을 Gauss-Jordan Elimination이라고 한다. . Definition.1.4. Matrix Decomposition . Matrix Decomposition은 어떤 Matrix를 여러 행렬들의 곱으로 표현하는 것을 의미한다. 이는 Computational convenience, analytic simplicity를 목적으로 가진다. . . Application.1.1. LU Decomposition . $ text{Let} , m times n , text{matrix} , A.$ $ text{By Gauss Elimination, We can get matrix} , U. $ $ text{Then} , E_p cdots E_1 A = U $ $ L = (E_p cdots E_1)^{-1} $ . Ex) . $$ A = begin{bmatrix} 2 &amp; 4 &amp; -1 &amp; 5 &amp; -2 -4 &amp; -5 &amp; 3 &amp; -8 &amp; 1 2 &amp; -5 &amp; -4 &amp; 1 &amp; 8 -6 &amp; 0 &amp; 7 &amp; -3 &amp; 1 end{bmatrix} cong begin{bmatrix} 2 &amp; 4 &amp; -1 &amp; 5 &amp; -2 0 &amp; 3 &amp; 1 &amp; 2 &amp; -3 0 &amp; -9 &amp; -3 &amp; -4 &amp; 10 0 &amp; 12 &amp; 4 &amp; 12 &amp; -5 end{bmatrix} cong begin{bmatrix} 2 &amp; 4 &amp; -1 &amp; 5 &amp; -2 0 &amp; 3 &amp; 1 &amp; 2 &amp; -3 0 &amp; 0 &amp; 0 &amp; 2 &amp; 1 0 &amp; 0 &amp; 0 &amp; 4 &amp; 7 end{bmatrix} cong begin{bmatrix} 2 &amp; 4 &amp; -1 &amp; 5 &amp; -2 0 &amp; 3 &amp; 1 &amp; 2 &amp; -3 0 &amp; 0 &amp; 0 &amp; 2 &amp; 1 0 &amp; 0 &amp; 0 &amp; 0 &amp; 5 end{bmatrix} = U $$$$ L_{C_1} = frac{1}{2} begin{bmatrix} 2 -4 2 -6 end{bmatrix}, , L_{C_2} = frac{1}{3} begin{bmatrix} 0 3 -9 12 end{bmatrix}, , L_{C_3} = frac{1}{2} begin{bmatrix} 0 0 2 4 end{bmatrix}, , L_{C_4} = frac{1}{5} begin{bmatrix} 0 0 0 5 end{bmatrix}. $$ $$ text{Therefore, } , L = begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 -2 &amp; 1 &amp; 0 &amp; 0 1 &amp; -3 &amp; 1 &amp; 0 -3 &amp; 4 &amp; 2 &amp; 1 end{bmatrix} $$Suppose we have to solve $ A mathbf{x} = mathbf{b} $. We can get $ A = LU $. Then . $$ L mathbf{d} = mathbf{b} U mathbf{x} = mathbf{d} $$ import numpy as np from scipy.linalg import lu A = np.array([[2, 4, -1, 5, -2], [-4, -5, 3, -8, 1], [2, -5, -4, 1, 8], [-6, 0, 7, -3, 1]]) P, L, U = lu(A) # P is permutating matrix(pivoting) P, L, U . (array([[0., 0., 0., 1.], [0., 1., 0., 0.], [0., 0., 1., 0.], [1., 0., 0., 0.]]), array([[ 1. , 0. , 0. , 0. ], [ 0.66666667, 1. , 0. , 0. ], [-0.33333333, 1. , 1. , 0. ], [-0.33333333, -0.8 , -0. , 1. ]]), array([[-6.00000000e+00, 0.00000000e+00, 7.00000000e+00, -3.00000000e+00, 1.00000000e+00], [ 0.00000000e+00, -5.00000000e+00, -1.66666667e+00, -6.00000000e+00, 3.33333333e-01], [ 0.00000000e+00, 0.00000000e+00, -8.88178420e-16, 6.00000000e+00, 8.00000000e+00], [ 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, -8.00000000e-01, -1.40000000e+00]])) . . 1.2. Determinants and Inverse Matrices . Theorem.1.6. . $n$차 정방행렬 $A = [a_{ij}]$에 대하여 $|A| = |A^ top|$ . Proof. Trivial. . Theorem.1.7. . $n$차 정방행렬 $A = [a_{ij}]$와 수 $k$에 대하여 $|kA| = k^n|A|$ . Proof. Trivial. . Theorem.1.8. . 두 행렬의 곱의 행렬식은 각각의 행렬식의 곱과 같다. . Proof. Trivial. . Definition.1.5. Minor Determinant and Cofactor . $n$차 정방행렬 $A = [a_{ij}]$의 $i$번째 행과 $j$번째 열을 삭제하고 남은 $(n - 1)$차 정방행렬의 행렬식을 행렬 $A$의 $(i, j)-$Minor determinant이라고 하고 $M_{ij}$로 쓴다. 또한 $M_{ij}$에 부호 $(-1)^{i + j}$를 곱한 $(-1)^{i + j}M_{ij}$를 행렬 $A$의 $(i, j)$-Cofactor라고 하고 $C_{ij}$로 쓴다. 즉, . $$ C_{ij} = (-1)^{i + j}M_{ij} $$ Definition.1.6. Adjoint Matrix and Cofactor Matrix . $n$차 정방행렬 $A = [a_{ij}]$에서 $(i, j)$-Cofactor $C_{ij}$를 $(i, j)$ 성분으로 하는 행렬 $C = [C_{ij}]$의 전치행렬 $C^ top$를 $A$의 Adjoint matrix또는 여인수 행렬(Cofactor matrix)이라고 하며, $ text{adj}(A)$로 나타낸다. . $$ text{adj}(A) = [C_{ij}]^ top = begin{bmatrix} C_{11} &amp; C_{21} &amp; cdots &amp; C_{n1} C_{12} &amp; C_{22} &amp; cdots &amp; C_{n2} vdots &amp; vdots &amp; ddots &amp; vdots C_{1n} &amp; C_{2n} &amp; cdots &amp; C_{nn} end{bmatrix} $$ Theorem.1.9. . $n$차 정방행렬 $A = [a_{ij}]$에 대하여 다음이 성립한다. . $ text{(1)} quad text{adj}(A)A = A text{adj}(A) = |A|I_n$ $ text{(2)} quad A$가 정칙행렬일 필요충분조건은 $|A| neq 0$이다. $ text{(3)} quad A$가 정칙행렬이면 $A^{-1} = frac{1}{|A|} text{adj}(A)$이다. . Proof. Trivial. . Definition.1.7. Minor Determinant and Rank . $m times n$행렬 $A$의 $m - r$개 행과 $n - r$개의 열을 제거하고 남은 $r$차의 정방행렬에 대한 행렬식을 $A$의 $r$차의 소행렬식(Minor determinant)이라고 한다. 만약 $A$의 $r$차 소행렬식 중에서 $0$이 아닌 것이 적어도 하나 존재하고, $r$보다 큰 모든 $s(r &lt; s)$에 대하여 소행렬식이 모두 $0$일때, $r$을 행렬 $A$의 계수(rank)라고 하고 $$ text{rank}(A) = r $$ 이라고 나타낸다. . Theorem.1.10. . $ text{If} , A cong A^ prime, , text{then} , text{rank}(A) = text{rank}(A^ prime)$ . Proof. Trivial. . Theorem.1.11. . 행렬의 계수(rank)는 그 행렬의 rref에서 leading 1의 개수와 같다. . Proof. Trivial. . 1.3. Rank and Solution of System of Linear Equation . 연립 일차 방정식의 해의 종류는 다음과 같다. . No solution | Particular solution(Only one solution) | Infinitely many solutions | . Definition.1.8. Homogeneous System of Linear Equations . 미지수 $x_1, , x_2, , cdots, , x_n$에 관한 연립일차방정식 . $$ begin{cases} a_{11}x_1 + a_{12}x_2 + cdots + a_{1n}x_n = 0 a_{21}x_1 + a_{22}x_2 + cdots + a_{2n}x_n = 0 ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; vdots a_{m1}x_1 + a_{m2}x_2 + cdots + a_{mn}x_n = 0 end{cases} $$을 Homogeneous System of Linear Euation이라고 한다. . Homogeneous System of Linear Equation은 다음과 같은 해를 가진다. . Trivial solution | Infinitely many solution including trivial solution. | . Theorem.1.12. . $ text{In homogeneous system of linear equations, if} , m &lt; n, , text{then the system have infinitely many solutions.} $ . Proof. Trivial. . Theorem.1.13. . 행렬 $A_{m times n}, , mathbf{b}_{m times 1} = [b_1 , b_2 , cdots , b_m]^ top, , mathbf{x}_{n times 1} = [x_1 , x_2 , cdots , x_n]^ top$에 대하여 연립일차방정식 $A mathbf{x} = mathbf{b}$가 해를 가질 필요충분조건은 . $$ text{rank}(A) = text{rank}([A quad B]) $$이며, $A mathbf{x} = mathbf{b}$가 해를 가지는 경우는 다음과 같다. . $ text{If} , text{rank}(A) = text{rank}([A quad B]) = n, , text{then} , A mathbf{x} = mathbf{b} , text{have a particular solution.}$ | $ text{If} , text{rank}(A) = text{rank}([A quad B]) &lt; n, , text{then} , A mathbf{x} = mathbf{b} , text{have infinitely many solutions.}$ | . Remark) . $ text{If} , text{rank}(A) neq text{rank}([A quad B]), , text{then} , nexists , text{solution of} , A mathbf{x} = mathbf{b}$. | $ text{If} , text{rank}(A) = text{rank}([A quad B]) = n, , text{then} , exists^1 , text{solution of} , A mathbf{x} = mathbf{b}$. | $ text{If} , text{rank}(A) = text{rank}([A quad B]) &lt; n, , text{then} , exists^ infty , text{solution of} , A mathbf{x} = mathbf{b}$. | Proof. Trivial. . Theorem.1.14. . $ m times n $행렬 $A$에 대하여 만약 $m &lt; n$이면 동차연립일차방정식 $A mathbf{x} = mathbf{0}$는 무한히 많은 해를 가진다. . Proof. Trivial. . Theorem.1.15. . $n$차 정방행렬 $A$를 계수행렬로 가지는 동차연립일차방정식 $A mathbf{x} = mathbf{0}$가 자명해가 아닌 해를 가질 필요충분조건은 $ text{rank}(A) &lt; n$이다. . Proof. Trivial. .",
            "url": "https://ndo04343.github.io/blog/linear-algebra/2022/01/25/ch01-linear-equations-and-inverse-matrices.html",
            "relUrl": "/linear-algebra/2022/01/25/ch01-linear-equations-and-inverse-matrices.html",
            "date": " • Jan 25, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "HEESUNG YANG .",
          "url": "https://ndo04343.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  
  

  

  
  

  

  

  
  

  
  

  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ndo04343.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}