{
  
    
        "post0": {
            "title": "[TEST] Test Report",
            "content": "4dhsecT81J83D0ZNeLscxAdGipTnrCZ+vjnmIcHJXSpN6oufU2PuZzB+39s/DUpZ9dIhScj/86muIVOPHE0KcvMsZ6z2in7U6coR4dZmk+CO+3eb52bMhpup4bXE5BgbD0QM8vOt8E4U+1VmnBMfGFh0msyOR2bVfdDPxIeGjDzVhThxdwIhHjW/57iY1A4LM7L9gxIdiZjYrDi+liYLc8r+RRF3XxgeOMSu6+Mn1G8owNBVCxWgsxkmLCHVN1il5vzgvPkWQVdvBnu+o21QwjyXaygWKaZiGBNUbgQHFwNxm+S41RTVzzKE0PyYeh2H6G227YIEsPuUGs/FindhRGb+3nQMxRwnRONDF7fGV5Si0h/Pu9HH69CYKZVO6gFgeqBBe3nGtgNWG7Q5dQgtHmD9IWkQOZREb3mjiXnbnqmB/zpyo6MB+MruJgsXPzYe7uidt0flMLcxh68Rejl7SFwHshaCwgJW7QJEnCrhrnBcHPESu80dYt9KkjmvbBcjIXfYrxPbXSbPSmTV2mBJU+4dTsBqYsTYEGiqftjlk5Iw1gqvDHazGjdj0uJryn4o15ceIyLEOF6/fDGsflO3AmBdAqftKH4Lgz9t+YLo+PQ5smg8NqZseJM0Saz+i+zeWUqvY0tkSw2qQ1ZbS2uNSu51+liwQ1RWByiLMgYhjwaQZOXxXV6J1ulzlVvMUleu5TWP6mwI1FMy2+6/WB2HaJoC4O77ubGf/4KyjmtI1LjnVegIjjWUsAWZU1j+qL+ZtD2ynqLnEh18555s9mVNztK5miaLDelK0n1p4NxfwESuBTzYFJnFNyfc7ZWQNZk/KinDLCkZ/hx86Vi4HEOqVCPBuvww9PPTc+2Eq4n3iU5gyuey4FEYBVqhWXqah2nfoC7VJryEfzw4Y7nopQw8YDw/MbQjNnzdSfWR7nxhcnvAi2grrHIoLgPd/N++cIoBPxZeKr8XEgFO8iWMflCfwmSfXDb2igRZuXLRc0Iq14HWy65tZHyC0E5Ijar5+szX0BFIZIXtqI0orWJL2cxHtSO3UiyO1gvTEwZQxzs3nHvXJLMrRmVXkjloH4mx2q2v+tntFJaESLvtqel6Kh2MICDCigVLipcD2j0+sOx0lsk+2DWpmeaURXGQPgqvl7YAZMRxkQPtiroRVwym3jRq5+1Gu3Pwl3v5YEpNxH1rM0E= .",
            "url": "https://ndo04343.github.io/blog/encrypted/report/2022/01/27/test-report.html",
            "relUrl": "/encrypted/report/2022/01/27/test-report.html",
            "date": " • Jan 27, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "[OptimizationTheory] CH02. Search based Optimization",
            "content": "2.1. Grid Search . $$ mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , f( mathbf{x}) , text{where} , f:(0,1)^n rightarrow mathbb{R} , text{and} , mathbf{x} in {(a_1, a_2, cdots, a_n) : a_1, a_2, cdots, a_n in {0, frac{1}{m}, frac{2}{m}, cdots, frac{m - 1}{m} } }, , m in mathbb{N} $$Example) . In KNN classification, we can obtain $k$ with grid search. | In SVM classification with RBF kernel, we can obtain $c$ and $ gamma$. | . from sklearn.model_selection import GridSearchCV from sklearn import svm, datasets iris = datasets.load_iris() params = { &#39;kernel&#39;: (&#39;linear&#39;, &#39;rbf&#39;), &#39;C&#39;: [1, 10] } svc = svm.SVC() clf = GridSearchCV(svc, params) clf.fit(iris.data, iris.target) clf.cv_results_ . {&#39;mean_fit_time&#39;: array([0.00038528, 0.00042386, 0.00036063, 0.00038018]), &#39;std_fit_time&#39;: array([4.15368686e-05, 6.48989211e-06, 1.46394444e-05, 7.80994636e-06]), &#39;mean_score_time&#39;: array([0.00018463, 0.00020938, 0.00017271, 0.00018597]), &#39;std_score_time&#39;: array([1.26002358e-05, 1.30935003e-06, 6.32595976e-07, 3.16657214e-06]), &#39;param_C&#39;: masked_array(data=[1, 1, 10, 10], mask=[False, False, False, False], fill_value=&#39;?&#39;, dtype=object), &#39;param_kernel&#39;: masked_array(data=[&#39;linear&#39;, &#39;rbf&#39;, &#39;linear&#39;, &#39;rbf&#39;], mask=[False, False, False, False], fill_value=&#39;?&#39;, dtype=object), &#39;params&#39;: [{&#39;C&#39;: 1, &#39;kernel&#39;: &#39;linear&#39;}, {&#39;C&#39;: 1, &#39;kernel&#39;: &#39;rbf&#39;}, {&#39;C&#39;: 10, &#39;kernel&#39;: &#39;linear&#39;}, {&#39;C&#39;: 10, &#39;kernel&#39;: &#39;rbf&#39;}], &#39;split0_test_score&#39;: array([0.96666667, 0.96666667, 1. , 0.96666667]), &#39;split1_test_score&#39;: array([1. , 0.96666667, 1. , 1. ]), &#39;split2_test_score&#39;: array([0.96666667, 0.96666667, 0.9 , 0.96666667]), &#39;split3_test_score&#39;: array([0.96666667, 0.93333333, 0.96666667, 0.96666667]), &#39;split4_test_score&#39;: array([1., 1., 1., 1.]), &#39;mean_test_score&#39;: array([0.98 , 0.96666667, 0.97333333, 0.98 ]), &#39;std_test_score&#39;: array([0.01632993, 0.02108185, 0.03887301, 0.01632993]), &#39;rank_test_score&#39;: array([1, 4, 3, 1], dtype=int32)} . clf.best_params_ . {&#39;C&#39;: 1, &#39;kernel&#39;: &#39;linear&#39;} . 2.2. Random Search . Random search randomly selects a point to search for. It is generally faster than grid search. . from sklearn import svm, datasets from sklearn.model_selection import RandomizedSearchCV iris = datasets.load_iris() params = { &#39;kernel&#39;: (&#39;linear&#39;, &#39;rbf&#39;), &#39;C&#39;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] } svc = svm.SVC() clf = RandomizedSearchCV(svc, params, n_iter=5) clf.fit(iris.data, iris.target) clf.cv_results_ . {&#39;mean_fit_time&#39;: array([0.0004076 , 0.00036163, 0.00039191, 0.00040193, 0.00035834]), &#39;std_fit_time&#39;: array([2.62769725e-05, 5.79195235e-06, 4.55473096e-06, 1.88120569e-05, 1.21943587e-05]), &#39;mean_score_time&#39;: array([0.00020247, 0.00017519, 0.00019407, 0.00019431, 0.00019393]), &#39;std_score_time&#39;: array([1.38532479e-05, 6.97552626e-07, 2.92001932e-06, 2.64633883e-06, 3.75817868e-05]), &#39;param_kernel&#39;: masked_array(data=[&#39;rbf&#39;, &#39;linear&#39;, &#39;rbf&#39;, &#39;rbf&#39;, &#39;linear&#39;], mask=[False, False, False, False, False], fill_value=&#39;?&#39;, dtype=object), &#39;param_C&#39;: masked_array(data=[8, 4, 4, 5, 6], mask=[False, False, False, False, False], fill_value=&#39;?&#39;, dtype=object), &#39;params&#39;: [{&#39;kernel&#39;: &#39;rbf&#39;, &#39;C&#39;: 8}, {&#39;kernel&#39;: &#39;linear&#39;, &#39;C&#39;: 4}, {&#39;kernel&#39;: &#39;rbf&#39;, &#39;C&#39;: 4}, {&#39;kernel&#39;: &#39;rbf&#39;, &#39;C&#39;: 5}, {&#39;kernel&#39;: &#39;linear&#39;, &#39;C&#39;: 6}], &#39;split0_test_score&#39;: array([0.96666667, 0.96666667, 0.96666667, 0.96666667, 1. ]), &#39;split1_test_score&#39;: array([1., 1., 1., 1., 1.]), &#39;split2_test_score&#39;: array([1. , 0.93333333, 0.96666667, 1. , 0.9 ]), &#39;split3_test_score&#39;: array([0.96666667, 0.96666667, 0.96666667, 0.96666667, 0.96666667]), &#39;split4_test_score&#39;: array([1., 1., 1., 1., 1.]), &#39;mean_test_score&#39;: array([0.98666667, 0.97333333, 0.98 , 0.98666667, 0.97333333]), &#39;std_test_score&#39;: array([0.01632993, 0.02494438, 0.01632993, 0.01632993, 0.03887301]), &#39;rank_test_score&#39;: array([1, 4, 3, 1, 4], dtype=int32)} . clf.best_params_ . {&#39;kernel&#39;: &#39;rbf&#39;, &#39;C&#39;: 8} . 2.3. Baysian Optimization . Bayesian optimization is a sequential design strategy for global optimization of black-box functions that does not assume any functional forms. The above methods assume that the results of each trial are independent of each other. However, bayesian optimization optimizes by selecting the next candidate point using the results of each trial. . def black_box_function(x, y): return -x ** 2 - (y - 1) ** 2 + 1 . from bayes_opt import BayesianOptimization # Bounded region of parameter space pbounds = {&#39;x&#39;: (2, 4), &#39;y&#39;: (-3, 3)} optimizer = BayesianOptimization( f=black_box_function, pbounds=pbounds, random_state=1, ) . optimizer.maximize( init_points=2, n_iter=3, ) . | iter | target | x | y | - | 1 | -7.135 | 2.834 | 1.322 | | 2 | -7.78 | 2.0 | -1.186 | | 3 | -7.11 | 2.218 | -0.7867 | | 4 | -12.4 | 3.66 | 0.9608 | | 5 | -6.999 | 2.23 | -0.7392 | ================================================= . optimizer.res . [{&#39;target&#39;: -7.135455292718879, &#39;params&#39;: {&#39;x&#39;: 2.8340440094051482, &#39;y&#39;: 1.3219469606529488}}, {&#39;target&#39;: -7.779531005607566, &#39;params&#39;: {&#39;x&#39;: 2.0002287496346898, &#39;y&#39;: -1.1860045642089614}}, {&#39;target&#39;: -7.109925819441113, &#39;params&#39;: {&#39;x&#39;: 2.2175526295255183, &#39;y&#39;: -0.7867249801593896}}, {&#39;target&#39;: -12.397162416009818, &#39;params&#39;: {&#39;x&#39;: 3.660003815774634, &#39;y&#39;: 0.9608275029525108}}, {&#39;target&#39;: -6.999472814518675, &#39;params&#39;: {&#39;x&#39;: 2.2303920156083024, &#39;y&#39;: -0.7392021938893159}}] . optimizer.max . {&#39;target&#39;: -6.999472814518675, &#39;params&#39;: {&#39;x&#39;: 2.2303920156083024, &#39;y&#39;: -0.7392021938893159}} . 2.4. Golden-Section Search . Golden-section search algorithm is search algorithm for finding a minumum on an interval $[x_l, x_u]$ with a single minimum(unimodal interval). It uses the golden ratio $ phi = 1.6180 cdots $ to determine location of two interior points $x_1$ and $x_2$. By using the golden ratio, one of the interior points can be re-used in the next iteration. . $$ begin{matrix} text{Let} , d = ( phi - 1)(x_u - x_l) x_1 = x_l + d, , x_2 = x_u - d end{matrix} $$ Similarily, compute new $d$ about $x_1$ and $x_2$. Afterwards, it repeats the specified number of times or until the relative error is lower than the specified threshold. . from scipy import optimize def f(x): return (x - 1)**2 minimum = optimize.golden(f, brack=(0, 5)) minimum . 1.000000003917054 .",
            "url": "https://ndo04343.github.io/blog/optimization-theory/2022/01/26/ch02-search-based-optimization.html",
            "relUrl": "/optimization-theory/2022/01/26/ch02-search-based-optimization.html",
            "date": " • Jan 26, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "[OptimizationTheory] CH01. Introduction",
            "content": "1.0. Optimization . Optimization is the process of creating something that is as effective as possible. From a mathematical perspective, optimization deals with finding the maxima and minima of a function that depends on one or more variables. . For example, determin the optimum analytically . $$ z = z_0 + frac{m}{c}(v_0 + frac{mg}{c})(- exp(-(c/m)t)) - frac{mg}{c}t quad text{where} , g = 9.81, , z_0 = 100, , v_0 = 55, , m = 80, , c = 15 $$ Definition.1.1. Optimization Problem . Following problem that find optimal solution $ mathbf{x}$ are called optimization problem. . $$ mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , f( mathbf{x}) $$where $ mathbf{x}$ is optimization variable, $f : mathbb{R}^n rightarrow mathbb{R}$ is objective function. In this situation, $ mathbf{x}^*$ is called optimal solution. . It is very difficult to solve general optimization problem. Many optimization method involve some compromise, e.g., very long computation time, or not always finding the solution. However, certain problems can be solved efficiently and reliably. For example, . Least-squares problems | Linaer programming problems | Convex optimization problems | . 1.1. Classification of Optimization Method . There are some kinds of optimization method like . Search | Least-Squares | Linear Programming/Nonlinear Optimization | Convex Optimization | Gradient based Optimization | . Above methods are not separated by analytical method and numerical method. . 1.2. Search based Algorithm . Search based algorithms are simplest method in optimization theory. It just computes the objective function value for many $x$ candidates and finds the minimum point. Examples of algorithms are as follows. . Grid Search | Golden-Section Search | . These algorithms can be used effectively for single variable functions, but for high-dimensional multivariate functions, the amount of computation increases exponentially. And also, the solution does not guarantee a global optimum. . 1.3. Least-Squares(Mature technology) . Form like . $$ mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , || A mathbf{x} - mathbf{b} ||_2^2 $$are called least-squares problem. Optimal solution can be obtained analytically e.g., $ mathbf{x}^* = (A^T A)^{-1} A^T mathbf{b}$. There are reliable and efficient algorithms and software that have $n^2k , (A in mathbb{R}^{k times n})$ time complexity. It can increase flexibility by few standard techniques like including weights, adding regularization terms. . 1.4. Linear Programming(Mature technology)/Nonlinear Programming . Form like . $$ mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , mathbf{c}^T mathbf{x} quad s.t. quad mathbf{a}_i^T mathbf{x} le mathbf{b}_i, , i = 1, cdots,m $$are called linear programming. There is no analytical formula for solution, but there are reliable and efficient algorithms and software that have $n^2m$ time complexity. . Form like . $$ mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , f( mathbf{x}) quad s.t. quad g_i( mathbf{x}) le 0, , h_j( mathbf{x}) = 0, , i=1, cdots,p, , j=1, cdots,q $$are called nonlinear programming. There are local optimization methods and global optimization methods. . 1.5. Convex Optimization . Form like . $$ mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , f( mathbf{x}) quad s.t. quad g_i( mathbf{x}) le 0, , h_j( mathbf{x}) = 0, , i=1, cdots,p, , j=1, cdots,q, , f , text{is a convex function.} $$This problem includes least-squares problems and linear programming as special cases. $g_i$ is called an inequality constraint function and $h_j$ is called an equality constraint function. There are no analytical solutions, but reliable and efficient algorithms that have $ text{max} {n^3, n^2p, n^2q }$ time complexity roughly. . Using convex optimization often difficult to recognize. However, there are many tricks for transforming problems into convex form. Many problems can be solved via convex optimization. . 1.6. Gradient based Optimization . A gradient based optimization is an algorithm to solve problems of the form $$ mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , f( mathbf{x}) $$ with the search directions defined by the gradient of the function at the current point. .",
            "url": "https://ndo04343.github.io/blog/optimization-theory/2022/01/26/ch01-introduction.html",
            "relUrl": "/optimization-theory/2022/01/26/ch01-introduction.html",
            "date": " • Jan 26, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "[LinearAlgebra] CH03. Quadratic Form",
            "content": "3.1. Quadratic Form . Definition.3.1. Symmetric Matrix and Orthogonal Matrix . $$ text{If} , A^T = A , text{, then} , A , text{is a symetric matrix.} $$$$ text{If} , A^T = A^{-1} , text{, then} , A , text{is an orthogonal matrix.} $$ Theorem.3.1. . $n times n$ 대칭행렬 $A$에 대한 이차형식 $q( mathbf{x}) = mathbf{x}^T A mathbf{x}$는 적당한 직교행렬 $P$를 이용하여 변수벡터를 $ mathbf{x} = P mathbf{y}$로 변환하여, . $$ g( mathbf{y}) = lambda_1 y_1^2 + lambda_2 y_2^2 + cdots + lambda_n y_n^2 $$로 고칠 수 있다. 단, $ mathbf{y} = [y_1 , y_2 , cdots , y_n]^T in mathbb{R}^n$이고, $ lambda_1, , lambda_2, , cdots, , lambda_n$은 $q( mathbf{x})$의 계수행렬 $A$의 고유치이다. . Proof. Trivial. . Definition.3.2. Problem of Principal Axes and Standard Form . Theorem.3.1. 에서 이차형식 . $$ q( mathbf{x}) = mathbf{x}^T A mathbf{x} quad cdots (1) $$을 $ mathbf{x}$의 좌표 $(x_1, , x_2, , cdots, , x_n)$을 적당히 변환하여 . $$ g( mathbf{y}) = lambda_1 y_1^2 + lambda_2 y_2^2 + cdots + lambda_n y_n^2 quad cdots (2) $$로 고치는 문제는 응용상 매우 중요하다. 이 문제를 이차형식의 주축문제(problem of principal axes)라고 한다. 그리고 이차형식 $(2)$를 이차형식 $(1)$의 표준형(standard form)이라고 한다. . Theorem.3.2. . 계수행렬이 $n times n$ 대칭행렬 $A$인 이차형식 $f( mathbf{x}) = mathbf{x}^T A mathbf{x}$의 변수벡터 $ mathbf{x}$를 직교행렬 $P$로 변환하여 표준형으로 고쳤을 때, 그 계수들 중에서 양인 것의 개수 $p$와 음인 것의 개수 $q$는 $f( mathbf{x})$를 표준형으로 고치는 방법에 관계없이 일정하다. . Proof. Trivial. . Definition.3.3. Positive Definite Quadratic Form and Matrix . 계수행렬이 대칭행렬 $A$인 이차형식 $f( mathbf{x}) = mathbf{x}^T A mathbf{x}$를 표준형으로 고쳐서, 그 계수들 중에서 양인 것의 개수를 $p$, 음인 것의 개수를 $q$라고 할 때, $(p, q)$를 $f( mathbf{x})$ 또는 $A$의 부호수(number of sign)라고 하고, . $$ sgn(A) = (p, q) $$와 같이 나타낸다. 또, 모든 $ mathbf{x} neq mathbf{0}$에 대하여 $f( mathbf{x}) = mathbf{x}^TA mathbf{x} &gt; 0$일 때, $f( mathbf{x})$를 양정치이차형식(positive definite quadratic form)이라고 하고, $A$를 양정치행렬(positive definite matrix)이라고 한다. . Theorem.3.3. . $n times n$ 대칭행렬을 $A$의 고유치가 $ lambda_1, , lambda_2, , cdots, , lambda_n( lambda_1 ge lambda_2 ge cdots ge lambda_n)$이라고 하고, $ mathbf{x} in mathbb{R}^n$를 임의의 단위벡터($|| mathbf{x}|| = 1)$라고 하면 다음 사항들이 성립한다. . $ lambda_n le mathbf{x}^T A mathbf{x} le lambda_1$ | 단위벡터가 $ mathbf{x}$가 $ lambda_i$에 대응되는 $A$의 고유벡터이면 $ mathbf{x}^T A mathbf{x} = lambda_i$ | . Proof. Trivial. . Theorem.3.4. . $n$차 정방행렬에 대하여 다음은 동치이다. . $A$는 양정치행렬이다. | $A$의 모든 고유치가 양수이다. | $A = S^2$($S$는 양정치행렬)으로 나타난다. | $A = B^TB$($B$는 정칙행렬)으로 나타난다. | Proof. Trivial. . 3.2. Classification of Quadratic Lines and Plane . 3.1.에서 다룬 내용은 이차곡선의 분류 문제로 연결된다. 쓸 일이 없으니까 간단하게 종합정리만하면 다음과 같다. . 이차곡선 . $|A| &gt; 0$ 일 때, $| bar{A}| &gt; 0$ : 허타원(Imaginary ellipse) | $| bar{A}| &lt; 0$ : 타원(Ellipse) | $| bar{A}| = 0$ : 점타원 | | $|A| &lt; 0$ 일 때, $| bar{A}| neq 0$ : 쌍곡선(Hyperbola) | $| bar{A}| = 0$ : 서로 만나는 두 직선 | | $|A| = 0$ 일 때, $| bar{A}| neq 0$ : 포물선(Parabola) | $| bar{A}| = 0$ 일 때, $rank( bar{A}) = 2$ 일 때, $sgn( bar{A}) = (2, 0)$ : $ emptyset$ | $sgn( bar{A}) = (1, 1)$ : 나란한 두 직선 | | $rank( bar{A}) = 1$ : 일치하는 두 직선 | | | 이차곡면 . $|A| &gt; 0$ $| bar{A}| &gt; 0$ : 허타원면(Imaginary ellipsoid) | $| bar{A}| &lt; 0$ : 타원면(Ellipsoid) | $| bar{A}| = 0$ : 점타원면 | | $|A| &lt; 0$ $| bar{A}| &gt; 0$ : 일엽쌍곡선(Hyperboloid of one sheet) | $| bar{A}| &lt; 0$ : 이엽쌍곡선(Hyperboloid of two sheets) | | $|A| = 0$ 일 때, $| bar{A}| neq 0$ : 쌍곡포물면 | $| bar{A}| = 0$ 일 때, $rank(A) = 3$ : 이차추면 | $rank(A) = 2$ 일 때, $rank( bar{A}) = 4$ : 타원포물선 | $rank( bar{A}) = 3$ 일 때, $sgn(A) = (2, 0)$ : 타원주면 | $sgn(A) = (1, 1)$ : 쌍곡주면 | | $rank( bar{A}) = 2$ : 서로 만나는 두 평면 | | $rank(A) = 1$ 일 때, $rank( bar{A}) = 3$ : 포물주면 | $rank( bar{A}) = 2$ 일 때, $sgn(A) = (1, 1)$ : 나란한 두 평면 | $sgn(A) = (2, 0)$ : $ emptyset$ | | $rank( bar{A}) = 1$ : 하나의 평면 | | | |",
            "url": "https://ndo04343.github.io/blog/linear-algebra/2022/01/25/ch03-quadratic-form.html",
            "relUrl": "/linear-algebra/2022/01/25/ch03-quadratic-form.html",
            "date": " • Jan 25, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "[LinearAlgebra] CH02. Linear Transformation",
            "content": "2.0. Elementary Definitions and Theorems . Definition.2.1. Linear Transformation . $ mathbb{K} $상의 벡터공간 $V$와 $W$에 대해서, 다음 두 조건을 만족하는 함수 $T , : , V rightarrow W$를 linear transformation이라고 한다. . $ ^ forall mathbf{x} mathbf{y} in V, , ^ forall alpha in mathbb{K}, $ . $ T( mathbf{x} + mathbf{y}) = T( mathbf{x}) + T( mathbf{y}) $ | $ T( alpha mathbf{x}) = alpha T( mathbf{x}) $ | 특히, 일차변환 $T , : , V rightarrow W$가 전단사일때, 이를 linear isomorphism이라 한다. 그리고 이 때의 벡터공간 $V$와 $W$는 linear isomorphic이라고 하고, $ V cong W $로 나타낸다. . EX) . Zero transformation | Identity transformation | Inverse transformation of linear transformation $ T $(Not always). | Matrix transformation | . $ text{Let} , A = [a_{ij}]_{m times n} , text{for} , a_{ij} in mathbb{K} $. $ text{If} , ^ forall mathbf{x} in mathbb{K}^n, , T_A( mathbf{x}) = A mathbf{x} $, . $$ T_A , : , mathbb{K}^n rightarrow mathbb{K}^m $$ Differential transformation | Definite integral transformation | . 2.1. Symmetric Matrix and Orthogonal Matrix . Definition.2.2. Symmetric Matrix and Orthogonal Matrix . $$ text{If} , A^T = A , text{, then} , A , text{is a symetric matrix.} $$$$ text{If} , A^T = A^{-1} , text{, then} , A , text{is an orthogonal matrix.} $$ Theorem.2.1. . $ text{Let} , Q_1 = [q_{ij}^{(1)}]_{n times n} , text{and} , Q_2 = [q_{ij}^{(2)}]_{n times n}$. . $$ text{If} , Q_1, , Q_2 , text{are orthogonal matrices, then the followings are true.} $$ $ text{Every pairs of columns are orthogonal.} $ | $ text{Every columns are unit vector.} $ | $ Q_1Q_2 , text{is also orthogonal matrix.} $ | $ |Q| = 1 , text{or} , |Q| = -1 $ | . Proof. Trivial. . Theorem.2.2. . $ text{Let} , A , text{be a matrix.}$. . $$ text{Then,} , A^TA , text{is a symmetric matrix.} $$ $ (i, i) text{-element of} , A^TA = A_{C_i}^TA_{C_i} ge 0 $ | . Proof. Trivial. . 2.2. Eigenvalue and Eigenvector . Consider $T_A , : , V rightarrow V$ and following vector equation. . $$ T_A( mathbf{x}) = A mathbf{x} = lambda mathbf{x} quad text{for} , lambda in mathbb{R}. $$$ text{Since} , (A - lambda I_n) mathbf{x} = mathbf{0} Leftrightarrow A mathbf{x} = lambda mathbf{x}, $ $ text{by Basic Theorem of Algebra, above equation have} , n , text{complex solutions in} , V. $ . Definition.2.3. Eigenvalue and Eigenvector . $ text{Let} , T_A , : , V rightarrow V , text{and} , text{correspond with matrix} , A$. . For $ lambda in mathbb{K}, , T( mathbf{x}) = lambda mathbf{x}$ is called characteristic equation of linear transform $T$ . | In $T( mathbf{x}) = lambda mathbf{x}$, $ lambda$ is called eigenvalue, and corresponding $ mathbf{x}( neq mathbf{0}, in V)$ is called eigenvector . | Theorem.2.3. . $n$차 정방행렬 A와 정칙행렬 $N$에 대해서 $A, A^T, N^{-1}AN$의 고유치는 일치한다. . Proof. Trivial. . Theorem.2.4. . $ text{Let} , A = [a_{ij}]_{n times n}.$ . $$ begin{matrix} prod_{k = 1}^{n} lambda_k = |A| sum_{k = 1}^{n} lambda_k = sum_{k = 1}^{n} a_{kk} end{matrix} $$Proof. Trivial. . 2.3. Diagonalization . 유한차원 벡터공간 $V$의 일차변환 $T:V rightarrow V$에 대해서, $T$의 행렬을 대각행렬로 만드는 $V$의 기저가 존재할까? . 위의 문제는 theorem.2.3. 에 의해서 다음 문제와 동치이다. . 주어진 실정방행렬 $A$에 대해서, $N^{-1}AN$이 대각행렬이 되는 정칙행렬 $N$이 존재하는가? (복소정방행렬 $A$에 대해서는 $ bar{N}^{-1}AN$에 대해서 따진다.) . Definition.2.3. Diagonalizable . $ text{Let} , A = [a_{ij}]_{n times n} , text{for} , a_{ij} in mathbb{R}$. $ text{If} , exists , text{invertible matrix} , N in mathbb{M}_{n times n}( mathbb{R}) quad s.t. quad N^{-1}AN = diag(d_1, d_2, cdots, d_n),$ $ text{then} , A , text{is diagonalizable by} , N$. . Theorem.2.5. . $ text{Let} , A = [a_{ij}]_{n times n} , text{for} , a_{ij} in mathbb{R}, , lambda_1, lambda_2, cdots, lambda_n , text{are eigenvalues of matrix} , A, , text{and} , mathbf{x}_1, mathbf{x}_2, cdots, mathbf{x}_n , text{are corresponding eigenvectors of eigenvalues}$. $ text{Assume that} , { mathbf{x}_1, mathbf{x}_2, cdots, mathbf{x}_n} , text{are ordered basis of} , mathbb{R}^n$. $ text{Let} , N = [ mathbf{x}_1 , mathbf{x}_2 , cdots , mathbf{x}_n]$. $ text{Then} , N , text{is invertible and} , A , text{is diagonalizable by} , N$. $ text{That is}$ . $$ N^{-1}AN = diag( lambda_1, lambda_2, cdots, lambda_n) $$. . $ text{If} , lambda_1, lambda_2, cdots, lambda_n , text{are different with each other, then the eigenvectors} , mathbf{x}_1, mathbf{x}_2, cdots, mathbf{x}_n , text{are linearly independent and diagonalizable.} $ | $ text{Eigenvector can be multiplied any scalar except zero.} $ | . Proof. Trivial. . Theorem.2.6. . $ text{Let} , A , text{be a diagonalizable real matrix.}$ $ text{Then}$ . $$ A^k = N^{-1}D^kN quad text{for} , k in mathbb{Z}. $$Proof. Trivial. . 유한차원 내적공간 $V$의 일차변환 $T:V rightarrow V$에 대해서, $T$의 행렬을 대각행렬로 만드는 $V$의 정규직교기저가 존재할까? . 위의 문제는 theorem.2.3. 에 의해서 다음 문제와 동치이다. . 주어진 실정방행렬 $A$에 대해서, $P^{-1}AP$이 대각행렬이 되는 직교행렬 $P$가 존재하는가? (복소정방행렬 $A$에 대해서는 $ bar{P}^{-1}AP$에 대해서 따진다.) . Definition.2.4. Orthogonally Diagonalizable . 실정방행렬 $A$가 직교행렬 $P$에 의해서 대각화되면 A는 orthogonally diagonalizable이라고 한다. . Theorem.2.7. . 실대칭행렬 $A$에 대해서, 서로 다른 고유치에 대응되는 고유벡터는 직교한다. . Proof. $ text{Let} , A , text{be orthogonally diagonalizable and} , lambda_1, lambda_2, cdots, lambda_n, mathbf{x}_1, mathbf{x}_2, cdots, mathbf{x}_n , text{are eigenvalues and corresponding eigenvectors.} $ $ text{For} , i neq j, $ $$ begin{matrix} lambda_i mathbf{x}_i^T mathbf{x}_j &amp;= (A mathbf{x}_i)^T mathbf{x}_j &amp;= mathbf{x}_i^T A^T mathbf{x}_j &amp;= mathbf{x}_i(A mathbf{x}_j) &amp;= mathbf{x}_i^T ( lambda_j mathbf{x}_j) end{matrix} $$ . $ text{Therefore,} , ( lambda_i - lambda_j) mathbf{x}_i^T mathbf{x}_j = 0. $ $ text{Since} , lambda_i - lambda_j neq 0, mathbf{x}_i^T mathbf{x}_j = 0.$ . $$ therefore quad mathbf{x}_i perp mathbf{x}_j , text{for} , i neq j quad blacksquare$$ . Theorem.2.8. . $n$차 실정방행렬 $A$에 대해서, $A$가 직교대각화가능일 필요충분조건은 $A$가 대칭행렬이다. . Proof. Trivial. . 2.3. Singular Value Decomposition . Definition.2.5. Positive Definite . $ text{Let} , A , text{be a symmetric matrix.} $ . $$ text{If} , ^ forall mathbf{x} neq mathbf{0}, , mathbf{x}^T A mathbf{x} &gt; 0, , text{then} , A , text{is called positive definite.} $$$$ text{If} , ^ forall mathbf{x} neq mathbf{0}, , mathbf{x}^T A mathbf{x} ge 0, , text{then} , A , text{is called positive semidefinite.} $$$$ text{If} , ^ forall mathbf{x} neq mathbf{0}, , mathbf{x}^T A mathbf{x} &lt; 0, , text{then} , A , text{is called negative definite.} $$$$ text{If} , ^ forall mathbf{x} neq mathbf{0}, , mathbf{x}^T A mathbf{x} le 0, , text{then} , A , text{is called negative semidefinite.} $$ Theorem.2.9. . 대칭행렬 $A$에 대하여 다음이 성립한다. . $A$가 Positive definite면 모든 $A$의 모든 고윳값은 양수이다. | $A$가 Positive semidefinite면 모든 $A$의 모든 고윳값은 음이 아닌 수수이다. | $A$가 Negative definite면 모든 $A$의 모든 고윳값은 음수이다. | $A$가 Negative semidefinite면 모든 $A$의 모든 고윳값은 양이 아닌 실수이다. | . Proof. Chapter03 이후 . . Application.2.1. Eigenvalue Decomposition(EVD) . By above theorems. . Application.2.2. Singular Value Decomposition(SVD) . Any matrix $A_{m times n}$ can be decomposed as $ A = U_{m times m} Sigma_{m times n} {V_{n times n}}^T $ where $ AA^T = U Sigma Sigma^T U^T, , A^TA = V Sigma^T Sigma V^T $ . .",
            "url": "https://ndo04343.github.io/blog/linear-algebra/2022/01/25/ch02-linear-transformation.html",
            "relUrl": "/linear-algebra/2022/01/25/ch02-linear-transformation.html",
            "date": " • Jan 25, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "[LinearAlgebra] CH01. Linear Equations and Inverse Matrices",
            "content": "1.0. Elementary Definitions and Theorems . 이 노트에서 사용하는 수학 표기는 표기법을 따른다. | 기본적인 행렬의 연산과 성질들은 생략한다. | . Definition.1.1. Matrix . 임의의 자연수 $m$과 $n$에 대하여 $mn$개의 수 $(a_{ij} in mathbb{K}, , i = 1, 2, cdots, m, , j = 1, 2, cdots, n)$를 다음과 같이 배열한 도식(diagram) $A$를 $m times n$ 행렬(matrix)이라고 한다. . $$ A = begin{bmatrix} a_{11} &amp; a_{12} &amp; cdots &amp; a_{1j} &amp; cdots &amp; a_{1n} a_{21} &amp; a_{22} &amp; cdots &amp; a_{2j} &amp; cdots &amp; a_{2n} vdots &amp; vdots &amp; ddots &amp; vdots &amp; &amp; vdots a_{i1} &amp; a_{i2} &amp; cdots &amp; a_{ij} &amp; cdots &amp; a_{in} vdots &amp; vdots &amp; &amp; vdots &amp; ddots &amp; vdots a_{m1} &amp; a_{m2} &amp; cdots &amp; a_{mj} &amp; cdots &amp; a_{mn} end{bmatrix} $$행렬 $A$를 간단히 $A = [a_{ij}]$ 또는 $A = [a_{ij}]_{m times n}$과 같이 나타낸다. . 이 글에서 행렬과 관련된 기본 정의와 정리들은 위의 표기를 따른다. . 1.1. System of Linear Equations . Definition.1.2. Augmented Matrices and Coefficient Matrices . $n$개의 미지수 $x_1, , x_2, , cdots, , x_n$에 관한 연립일차방정식 . $$ begin{cases} a_{11}x_1 + a_{12}x_2 + cdots + a_{1n}x_n = b_1 a_{21}x_1 + a_{22}x_2 + cdots + a_{2n}x_n = b_2 ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; vdots a_{m1}x_1 + a_{m2}x_2 + cdots + a_{mn}x_n = b_m end{cases}, $$에 대하여 . $$ A = begin{bmatrix} a_{11} &amp; a_{12} &amp; cdots &amp; a_{1n} a_{21} &amp; a_{22} &amp; cdots &amp; a_{2n} vdots &amp; vdots &amp; ddots &amp; vdots a_{m1} &amp; a_{m2} &amp; cdots &amp; a_{mn} end{bmatrix}_{m times n}, mathbf{x} = begin{bmatrix} x_{1} x_{2} vdots x_{n} end{bmatrix}_{n times 1}, text{and} , mathbf{b} = begin{bmatrix} b_{1} b_{2} vdots b_{m} end{bmatrix}_{m times 1} $$로 둘때, 연립일차방정식을 행렬을 써서 나타내면 . $$ A mathbf{x} = mathbf{b} $$이다. 이때, 연립일차방정식에 대응되는 행렬 $[A quad mathbf{b}]_{m times (n + 1)}$를 주어진 연립일차방정식의 첨가행렬(Augmented matrix)이라고 하고 $A$를 연립일차방정식의 계수행렬(Coeffficient matrix)이라고 한다. . Theorem.1.1. . $A = [a_{ij}]_{n times n}, text{and} , B = [b_{ij}]_{n times n}$에 대하여 다음이 성립한다. . $$ qquad (AB)^ top = B^ top A^ top $$Proof. Trivial. . Theorem.1.2. . Let $A$ is invertible and have $n$-degree. . $$ qquad (A^ top)^{-1} = (A^{-1})^ top $$Proof. Trivial. . Theorem.1.3. . Square matrix $A$에 대하여, 다음이 성립한다. . $$ text{(1)} quad text{If} , [A quad I] cong_R [I quad P], , text{then} , P = A^{-1} text{(2)} quad text{If} , begin{bmatrix} A I end{bmatrix} cong_C begin{bmatrix} I P end{bmatrix}, , text{then} , P = A^{-1} $$Proof. Trivial. . Theorem.1.4. . Square matrix $A, , B, , C$에 대하여, $A$가 가역행렬일때, . $$ text{If} , [A quad B] cong_R [I quad C], , text{then} , C = A^{-1}B $$Proof. Trivial. . Theorem.1.5. . $ text{If} , A , text{and} , B , text{are} , n text{-degree square matrices, then followings are true.} $ . $$ text{(1)} quad Tr(AB) = Tr(BA) text{(2)} quad Tr(PAP^{-1}) = Tr(A) $$Proof. Trivial. . Definition.1.3. Gauss-Jordan Elimination and Gauss Elimination . Forward substitution을 통해서 어떤 행렬을 ref(row echelon form)으로 변환하는 과정을 Gauss Elimination이라고 하고, Back substitution까지 진행하여 어떤 행렬을 rref(reduced row echelon form으로 변환하는 과정을 Gauss-Jordan Elimination이라고 한다. . Definition.1.4. Matrix Decomposition . Matrix Decomposition은 어떤 Matrix를 여러 행렬들의 곱으로 표현하는 것을 의미한다. 이는 Computational convenience, analytic simplicity를 목적으로 가진다. . . Application.1.1. LU Decomposition . $ text{Let} , m times n , text{matrix} , A.$ $ text{By Gauss Elimination, We can get matrix} , U. $ $ text{Then} , E_p cdots E_1 A = U $ $ L = (E_p cdots E_1)^{-1} $ . Ex) . $$ A = begin{bmatrix} 2 &amp; 4 &amp; -1 &amp; 5 &amp; -2 -4 &amp; -5 &amp; 3 &amp; -8 &amp; 1 2 &amp; -5 &amp; -4 &amp; 1 &amp; 8 -6 &amp; 0 &amp; 7 &amp; -3 &amp; 1 end{bmatrix} cong begin{bmatrix} 2 &amp; 4 &amp; -1 &amp; 5 &amp; -2 0 &amp; 3 &amp; 1 &amp; 2 &amp; -3 0 &amp; -9 &amp; -3 &amp; -4 &amp; 10 0 &amp; 12 &amp; 4 &amp; 12 &amp; -5 end{bmatrix} cong begin{bmatrix} 2 &amp; 4 &amp; -1 &amp; 5 &amp; -2 0 &amp; 3 &amp; 1 &amp; 2 &amp; -3 0 &amp; 0 &amp; 0 &amp; 2 &amp; 1 0 &amp; 0 &amp; 0 &amp; 4 &amp; 7 end{bmatrix} cong begin{bmatrix} 2 &amp; 4 &amp; -1 &amp; 5 &amp; -2 0 &amp; 3 &amp; 1 &amp; 2 &amp; -3 0 &amp; 0 &amp; 0 &amp; 2 &amp; 1 0 &amp; 0 &amp; 0 &amp; 0 &amp; 5 end{bmatrix} = U $$$$ L_{C_1} = frac{1}{2} begin{bmatrix} 2 -4 2 -6 end{bmatrix}, , L_{C_2} = frac{1}{3} begin{bmatrix} 0 3 -9 12 end{bmatrix}, , L_{C_3} = frac{1}{2} begin{bmatrix} 0 0 2 4 end{bmatrix}, , L_{C_4} = frac{1}{5} begin{bmatrix} 0 0 0 5 end{bmatrix}. $$ $$ text{Therefore, } , L = begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 -2 &amp; 1 &amp; 0 &amp; 0 1 &amp; -3 &amp; 1 &amp; 0 -3 &amp; 4 &amp; 2 &amp; 1 end{bmatrix} $$Suppose we have to solve $ A mathbf{x} = mathbf{b} $. We can get $ A = LU $. Then . $$ L mathbf{d} = mathbf{b} U mathbf{x} = mathbf{d} $$ import numpy as np from scipy.linalg import lu A = np.array([[2, 4, -1, 5, -2], [-4, -5, 3, -8, 1], [2, -5, -4, 1, 8], [-6, 0, 7, -3, 1]]) P, L, U = lu(A) # P is permutating matrix(pivoting) P, L, U . (array([[0., 0., 0., 1.], [0., 1., 0., 0.], [0., 0., 1., 0.], [1., 0., 0., 0.]]), array([[ 1. , 0. , 0. , 0. ], [ 0.66666667, 1. , 0. , 0. ], [-0.33333333, 1. , 1. , 0. ], [-0.33333333, -0.8 , -0. , 1. ]]), array([[-6.00000000e+00, 0.00000000e+00, 7.00000000e+00, -3.00000000e+00, 1.00000000e+00], [ 0.00000000e+00, -5.00000000e+00, -1.66666667e+00, -6.00000000e+00, 3.33333333e-01], [ 0.00000000e+00, 0.00000000e+00, -8.88178420e-16, 6.00000000e+00, 8.00000000e+00], [ 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, -8.00000000e-01, -1.40000000e+00]])) . . 1.2. Determinants and Inverse Matrices . Theorem.1.6. . $n$차 정방행렬 $A = [a_{ij}]$에 대하여 $|A| = |A^ top|$ . Proof. Trivial. . Theorem.1.7. . $n$차 정방행렬 $A = [a_{ij}]$와 수 $k$에 대하여 $|kA| = k^n|A|$ . Proof. Trivial. . Theorem.1.8. . 두 행렬의 곱의 행렬식은 각각의 행렬식의 곱과 같다. . Proof. Trivial. . Definition.1.5. Minor Determinant and Cofactor . $n$차 정방행렬 $A = [a_{ij}]$의 $i$번째 행과 $j$번째 열을 삭제하고 남은 $(n - 1)$차 정방행렬의 행렬식을 행렬 $A$의 $(i, j)-$Minor determinant이라고 하고 $M_{ij}$로 쓴다. 또한 $M_{ij}$에 부호 $(-1)^{i + j}$를 곱한 $(-1)^{i + j}M_{ij}$를 행렬 $A$의 $(i, j)$-Cofactor라고 하고 $C_{ij}$로 쓴다. 즉, . $$ C_{ij} = (-1)^{i + j}M_{ij} $$ Definition.1.6. Adjoint Matrix and Cofactor Matrix . $n$차 정방행렬 $A = [a_{ij}]$에서 $(i, j)$-Cofactor $C_{ij}$를 $(i, j)$ 성분으로 하는 행렬 $C = [C_{ij}]$의 전치행렬 $C^ top$를 $A$의 Adjoint matrix또는 여인수 행렬(Cofactor matrix)이라고 하며, $ text{adj}(A)$로 나타낸다. . $$ text{adj}(A) = [C_{ij}]^ top = begin{bmatrix} C_{11} &amp; C_{21} &amp; cdots &amp; C_{n1} C_{12} &amp; C_{22} &amp; cdots &amp; C_{n2} vdots &amp; vdots &amp; ddots &amp; vdots C_{1n} &amp; C_{2n} &amp; cdots &amp; C_{nn} end{bmatrix} $$ Theorem.1.9. . $n$차 정방행렬 $A = [a_{ij}]$에 대하여 다음이 성립한다. . $ text{(1)} quad text{adj}(A)A = A text{adj}(A) = |A|I_n$ $ text{(2)} quad A$가 정칙행렬일 필요충분조건은 $|A| neq 0$이다. $ text{(3)} quad A$가 정칙행렬이면 $A^{-1} = frac{1}{|A|} text{adj}(A)$이다. . Proof. Trivial. . Definition.1.7. Minor Determinant and Rank . $m times n$행렬 $A$의 $m - r$개 행과 $n - r$개의 열을 제거하고 남은 $r$차의 정방행렬에 대한 행렬식을 $A$의 $r$차의 소행렬식(Minor determinant)이라고 한다. 만약 $A$의 $r$차 소행렬식 중에서 $0$이 아닌 것이 적어도 하나 존재하고, $r$보다 큰 모든 $s(r &lt; s)$에 대하여 소행렬식이 모두 $0$일때, $r$을 행렬 $A$의 계수(rank)라고 하고 $$ text{rank}(A) = r $$ 이라고 나타낸다. . Theorem.1.10. . $ text{If} , A cong A^ prime, , text{then} , text{rank}(A) = text{rank}(A^ prime)$ . Proof. Trivial. . Theorem.1.11. . 행렬의 계수(rank)는 그 행렬의 rref에서 leading 1의 개수와 같다. . Proof. Trivial. . 1.3. Rank and Solution of System of Linear Equation . 연립 일차 방정식의 해의 종류는 다음과 같다. . No solution | Particular solution(Only one solution) | Infinitely many solutions | . Definition.1.8. Homogeneous System of Linear Equations . 미지수 $x_1, , x_2, , cdots, , x_n$에 관한 연립일차방정식 . $$ begin{cases} a_{11}x_1 + a_{12}x_2 + cdots + a_{1n}x_n = 0 a_{21}x_1 + a_{22}x_2 + cdots + a_{2n}x_n = 0 ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; vdots a_{m1}x_1 + a_{m2}x_2 + cdots + a_{mn}x_n = 0 end{cases} $$을 Homogeneous System of Linear Euation이라고 한다. . Homogeneous System of Linear Equation은 다음과 같은 해를 가진다. . Trivial solution | Infinitely many solution including trivial solution. | . Theorem.1.12. . $ text{In homogeneous system of linear equations, if} , m &lt; n, , text{then the system have infinitely many solutions.} $ . Proof. Trivial. . Theorem.1.13. . 행렬 $A_{m times n}, , mathbf{b}_{m times 1} = [b_1 , b_2 , cdots , b_m]^ top, , mathbf{x}_{n times 1} = [x_1 , x_2 , cdots , x_n]^ top$에 대하여 연립일차방정식 $A mathbf{x} = mathbf{b}$가 해를 가질 필요충분조건은 . $$ text{rank}(A) = text{rank}([A quad B]) $$이며, $A mathbf{x} = mathbf{b}$가 해를 가지는 경우는 다음과 같다. . $ text{If} , text{rank}(A) = text{rank}([A quad B]) = n, , text{then} , A mathbf{x} = mathbf{b} , text{have a particular solution.}$ | $ text{If} , text{rank}(A) = text{rank}([A quad B]) &lt; n, , text{then} , A mathbf{x} = mathbf{b} , text{have infinitely many solutions.}$ | . Remark) . $ text{If} , text{rank}(A) neq text{rank}([A quad B]), , text{then} , nexists , text{solution of} , A mathbf{x} = mathbf{b}$. | $ text{If} , text{rank}(A) = text{rank}([A quad B]) = n, , text{then} , exists^1 , text{solution of} , A mathbf{x} = mathbf{b}$. | $ text{If} , text{rank}(A) = text{rank}([A quad B]) &lt; n, , text{then} , exists^ infty , text{solution of} , A mathbf{x} = mathbf{b}$. | Proof. Trivial. . Theorem.1.14. . $ m times n $행렬 $A$에 대하여 만약 $m &lt; n$이면 동차연립일차방정식 $A mathbf{x} = mathbf{0}$는 무한히 많은 해를 가진다. . Proof. Trivial. . Theorem.1.15. . $n$차 정방행렬 $A$를 계수행렬로 가지는 동차연립일차방정식 $A mathbf{x} = mathbf{0}$가 자명해가 아닌 해를 가질 필요충분조건은 $ text{rank}(A) &lt; n$이다. . Proof. Trivial. .",
            "url": "https://ndo04343.github.io/blog/linear-algebra/2022/01/25/ch01-linear-equations-and-inverse-matrices.html",
            "relUrl": "/linear-algebra/2022/01/25/ch01-linear-equations-and-inverse-matrices.html",
            "date": " • Jan 25, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "HEESUNG YANG .",
          "url": "https://ndo04343.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  
  

  

  
  

  

  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ndo04343.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}