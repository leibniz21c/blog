{
  
    
        "post0": {
            "title": "[OptimizationTheory] CH05. Constrained Optimizations",
            "content": "5.1. Introduction . Many practical optimization problems have constraints like equality and inequality constraint. Consider following problem: . $$ mathbf{w}^* = underset{ mathbf{w}}{ mathrm{argmin}} || mathbf{y} - X mathbf{w} ||_2^2 quad s.t. quad || mathbf{w}||_2^2 le 1. $$Above problem is inequality constrained optimization problem, and luckly $MSE$ cost function is convex function. Therefore, we can obtain optimal solution analytically. However, if obtained optimal solution doesn&#39;t satisfy the inequality condition, we have to find another solution. We can define general constained optimization problem like following. . Definition.5.1. Constrained Optimization Problem . Following problem that find optimal solution $ mathbf{x}$ are called constrained optimization problem. . $$ begin{matrix} mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , f( mathbf{x}) quad s.t. quad g_i( mathbf{x}) le 0, , i=1, cdots,m, h_j( mathbf{x}) = 0, , j=1, cdots,k end{matrix} $$where $f: mathbb{R}^n rightarrow mathbb{R}$ is a objective function, $ mathbf{g} : mathbb{R}^n rightarrow mathbb{R}^m$ is a inequality constraint function, and $ mathbf{h}: mathbb{R}^n rightarrow mathbb{R}^k $ is a equality constraint function. . 5.2. Lagrange Multiplier . Definition.5.2. Lagrangian Function . In above constrained optimization problem, there is corresponding Lagrangian function . $$ begin{matrix} mathcal{L}( mathbf{x}, mathbf{ lambda}, mathbf{ mu}) &amp;= f( mathbf{x}) + sum_{i = 1}^{m} lambda_ig_i( mathbf{x}) + sum_{j = 1}^{k} mu_j h_j( mathbf{x}) &amp;= f( mathbf{x}) + mathbf{ lambda}^T mathbf{g}( mathbf{x}) + mathbf{ mu}^T mathbf{h}( mathbf{x}) end{matrix} $$ where $ lambda_i, mu_j$ for $i=1, cdots,m, , j = 1, cdots,k$ are dual variables. . Theorem.5.1. Lagrangian Multiplier . In equality constrained optimization problem, if $ mathbf{x}^* in mathbb{R}^n $ is a local minimum, then there exist $ mathbf{ mu}^* in mathbb{R}^k $ such that . $$ begin{cases} frac{ partial}{ partial mathbf{x}} mathcal{L}( mathbf{x}, mathbf{ mu}) = mathbf{0} frac{ partial}{ partial mathbf{ mu}} mathcal{L}( mathbf{x}, mathbf{ mu}) = mathbf{0} end{cases}. $$And system of equations contain $n + k$ equation. . . Consider following problem from algebra. . Ex) . 2차원 평면상의 원 $x^2 + y^2 = k$과 직선 $y = sqrt{3}x + 4 sqrt{3}$을 지날때, k의 최소를 구하시오. . sol) . $ text{Let} , mathbf{x} = begin{bmatrix} x y end{bmatrix} = begin{bmatrix} x_1 x_2 end{bmatrix} in mathbb{R}^2. $ $ text{Then, this problem can be convert following constrained optimization problem.} $ . $$ mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , mathbf{x}^T mathbf{x} quad s.t. quad g( mathbf{x}) = [ sqrt{3} , - 1] mathbf{x} + 4 sqrt{3} = 0. $$$ text{Then the Lagrangian function of above constrained optimization problem is} $ . $$ mathcal{L}( mathbf{x}, mu) = mathbf{x}^T mathbf{x} + mu ([ sqrt{3} , - 1] mathbf{x} + 4 sqrt{3}). $$ $$ frac{ partial}{ partial mathbf{x}} mathcal{L}( mathbf{x}, mu) = 2 mathbf{x} + mu [ sqrt{3} , -1]^T = mathbf{0} qquad cdots , (1) $$$$ frac{ partial}{ partial mu} mathcal{L}( mathbf{x}, mu) = [ sqrt{3} , -1] mathbf{x} + 4 sqrt{3} = 0 qquad cdots , (2) $$ $ text{By theorem.5.1., solution of above equation is a local minimum of optimization problem.}$ $ text{Also, above Lagrangian function is convex.} quad ( because , text{Property.3.3})$ $ text{By, theorem.3.1.,}$ . $$ therefore quad mathbf{x}^* = begin{bmatrix} - frac{ sqrt{3} mu}{2} frac{ mu}{2} end{bmatrix}, , min k = 12 $$ Of course, it is reasonable to use gradient based optimization when we optimize the Lagrangian function. Above solution is obtained just analytically, not numerical method. . 5.3. Karush-Kuhn-Tucker(KKT) Conditions . Consider following problem. . $$ min x_1^2 + x_2^2 quad s.t. quad x_1 + x_2 = 0 $$In above problem, the value of $ mu$ is zero. This means that the equality constraint doesn&#39;t dependent on optimization problem. In this situation, above constrained optimization problem has equivalence relation with unconstrained optimization; $ min x_1^2 + x_2^2 $. . Theorem.5.2. Karush-Kuhn-Tucker(KKT) Conditions . If $ mathbf{x}^* in mathbb{R}^n $ is a local minimum, then there exist $ mathbf{ lambda}^* in mathbb{R}^m $ and $ mathbf{ mu}^* in mathbb{R}^k $ such that . $ (1) , text{Stationarity} quad frac{ partial}{ partial mathbf{x}} mathcal{L}( mathbf{x}, mathbf{ lambda}, mathbf{ mu}) = mathbf{0} $ . $ (2) , text{Primal feasibility} quad ^ forall i, , g_i( mathbf{x}^*) le 0, , mathbf{h}( mathbf{x}^*) = mathbf{0} $ . $ (3) , text{Complementary slackness} quad ^ forall i, , lambda_i^* g_i( mathbf{x}^*) = 0 $ . $ (4) , text{Dual feasibility} quad ^ forall i, , lambda_i^* ge 0 $ . Remark that if optimal solution isn&#39;t dependent with inequality constraint, then it is same with unconstrained optimization problem, and if optimal solution is dependent with inequality constraint, then optimal solution is on the boundary line of inequatlity constraint. Consider following problems. . $$ min x_1^2 + x_2^2 quad s.t. quad x_1 + x_2 - 1 le 0 qquad cdots , (1) $$$$ min x_1^2 + x_2^2 quad s.t. quad x_1 + x_2 - 1 ge 0 qquad cdots , (2) $$ All constraint conditions are divided into those that affect the result and those that do not. In $(1)$, that is same with . $$ min x_1^2 + x_2^2. $$ . Furthermore, since optimal solution can be in boundary line of inequality constraint, the inequality constraint that affect the result can be converted equality constraint. Therefore, $(2)$ is same with . $$ min x_1^2 + x_2^2 quad s.t. quad x_1 + x_2 - 1 = 0 $$Remark followings. Stationarity condition is because of Lagrange multiplier. Primal feasibility condition is trivial. Complementary slackness condition is derived from Lagrangian multiplier. We can understand that . $$ frac{ partial }{ partial lambda_i} mathcal{L}( mathbf{x}, mathbf{ lambda}, mathbf{ mu}) = g_i( mathbf{x}) $$, and since it can be constraint that doesn&#39;t affect the result, $ lambda_i$ can be zero or not. Therefore, . $$ ^ forall i, , lambda_i^* g_i( mathbf{x}^*) = 0. $$Dual feasibility condition is a condition that guarantees that the KKT condition is the same problem as the inequality constrained optimization problem. . 5.4. Duality . Consider following optimization problem . $$ begin{matrix} mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , f( mathbf{x}) quad s.t. quad g_i( mathbf{x}) le 0, , i=1, cdots,m, h_j( mathbf{x}) = 0, , j=1, cdots,k end{matrix} $$where $f: mathbb{R}^n rightarrow mathbb{R}$ is a convex function, $ mathbf{g} : mathbb{R}^n rightarrow mathbb{R}^m$ is a convex function for each $g_i$, and $ mathbf{h}: mathbb{R}^n rightarrow mathbb{R}^k $ is an affine function. Above optimization problem is called convex. For convex problems. KKT conditions becomes necessary and also sufficient for global optimality. . From now on, we consider above problem with duality. Duality means that the primal problem of optimization problem can view the dual problem. In Lagrangian method, it is called Lagrangian dual problem. . Definition.5.2. Lagrangian Dual Function . $$ mathcal{D}( mathbf{ lambda}, mathbf{ mu}) = underset{ mathbf{x}}{ min} mathcal{L}( mathbf{x}, mathbf{ lambda}, mathbf{ mu}) = underset{ mathbf{x}}{ min} left { f( mathbf{x} + mathbf{ lambda}^T mathbf{h}( mathbf{x}) + mathbf{ mu}^T mathbf{g}( mathbf{x}) right } $$ Theorem.5.3. Lagrange Dual Problem . The problem $$ begin{matrix} mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , f( mathbf{x}) quad s.t. quad g_i( mathbf{x}) le 0, , i=1, cdots,m, h_j( mathbf{x}) = 0, , j=1, cdots,k end{matrix} $$ where $f: mathbb{R}^n rightarrow mathbb{R}$ is a convex function, $ mathbf{g} : mathbb{R}^n rightarrow mathbb{R}^m$ is a convex function for each $g_i$, and $ mathbf{h}: mathbb{R}^n rightarrow mathbb{R}^k $ is an affine function, is equivalent with . $$ underset{ mathbf{ lambda}, mathbf{ mu}}{ max} mathcal{D}( mathbf{ lambda}, mathbf{ mu}) quad text{for} , lambda_i ge 0, , i=1, cdots,m. $$ . In above problem, $ mathcal{D}( mathbf{ lambda}, mathbf{ mu}) le mathcal{L}( mathbf{x}^*, mathbf{ lambda}, mathbf{ mu}) le f( mathbf{x}^*) $ where $ mathbf{x}^*$ is primal optimal. Therefore, $ mathcal{D}( mathbf{ lambda}, mathbf{ mu}) le underset{ lambda_i ge 0, mathbf{ mu}, , text{for} , i=1, cdots,m}{ max} mathcal{D}( mathbf{ lambda}, mathbf{ mu}) le f( mathbf{x}^*)$. $f( mathbf{x}^*)$ is called primal optimal, and $ underset{ lambda_i ge 0, mathbf{ mu}, , text{for} , i=1, cdots,m}{ max} mathcal{D}( mathbf{ lambda}, mathbf{ mu})$ is called dual optimal. . Definition.5.3. Duality . Let $p^*$ be primal optimal and $d^*$ be dual optimal of a dual problem. If $p^* ge d^*$, it is called weak duality, and if $p^* = d^*$, it is called strong duality. And $p^* - d^*$ is called duality gap. . Remark) . Dual function is a concave function(proved by definition of concave). | For a convex optimization problem, the strong duality usually holds (not always, i.e., When Slater’s condition is not satisfied.) | . . Ex) . $$ mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} mathbf{x}^T mathbf{x} quad s.t. quad A mathbf{x} = mathbf{b} $$",
            "url": "https://ndo04343.github.io/blog/optimization-theory/2022/02/11/ch05-constrained-optimizations.html",
            "relUrl": "/optimization-theory/2022/02/11/ch05-constrained-optimizations.html",
            "date": " • Feb 11, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "[NDIR] Identification of MIR-Flickr Near-Duplicate Images",
            "content": "1. Overview . They give a new test set based on a large, serendipitously selected collection(mir-flickr 1M) of high quality images. It disclose a set of 1,958 near-duplicate clusters from within the set. The main contribution of this is the identification of these images, which may then be used by other authors to make comparisons as they see fit. . 2. Problem . The problems of NDIR tasks are, . In near-duplicate image retrieval, it is still(2015) uncommon for methods to be objectively compared with each other, because of a lack of any good framework in which to do so. | Published sets of near-duplicate images exists, but are typically small, specialist, or generated. | How can we quantitative comparison of different similarity functions for the detection of near-duplicate images. | . To measure recall and precision for a given similarity function, . It requires very large sets of benchmark images which should be no bias as to the type of images in the collection, nor the method with which the near-duplicates have been formed. | It requires a known ground truth of near-duplicates. | . 3. Related Works . (Kim et al., 2010) : Near duplicate keyframes detecting, used TRECVID dataset. | (Chm et al., 2007; Jinda-Apiraksa et al., 2013) : Duplicate or near-duplicate of a query image detection. . &quot;We do not have access to ground-truth data for our experiments, since we are not aware of any large public corpus in which near duplicate images have been annotated.&quot; . &quot;Although the target application of this dataset is image retrieval, it was selected due to the lack of other appropriate datasets&quot; . | (Jegou et al., 2008) :Near-duplicate imate detection with using INRIA Holidays Dataset, there is no information about duplicate or near-duplicate images.4. (Nister and Stewenius, 2006) : Near-duplicate image detection with using composed of 10,200 images in sets of 4 images of one object/scene, there is no information about how similar two sets might be and whether or not they should be considered duplicate or near-duplicate. | (Jinda-Apiraksa et al., 2013) : Near-duplicate image detection, in this publication, the authors give a dataset specifically built for near-duplicate image detection. | 4. Contributions . 4.1. Basic Definitions . Definition(ground truth) of kinds of near duplicates are followings: . Duplicate : same pair | Identical near-duplicate(IND) : derived from the same digital source after applying some transformations transformations : any operation which has been performed using a standard image editor, with the intent of making cosmetic changes. | . | Non-identical near-duplicate(NIND) : share the same scenes and objects | . For the puposes of benchmarking, they choose to primarily use the IND definition for thw following reasons: . (almost) objective | such pairs are relatively common in the MIR Flickr set | the resulting relation is an equivalence relation | For some reason pairs of images were classified in three categories: . IND, as defined above | pairs of images which are strikingly visually similar, but are not IND as defined, also not NIND | Image(filename=&quot;figure2-1.png&quot;, width=224, height=224) . Image(filename=&quot;figure2-2.png&quot;, width=224, height=224) . Above pictures are strikingly similar, but not near-duplicate. . pairs which do not meet either criteria | 4.2. Methodology . Used characterisations : . Eh(MPEG-7 Edge Histograms) | Ht(MPEG-7 Heterogeneous Textures) | Cs(MPEG-7 Colour Structures) | pHash(Perceptual Hashing) | Used distance metrics : . Man($L_1$ distance) | Euc(euclidean distance) | Cos(cosine distance) | Sed(structural entropic distance) | Ham(hamming distance over bitmaps) | They recommend cosine distance as proper distance metric. . 4.2. Cluster Identification . Remove perfect duplicate images(in this procedure, 378 images removed) For each characterisations For each similarity function Do Threshold-limited Nearest-Neighbour Search(10^12 times comparison, use Chavez et al., 2001; Zezula et al., 2006) Each of the resulting image pairs was inspected by them . At point of publication, this has resulted, . 1,958 near-duplicate clusters within the set, containing a total of 4,071 images(the mean size of a cluster is 2.08) | 543 pairs of strikingly similar | . 5. Semantic Comparison . They already of course have results for the functions used to construct the set. Each x-axis of each graphs are threshold. Sensitivity and PPV mean just recall and precision. Here are brief: . $$ begin{matrix} text{Precision} = frac{ text{TP}}{ text{TP} + text{FP}} text{Recall} = frac{ text{TP}}{ text{TP} + text{FN}} end{matrix} $$ Image(filename=&quot;figure3.png&quot;, width=350) . 6. Conclusions . Using a number of different near-duplicate finders, they have found around 2,000 pairs of images conforming to an objective definition of near-duplicate, almost all the pairs that exist within the collection. Also, they said . The exhaustive search for near-duplicates within the set will of course never be finished:any updates will be gratefully received by the authors, and communicated onwards through our website. If we research on NDIR with MFND dataset, we must contact them. . 7. Comments . How can we benchmark ndir algorithm with this dataset? Is there a example? | .",
            "url": "https://ndo04343.github.io/blog/near-duplicate%20image%20detection/paper-review/2022/02/11/Identification-of-MIR-Flickr-Near-Duplicate-Images.html",
            "relUrl": "/near-duplicate%20image%20detection/paper-review/2022/02/11/Identification-of-MIR-Flickr-Near-Duplicate-Images.html",
            "date": " • Feb 11, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "[OptimizationTheory] CH04. Gradient based Optimizations",
            "content": "4.1. Introduction . Gradient descent is an iterative first-order optimisation algorithm used to find a local minimum of a given function. . 4.2. Gradient Descent . Algorithm.4.1. Gradient Descent . Gradient descent is an iterative method to find a stationary point of an unconstraint optimization problem : $$ theta^* = underset{ mathbf{ theta}} { arg min} L ( mathbf{ theta}) $$ . $$ L( mathbf{ theta} + eta mathbf{d}) approx L( mathbf{ theta}) + eta bigtriangledown _ mathbf{ theta} ^ T L( mathbf{ theta} ) mathbf{d} quad where quad eta &gt; 0, , left | mathbf{d} right | = 1 $$ $$ L( mathbf{ theta} + eta mathbf{d}) - L( mathbf{ theta}) approx eta bigtriangledown _ mathbf{ theta} ^ T L( mathbf{ theta} ) mathbf{d} = eta cos{( phi)} left | bigtriangledown _ mathbf{ theta} ^ T L( mathbf{ theta} ) right | $$ Find the directional vector $ mathbf{d}$ that minimizes $ L( mathbf{ theta} + eta mathbf{d} ) - L( mathbf{ theta}) le 0 $ . $$ cos{( phi)} = -1 , rightarrow , mathbf{d} = - frac{ bigtriangledown_ mathbf{ theta} L( mathbf{ theta} ) }{ left | bigtriangledown _ mathbf{ theta} L( mathbf{ theta} ) right | } $$ $$ therefore mathbf{ theta} + eta mathbf{d} = mathbf{ theta} - eta frac{ bigtriangledown_ mathbf{ theta} L( mathbf{ theta} ) }{ left | bigtriangledown _ mathbf{ theta} L( mathbf{ theta} ) right | } = mathbf{ theta} - alpha bigtriangledown_ mathbf{ theta} L( mathbf{ theta} ) $$ 4.3. 4 Types of Gradient Descent . $ (i) , text{Standard (or steepest) Gradient Descent} $ $$ mathbf{w} leftarrow mathbf{w} - eta bigtriangledown mathbb{E}[J( mathbf{w})] $$ . Practically infeasible | Thus, we need distribution about data $ mathbf{x}$ (Contradiction) | So, We can use sample mean | . $ (ii) , text{Stochastic(online) Gradient Descent} $ $$ mathbf{w} leftarrow mathbf{w} - eta bigtriangledown J_i( mathbf{w}) $$ . Simple to implement | Effective for large-scale problem | Much less memory | Unstable(zigzaging) | Purpose : We just consider one of data | It can be convergent. But there is little unstable. | . $ (iii) , text{Batch gradient Descent} $ $$ mathbf{w} leftarrow eta bigtriangledown sum_{i=1}^{N} J_i ( mathbf{w}) $$ . Accurate estimation of gradients | Parallelization of learning | Large memory | Big time-complexity can be problem in this method.(So slow) | But, there isn&#39;t problem in convergence. | Purpose : We consider all of data! | . $ (vi) , text{Mini-Batch Gradient Descent} $ $$ mathbf{w} leftarrow mathbf{w} - eta bigtriangledown sum_{i in mathfrak{I}}^{N} J_i ( mathbf{w}), quad 1 le left | mathfrak{I} right | le N $$ . Most generalized version | Effective to deal with large | Amount of training data | Purpose : We just consider seveal datas. | . 4.4. Newton&#39;s Method . Newton&#39;s method is zero finding algorithm. Many equations can be solved by this algorithm and bisection search algorithm in numerical analysis. We use this method too because of gradient necessary condition, which is $ nabla L = mathbf{0}$. . Algorithm.4.2. Newton-Rapson Method in Multivariate Function . In gradient updating context, we can find hyperplane of $L$ at $ mathbf{w}_0$ . $$ mathbf{y} = nabla^2 L( mathbf{w})^T ( mathbf{w} - mathbf{w}_0) + nabla L( mathbf{w}_0) $$ And we have to find next $ mathbf{w}$ by obtaining solution of following eqation: . $$ nabla^2 L( mathbf{w})^T ( mathbf{w} - mathbf{w}_0) + nabla L( mathbf{w}_0) = mathbf{0} $$ Therefore, . $$ mathbf{w}_1 = mathbf{w}_0 - H( mathbf{w}_0)^{-1} nabla L( mathbf{w}_0) $$ Actually, we can consider too polynomial approximation like Taylor series expansion. The result is surprising. . $$ L( mathbf{w} + Delta mathbf{w}) approx L( mathbf{w}) + nabla L ( mathbf{w})^T Delta mathbf{w} + frac{1}{2} Delta mathbf{w}^T H( mathbf{w}) Delta mathbf{w} $$ $$ frac{ partial}{ partial Delta mathbf{w}} L( mathbf{w} + Delta mathbf{w}) approx nabla L( mathbf{w}) + H( mathbf{w}) Delta mathbf{w} = mathbf{0} $$ $$ therefore , Delta mathbf{w} = H( mathbf{w})^{-1} nabla L( mathbf{w}) $$ The above result is the same as the result of the Newton Method. . 4.4. Quasi-Newton Method . The inverse of the Hessian matrix appearing in Newton&#39;s method is difficult to use because of its too much computation. By replacing this with an average gradient, the amount of computation can be reduced. Explore the BFGS method. . 4.5. Update Rule with Momentum . We can add a momentum term to the update equation to prevent slowing down of learning or reduce instability of learning. Basic update rule is following: . $$ mathbf{w}_{k + 1} = mathbf{w}_k - eta nabla_ mathbf{w} L( mathbf{w}_k) + gamma mathbf{w}_{k - 1} , text{for} , k ge 2. $$There are various variants of the gradient update algorithm using momentum. . Algorithm.4.4. Nesterov Accelerated Gradient(NAG) . When using Momentum, the direction of the gradient is also slightly shifted in the previous direction. . $$ mathbf{w}_{k + 1} = mathbf{w}_k - eta nabla_ mathbf{w} L( mathbf{w}_k + gamma mathbf{w}_{k - 1}) + gamma mathbf{w}_{k - 1} , text{for} , k ge 2. $$ 4.6. Update Rule with Adaptive Leaning Rate . If the learning rate is too small, the learning time is too long, and if the learning rate is too large, it diverges(zigzagging) and learning is not performed properly. AdaGrad solves this problem through learning rate decay. However, this also has a problem (zero convergence problem), so the following methods are used. . Algorithm.4.5. Adaptive Gradient(AdaGrad) . $$ mathbf{w}_{k + 1} = mathbf{w}_k - frac{ eta}{ sqrt{ epsilon + mathbf{d}_k}} odot nabla_ mathbf{w} L( mathbf{w}_k), , mathbf{d}_k = mathbf{d}_{k - 1} + nabla_ mathbf{w} L( mathbf{w}_k) odot nabla_ mathbf{w} L( mathbf{w}_k) $$ The above algorithm has a fatal flaw. Since $d$ is infinitely increasing, the amount of change in the gradient will converge to zero. . Algorithm.4.6. Root Mean Square Propagation(RMSProp) . $$ mathbf{w}_{k + 1} = mathbf{w}_k - frac{ eta}{ sqrt{ epsilon + mathbf{d}_k}} odot nabla_ mathbf{w} L( mathbf{w}_k), , mathbf{d}_k = gamma mathbf{d}_{k - 1} + (1 - gamma) nabla_ mathbf{w} L( mathbf{w}_k) odot nabla_ mathbf{w} L( mathbf{w}_k) $$ Algorithm.4.7. Adaptive Delta(AdaDelta) . $$ mathbf{w}_{k + 1} = mathbf{w}_k - frac{ sqrt{ epsilon + mathbf{u}_k}}{ sqrt{ epsilon + mathbf{d}_k}} odot nabla_ mathbf{w} L( mathbf{w}_k), , mathbf{d}_k = gamma mathbf{d}_{k - 1} + (1 - gamma) nabla_ mathbf{w} L( mathbf{w}_k) odot nabla_ mathbf{w} L( mathbf{w}_k), , mathbf{u}_k = gamma mathbf{u}_{k - 1} - (1 - gamma) frac{ sqrt{ epsilon + mathbf{u}_{k-1}}}{ sqrt{ epsilon + mathbf{d}_{k-1}}} odot nabla_ mathbf{w} L( mathbf{w}_{k-1}) , text{for} , k ge 2. $$ 4.6. Hybrid Update Rule with Momentum and Adaptive Learning Rate . Adaptive Moment Estimation(Adam) : Momentum + RMSProp | Nesterov-accelerated Adaptive Moment Estimation(NAdam) : NAG + Adam | . 4.7. Laerning Rate Scheduler . In implementations of neural network, the optimizer is important, but the learning rate scheduler is also important. In pytorch implementation, the followings are a commonly used learning rate scheduler. . Constant Learning Rate | LambdaLR | scheduler = LambdaLR(optimizer, lr_lambda = lambda epoch: 0.95 ** epoch) . def func(epoch): if epoch &lt; 40: return 0.5 elif epoch &lt; 70: return 0.5 ** 2 elif epoch &lt; 90: return 0.5 ** 3 else: return 0.5 ** 4 scheduler = LambdaLR(optimizer, lr_lambda = func . StepLR | scheduler = StepLR(optimizer, step_size=200, gamma=0.5) . MultiStepLR | scheduler = MultiStepLR(optimizer, milestones=[200, 350], gamma=0.5) . ExponentialLR | scheduler = ExponentialLR(optimizer, gamma=0.95) . CosineAnnealingLR | scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=0.001) . CosineAnnealingWarmRestarts | scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2, eta_min=0.001) . Custom CosineAnnealingWarmRestarts | import math from torch.optim.lr_scheduler import _LRScheduler class CosineAnnealingWarmUpRestarts(_LRScheduler): def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1): if T_0 &lt;= 0 or not isinstance(T_0, int): raise ValueError(&quot;Expected positive integer T_0, but got {}&quot;.format(T_0)) if T_mult &lt; 1 or not isinstance(T_mult, int): raise ValueError(&quot;Expected integer T_mult &gt;= 1, but got {}&quot;.format(T_mult)) if T_up &lt; 0 or not isinstance(T_up, int): raise ValueError(&quot;Expected positive integer T_up, but got {}&quot;.format(T_up)) self.T_0 = T_0 self.T_mult = T_mult self.base_eta_max = eta_max self.eta_max = eta_max self.T_up = T_up self.T_i = T_0 self.gamma = gamma self.cycle = 0 self.T_cur = last_epoch super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch) def get_lr(self): if self.T_cur == -1: return self.base_lrs elif self.T_cur &lt; self.T_up: return [(self.eta_max - base_lr)*self.T_cur / self.T_up + base_lr for base_lr in self.base_lrs] else: return [base_lr + (self.eta_max - base_lr) * (1 + math.cos(math.pi * (self.T_cur-self.T_up) / (self.T_i - self.T_up))) / 2 for base_lr in self.base_lrs] def step(self, epoch=None): if epoch is None: epoch = self.last_epoch + 1 self.T_cur = self.T_cur + 1 if self.T_cur &gt;= self.T_i: self.cycle += 1 self.T_cur = self.T_cur - self.T_i self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up else: if epoch &gt;= self.T_0: if self.T_mult == 1: self.T_cur = epoch % self.T_0 self.cycle = epoch // self.T_0 else: n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult)) self.cycle = n self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1) self.T_i = self.T_0 * self.T_mult ** (n) else: self.T_i = self.T_0 self.T_cur = epoch self.eta_max = self.base_eta_max * (self.gamma**self.cycle) self.last_epoch = math.floor(epoch) for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()): param_group[&#39;lr&#39;] = lr optimizer = optim.Adam(model.parameters(), lr = 0) scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0=150, T_mult=1, eta_max=0.1, T_up=10, gamma=0.5) .",
            "url": "https://ndo04343.github.io/blog/optimization-theory/2022/02/03/ch04-gradient-based-optimizations.html",
            "relUrl": "/optimization-theory/2022/02/03/ch04-gradient-based-optimizations.html",
            "date": " • Feb 3, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "[Calculus] CH03. Taylor Expansion",
            "content": "3.1. Mutivariate Taylor Polynomial . Definition.3.1. First Order Taylor Polynomial(Linear Approximation) . Let $f: mathbb{R}^n rightarrow mathbb{R}$ be a differentiable function. Then first order taylor polynomial of $f$ at $ mathbf{x}_0$ is . $$ P_1( mathbf{ mathbf{x}}) = f( mathbf{x}_0) + ( mathbf{x} - mathbf{x}_0)^T nabla_ mathbf{x} f( mathbf{x}_0). $$ Definition.3.2. Second Order Taylor Polynomial . Let $f: mathbb{R}^n rightarrow mathbb{R}$ be a twice differentiable function. Then second order taylor polynomial of $f$ at $ mathbf{x}_0$ is . $$ P_2( mathbf{ mathbf{x}}) = f( mathbf{x}_0) + ( mathbf{x} - mathbf{x}_0)^T nabla_ mathbf{x} f( mathbf{x}_0) + frac{1}{2!} ( mathbf{x} - mathbf{x}_0)^T nabla_ mathbf{x}^2 f( mathbf{x}_0) ( mathbf{x} - mathbf{x}_0) $$ We don&#39;t need more than this. .",
            "url": "https://ndo04343.github.io/blog/calculus/2022/02/03/ch03-taylor-expansion.html",
            "relUrl": "/calculus/2022/02/03/ch03-taylor-expansion.html",
            "date": " • Feb 3, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "[OptimizationTheory] CH03. Convex Optimization",
            "content": "3.1. Introduction . Consider the objective function that is differentiable in optimization problem. If $ mathbf{w}^*$ is optimal, $ nabla_ mathbf{w} L$ must be zero vector. This condition is called gradient necessary condition. In optimization problem, we have to find global optimal. However, we only find local optimal everytime. Also, if we find the solution of $ nabla_ mathbf{w} L = mathbf{0}$, there is no guarantee that the solution is a global optimal. . Here is something to think about. . [Dauphin14] Y. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, Y. Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. . Above paper suggest that local minima problem is actually a very rare case that does not occur in a high dimensional space. First reason is gradient necessary condition. Every elements in gradient must be a zero at some $ mathbf{w}^*$. But this is very rare case. Also, in every direction, gradient must form a convex shape in a high-dimensional space. But, the probability of that happening is close to zero. . Intuitively, in high dimensions, the chance that all the directions around a critical point lead upward is exponentially small w.r.t. the number of dimensions, unless the critical point is the global minimum or stands at an error level close to it, i.e., it is unlikely one can find a way to go further down. . In this context, since both local minima and the global minima are same, it can be seen that a convex function can be good loss function. . 3.2. Convex Optimization . Definition.3.1. Convex Set . A set $S$ is said to be convex if . $$ mathbf{x}, mathbf{y} in S, , text{then} , t mathbf{x} + (1 - t) mathbf{y} in S, , text{for} , t in [0, 1] $$ Definition.3.2. Convex Function . A function $f: S rightarrow mathbb{R}$ is said to be convex if . $$ ^ forall mathbf{x}_1, mathbf{x}_2 in S, , f(t mathbf{x}_1 + (1 - t) mathbf{x}_2) le tf( mathbf{x}_1) + (1 - t)f( mathbf{x}_2) , text{for} , t in [0, 1] $$where $S$ is convex subset of a real vector space. . Theorem.3.1. . In convex function, some local minimum is a global minimum. . Proof. Trivial(proof by contradiction). . Theorem.3.2. . A twice differentiable function $f: mathbb{R}^n rightarrow mathbb{R}$ is a convex function if and only if $ nabla^2 f succeq 0$. . Proof. Suppose $f$ has a positive semidefinite hessian matrix. Then for some $ mathbf{x}_0, mathbf{x}_1$ in the domain, and $t in [0, 1]$, we have . $$ g(t) = f(t mathbf{x}_0 + (1- t) mathbf{x}_1) $$which have the first and second derivative . $$ begin{matrix} frac{dg}{dt} = ( mathbf{x}_0 - mathbf{x}_1)^T nabla_ mathbf{x} f(t mathbf{x}_0 + (1 - t) mathbf{x}_1) frac{d^2g}{dt^2} = ( mathbf{x}_0 - mathbf{x}_1)^T nabla_ mathbf{x}^2 f(t mathbf{x}_0 + (1 - t) mathbf{x}_1)( mathbf{x}_0 - mathbf{x}_1) end{matrix} $$ Since the hessian matrix of $f$ is positive semidefinite, $ frac{d^2g}{dt^2} ge 0$ for $t in [0, 1]$. Then we can get . $$ begin{matrix} g(0) ge g(t) + g^ prime(t)(-t) g(1) ge g(t) + g^ prime(t)(1 - t) end{matrix} quad ( because , text{Taylor&#39;s theorem}) $$ Then $$ g(t) le tg(1) + (1 - t)g(0) $$ . $$ therefore , ^ forall mathbf{x}_0, mathbf{x}_1 in D, , f(t mathbf{x}_0 + (1 - t) mathbf{x}_1) le tf( mathbf{x}_0) + (1 - t) mathbf{x}_1 quad blacksquare $$ Definition.3.3. Convex Optimization Problem . A convex optimization problem is an optimization problem in which the objective function is a convex function and the feasible set is a convex set. . 3.3. Properties . Followings are convex function: . Exponential function | Power function(in some case) | Logarithmic function | Affine function | Quadratic function | Mean square error | Max function | Norm function | Softmax function | . Theorem.3.3. . Let $f( mathbf{x}) = h(g( mathbf{x})) = h(g_1( mathbf{x}), cdots, g_k( mathbf{x}))$ where $g: mathbb{R}^n rightarrow mathbb{R}^k, , h : mathbb{R}^k rightarrow mathbb{R}, , text{and} , f : mathbb{R}^n rightarrow mathbb{R}$. Then . $f$ is convex if $h$ is convex and nondecreasing in each argument, g is convex. | $f$ is convex if $h$ is convex and nonincreasing in each argument, g is concave. | . Proof. Trivial(by chain rule and above theorems). .",
            "url": "https://ndo04343.github.io/blog/optimization-theory/2022/02/03/ch03-convex-optimization.html",
            "relUrl": "/optimization-theory/2022/02/03/ch03-convex-optimization.html",
            "date": " • Feb 3, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "[LinearAlgebra] CH03. Quadratic Form",
            "content": "3.1. Quadratic Form . Definition.3.1. . $n times n$ 대칭행렬 $A = [a_{ij}]$와 벡터 $ mathbf{x} = [x_1 , x_2 , cdots , x_n]^T in mathbb{R}^n$에 대해서 . $$ q( mathbf{x}) = mathbf{x}^T A mathbf{x} = sum_{i = 1}^{n} sum_{j = 1}^{n} a_{ij} x_i x_j $$일 때, $A$를 $q( mathbf{x})$의 계수행렬(coefficient matrix)이라고 하며 $q( mathbf{x})$를 계수행렬이 $A$인 이차형식(quadratic form)이라고 하고, $x_1, , x_2, , cdots , , x_n $을 $q( mathbf{x})$의 변수(variable)라고 하며 $ mathbf{x}$를 변수 벡터라고 한다. . Remark . $q( mathbf{x}) = mathbf{x}^ mathbf{x} A mathbf{x} = &lt; mathbf{x}, A mathbf{x} &gt; = sum_{i = 1}^n a_{ii}x_i^2 + 2 sum_{i neq j} a_{ij} x_i x_j$ | $n$개의 실변수를 가지는 이차방정식은 다음과 같다. | $$ begin{matrix} f( mathbf{x}) = sum_{i = 1}^n sum_{j = 1}^n a_{ij}x_ix_j + sum_{i = 1}^n b_ix_i + c = 0 quad text{or} f( mathbf{x}) = mathbf{x}^TA mathbf{x} + mathbf{b}^T mathbf{x} + c = 0 end{matrix} $$위의 식에서 $ mathbf{b}^T mathbf{x} = sum_{i = 1}^n b_ix_i$를 $f( mathbf{x})$의 일차형식(linear form)이라고 한다. . $A$가 대칭행렬로 규정된 이유는 다음과 같다. | 만약 $A$가 정방행렬이라면, $A = S + T quad text{where} , S , text{is a symetric matrix, and} , T , text{is a skew-symetric matrix.}$ $$ begin{matrix} mathbf{x}^TT mathbf{x} &amp;= ( mathbf{x}^TT mathbf{x})^T quad ( because , mathbf{x}^TT mathbf{x} in R) &amp;= mathbf{x}^TT^T mathbf{x} &amp;= - mathbf{x}^TT mathbf{x} end{matrix} $$ . $$ therefore quad q( mathbf{x}) = mathbf{x}^TA mathbf{x} = mathbf{x}^TS mathbf{x} $$ Theorem.3.1. . $n times n$ 대칭행렬 $A$에 대한 이차형식 $q( mathbf{x}) = mathbf{x}^T A mathbf{x}$는 적당한 직교행렬 $P$를 이용하여 변수벡터를 $ mathbf{x} = P mathbf{y}$로 변환하여, . $$ g( mathbf{y}) = lambda_1 y_1^2 + lambda_2 y_2^2 + cdots + lambda_n y_n^2 $$로 고칠 수 있다. 단, $ mathbf{y} = [y_1 , y_2 , cdots , y_n]^T in mathbb{R}^n$이고, $ lambda_1, , lambda_2, , cdots, , lambda_n$은 $q( mathbf{x})$의 계수행렬 $A$의 고유치이다. . Proof. Trivial. . Definition.3.2. Problem of Principal Axes and Standard Form . Theorem.3.1. 에서 이차형식 . $$ q( mathbf{x}) = mathbf{x}^T A mathbf{x} quad cdots (1) $$을 $ mathbf{x}$의 좌표 $(x_1, , x_2, , cdots, , x_n)$을 적당히 변환하여 . $$ g( mathbf{y}) = lambda_1 y_1^2 + lambda_2 y_2^2 + cdots + lambda_n y_n^2 quad cdots (2) $$로 고치는 문제는 응용상 매우 중요하다. 이 문제를 이차형식의 주축문제(problem of principal axes)라고 한다. 그리고 이차형식 $(2)$를 이차형식 $(1)$의 표준형(standard form)이라고 한다. . Theorem.3.2. . 계수행렬이 $n times n$ 대칭행렬 $A$인 이차형식 $f( mathbf{x}) = mathbf{x}^T A mathbf{x}$의 변수벡터 $ mathbf{x}$를 직교행렬 $P$로 변환하여 표준형으로 고쳤을 때, 그 계수들 중에서 양인 것의 개수 $p$와 음인 것의 개수 $q$는 $f( mathbf{x})$를 표준형으로 고치는 방법에 관계없이 일정하다. . Proof. Trivial. . Definition.3.3. Positive Definite Quadratic Form and Matrix . 계수행렬이 대칭행렬 $A$인 이차형식 $f( mathbf{x}) = mathbf{x}^T A mathbf{x}$를 표준형으로 고쳐서, 그 계수들 중에서 양인 것의 개수를 $p$, 음인 것의 개수를 $q$라고 할 때, $(p, q)$를 $f( mathbf{x})$ 또는 $A$의 부호수(number of sign)라고 하고, . $$ sgn(A) = (p, q) $$와 같이 나타낸다. 또, 모든 $ mathbf{x} neq mathbf{0}$에 대하여 $f( mathbf{x}) = mathbf{x}^TA mathbf{x} &gt; 0$일 때, $f( mathbf{x})$를 양정치이차형식(positive definite quadratic form)이라고 하고, $A$를 양정치행렬(positive definite matrix)이라고 한다. . Theorem.3.3. . $n times n$ 대칭행렬을 $A$의 고유치가 $ lambda_1, , lambda_2, , cdots, , lambda_n( lambda_1 ge lambda_2 ge cdots ge lambda_n)$이라고 하고, $ mathbf{x} in mathbb{R}^n$를 임의의 단위벡터($|| mathbf{x}|| = 1)$라고 하면 다음 사항들이 성립한다. . $ lambda_n le mathbf{x}^T A mathbf{x} le lambda_1$ | 단위벡터가 $ mathbf{x}$가 $ lambda_i$에 대응되는 $A$의 고유벡터이면 $ mathbf{x}^T A mathbf{x} = lambda_i$ | . Proof. Trivial. . Theorem.3.4. . $n$차 정방행렬에 대하여 다음은 동치이다. . $A$는 양정치행렬이다. | $A$의 모든 고유치가 양수이다. | $A = S^2$($S$는 양정치행렬)으로 나타난다. | $A = B^TB$($B$는 정칙행렬)으로 나타난다. | Proof. Trivial. . 3.2. Classification of Quadratic Lines and Plane . 3.1.에서 다룬 내용은 이차곡선의 분류 문제로 연결된다. 쓸 일이 없으니까 간단하게 종합정리만하면 다음과 같다. . 이차곡선 . $|A| &gt; 0$ 일 때, $| bar{A}| &gt; 0$ : 허타원(Imaginary ellipse) | $| bar{A}| &lt; 0$ : 타원(Ellipse) | $| bar{A}| = 0$ : 점타원 | | $|A| &lt; 0$ 일 때, $| bar{A}| neq 0$ : 쌍곡선(Hyperbola) | $| bar{A}| = 0$ : 서로 만나는 두 직선 | | $|A| = 0$ 일 때, $| bar{A}| neq 0$ : 포물선(Parabola) | $| bar{A}| = 0$ 일 때, $rank( bar{A}) = 2$ 일 때, $sgn( bar{A}) = (2, 0)$ : $ emptyset$ | $sgn( bar{A}) = (1, 1)$ : 나란한 두 직선 | | $rank( bar{A}) = 1$ : 일치하는 두 직선 | | | 이차곡면 . $|A| &gt; 0$ $| bar{A}| &gt; 0$ : 허타원면(Imaginary ellipsoid) | $| bar{A}| &lt; 0$ : 타원면(Ellipsoid) | $| bar{A}| = 0$ : 점타원면 | | $|A| &lt; 0$ $| bar{A}| &gt; 0$ : 일엽쌍곡선(Hyperboloid of one sheet) | $| bar{A}| &lt; 0$ : 이엽쌍곡선(Hyperboloid of two sheets) | | $|A| = 0$ 일 때, $| bar{A}| neq 0$ : 쌍곡포물면 | $| bar{A}| = 0$ 일 때, $rank(A) = 3$ : 이차추면 | $rank(A) = 2$ 일 때, $rank( bar{A}) = 4$ : 타원포물선 | $rank( bar{A}) = 3$ 일 때, $sgn(A) = (2, 0)$ : 타원주면 | $sgn(A) = (1, 1)$ : 쌍곡주면 | | $rank( bar{A}) = 2$ : 서로 만나는 두 평면 | | $rank(A) = 1$ 일 때, $rank( bar{A}) = 3$ : 포물주면 | $rank( bar{A}) = 2$ 일 때, $sgn(A) = (1, 1)$ : 나란한 두 평면 | $sgn(A) = (2, 0)$ : $ emptyset$ | | $rank( bar{A}) = 1$ : 하나의 평면 | | | |",
            "url": "https://ndo04343.github.io/blog/linear-algebra/2022/02/02/ch03-quadratic-form.html",
            "relUrl": "/linear-algebra/2022/02/02/ch03-quadratic-form.html",
            "date": " • Feb 2, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "[Calculus] CH02. Vector Calculus",
            "content": "2.0. Overview . For $f: mathbb{R}^n rightarrow mathbb{R}$, we know that $ nabla f : mathbb{R}^n rightarrow mathbb{R}^n$. For example, . $$ begin{matrix} nabla_ mathbf{w} ( mathbf{x}^T mathbf{w}) = mathbf{x} nabla_ mathbf{w} ( mathbf{w}^T R mathbf{w}) = (R + R^T) mathbf{w} end{matrix} $$In this time, we consider about derivative of vector functions, matrix functions, and second derivative of multi-variate functions. . 2.1. Gradient of Various Functions . Definition.2.1. Gradient of a Scalar Function with Respect to a Vector . Let the function $ f : mathbb{R}^m rightarrow mathbb{R} $ be. $$ frac{ partial f( mathbf{w})}{ partial mathbf{w}} = nabla_ mathbf{w} f( mathbf{w}) = begin{bmatrix} frac{ partial f( mathbf{w})}{ partial w_1} frac{ partial f( mathbf{w})}{ partial w_2} cdots frac{ partial f( mathbf{w})}{ partial w_m} end{bmatrix}_{m times 1}. $$ . Example.2.1. . $ nabla_ mathbf{w} ( mathbf{x}^T mathbf{w}) = mathbf{x}$ | $ nabla_ mathbf{w} ( mathbf{w}^T R mathbf{w}) = (R + R^T) mathbf{w}$ | . Definition.2.2. Gradient of a Vector Function with Respect to a Vector . Let the function $ mathbf{g} : mathbb{R}^m rightarrow mathbb{R}^n $ be. If $ mathbf{g}( mathbf{w}) = $ $ begin{bmatrix} g_1( mathbf{w}) g_2( mathbf{w}) vdots g_n( mathbf{w}) end{bmatrix} $ and $ w , text{be a} , m times 1 , text{vector}$, . $$ frac{ partial mathbf{g}( mathbf{w})}{ partial mathbf{w}} = nabla_ mathbf{w} mathbf{g}( mathbf{w}) = begin{bmatrix} nabla_ mathbf{w} g_1( mathbf{w})^T nabla_ mathbf{w} g_2( mathbf{w})^T vdots nabla_ mathbf{w} g_n( mathbf{w})^T end{bmatrix} = begin{bmatrix} frac{ partial g_1( mathbf{w})}{ partial w_1} &amp; frac{ partial g_1( mathbf{w})}{ partial w_2} &amp; cdots &amp; frac{ partial g_1( mathbf{w})}{ partial w_m} frac{ partial g_2( mathbf{w})}{ partial w_1} &amp; frac{ partial g_2( mathbf{w})}{ partial w_2} &amp; cdots &amp; frac{ partial g_2( mathbf{w})}{ partial w_m} vdots &amp; vdots &amp; ddots &amp; vdots frac{ partial g_n( mathbf{w})}{ partial w_1} &amp; frac{ partial g_n( mathbf{w})}{ partial w_2} &amp; cdots &amp; frac{ partial g_n( mathbf{w})}{ partial w_m} end{bmatrix}_{n times m}, , text{and it is called Jacobian.} $$ . Example.2.2. . $ nabla_ mathbf{w} (A mathbf{w}) = A$ | . Definition.2.3. Hessian Matrix of a Scalar Function with Respect to a Vector . Let the function $ f : mathbb{R}^m rightarrow mathbb{R} $ and $ mathbf{w} , text{be a} , m times 1 $ vector. . $$ H = frac{ partial}{ partial mathbf{w}} nabla_ mathbf{w}f( mathbf{w}) = nabla_ mathbf{w}^2 f( mathbf{w}) = begin{bmatrix} frac{ partial ^2f( mathbf{w})}{ partial w_1^2} &amp; frac{ partial ^2f( mathbf{w})}{ partial w_1 partial w_2} &amp; cdots &amp; frac{ partial ^2f( mathbf{w})}{ partial w_1 partial w_m} frac{ partial ^2f( mathbf{w})}{ partial w_2 partial w_1} &amp; frac{ partial ^2f( mathbf{w})}{ partial w_2^2} &amp; cdots &amp; frac{ partial ^2f( mathbf{w})}{ partial w_2 partial w_m} vdots &amp; vdots &amp; ddots &amp; vdots frac{ partial ^2f( mathbf{w})}{ partial w_m partial w_1} &amp; frac{ partial ^2f( mathbf{w})}{ partial w_m partial w_2} &amp; cdots &amp; frac{ partial ^2f( mathbf{w})}{ partial w_m^2} end{bmatrix}_{m times m} $$ Hassian matrix is symmetric. | . Definition.2.4. Gradient of a Scalar Function with Respect to a Matrix . Let the function $ f : mathbb{R}^{m times n} rightarrow mathbb{R} $ be a function. . $$ frac{ partial f(W)}{ partial W} = nabla_W f(W) = begin{bmatrix} frac{ partial f(W)}{ partial w_{11}} &amp; frac{ partial f(W)}{ partial w_{12}} &amp; cdots &amp; frac{ partial f(W)}{ partial w_{1n}} frac{ partial f(W)}{ partial w_{21}} &amp; frac{ partial f(W)}{ partial w_{22}} &amp; cdots &amp; frac{ partial f(W)}{ partial w_{2n}} vdots &amp; vdots &amp; ddots &amp; vdots frac{ partial f(W)}{ partial w_{m1}} &amp; frac{ partial f(W)}{ partial w_{m2}} &amp; cdots &amp; frac{ partial f(W)}{ partial w_{mn}} end{bmatrix}_{m times n} $$ Example.2.3. . $ nabla_ mathbf{W} (trace(XW)) = X^T$ | Let $W$ be a $n times m$ matrix and $R$ be a $n times n$ matrix. $$ nabla_W (W^TRW) = (R + R^T)W $$ | $ nabla_W ( log |W|) = (W^{-1})^T$ | .",
            "url": "https://ndo04343.github.io/blog/calculus/2022/02/02/ch02-vector-calculus.html",
            "relUrl": "/calculus/2022/02/02/ch02-vector-calculus.html",
            "date": " • Feb 2, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "[Calculus] CH01. Basic Calculus",
            "content": "1.0. Functions . We&#39;ll use numerator-layout notation. . Definition.1.1. A Function $f$ of $n$ Variables . A function $f$ of $n$ variables is a rule that assigns a number $z = f(x_1, x_2, cdots, x_n)$ to an $n$-tuple $(x_1, x_2, cdots, x_n)$ of real numbers. . $$ f : mathbb{R}^n rightarrow mathbb{R} $$ Definition.1.2. Limits of a Function of $n$ Variables . Let $f$ be a function of $n$ variables whose domain $D$ includes points arbitrarily close to $ mathbf{a}$. Then we say that the limit of $f( mathbf{x})$ as $ mathbf{x}$ approaches $ mathbf{a}$ is $L$ and we write . $$ lim_{ mathbf{x} rightarrow mathbf{a}} f( mathbf{x}) = L $$if for every number $ epsilon &gt; 0$ there is a corresponding number $ delta &gt; 0$ such that if $ mathbf{x} in D$ and $0 &lt; || mathbf{x} - mathbf{a}|| &lt; delta$ then $|f( mathbf{x}) - L| &lt; epsilon $. . Definition.1.3. Continuity . A function $f$ of two variables is called continuous at $ mathbf{a}$ if . $$ lim_{ mathbf{x} rightarrow mathbf{a}} f( mathbf{x}) = f( mathbf{a}). $$We say $f$ is continuous on $D$ if $f$ is continuous at every point $ mathbf{a}$ in $D$. . 1.1. Partial Derivatives and Gradients . Definition.1.4. Partial Derivative of a Function of $n$ Variables . If $f$ is a function of $n$ variables, its partial derivatives are . $$ frac{ partial}{ partial x_i} f( mathbf{x}) = lim_{h rightarrow 0} frac{f(x_1, cdots, x_i + h, cdots, x_n) - f(x_1, cdots, x_i, cdots, x_n)}{h} quad text{for} , i=1,2, cdots,n $$Also, a vector of partial derivatives as above is called gradient; . $$ nabla_ mathbf{x} f( mathbf{x}) = ( frac{ partial}{ partial x_1} f( mathbf{x}), cdots, frac{ partial}{ partial x_n} f( mathbf{x})) $$ Theorem.1.1. . Let $V = {f | f: mathbb{R}^n rightarrow mathbb{R} , text{and} , f , text{is differentiable} }, , W = {f | f: mathbb{R}^n rightarrow mathbb{R}^n }$, and $D: V rightarrow W, , D(f) = nabla f$. Then the differential transformation $D$ is a linear transformation and it is called differential operator $D$. . Proof. $$ text{For} , 0 in V, , D(0) = mathbf{0} in W. qquad cdots , (1) $$ . $$ begin{aligned} text{For} , f, g in V, , D(f + g) &amp;= nabla_ mathbf{x} (f + g) &amp;= nabla_ mathbf{x} f + nabla_ mathbf{x} g &amp;= D(f) + D(g) qquad cdots , (2) end{aligned} $$ $$ begin{aligned} text{For} , c in mathbb{R}, , f in V, , D(cf) &amp;= nabla_ mathbf{x} f &amp;= c nabla_ mathbf{x} f &amp;= c D(f) qquad cdots , (3) end{aligned} $$ $$ therefore , D , text{is a linear transformation} quad ( because , (1), , (2), , text{and} , (3)) quad blacksquare $$ Definition.1.5. Linear Approximation . The linear approximation of $f$ at $ mathbf{x}_0$ is given as . $$ f( mathbf{x}) approx f( mathbf{x}_0) + nabla_ mathbf{x} f( mathbf{x}_0)^T ( mathbf{x} - mathbf{x}_0). $$ Definition.1.6. Differentiable . If $y = f( mathbf{x})$, $f$ is differentiable at $ mathbf{x}_0$ if $ Delta u$ can be expressed in the form . $$ Delta y = nabla_ mathbf{x} f( mathbf{x}_0)^T begin{bmatrix} Delta x_1 Delta x_2 vdots Delta x_n end{bmatrix} + mathbf{ epsilon}^T begin{bmatrix} Delta x_1 Delta x_2 vdots Delta x_n end{bmatrix} , text{where} , mathbf{ epsilon} rightarrow mathbf{0} , text{as} , begin{bmatrix} Delta x_1 Delta x_2 vdots Delta x_n end{bmatrix} rightarrow mathbf{0}. $$ Theorem.1.2. . If the gradient $ nabla_ mathbf{x} f$ exist near $ mathbf{x}_0$ and are continuous at $ mathbf{x}_0$, then $f$ is differentiable at $ mathbf{x}_0$. . Proof. . Trivial. . Definition.1.7. Differentials . For a differentiable function $y = f( mathbf{x})$, let the differentials $dx_i$ for $i=1,2, cdots,n$ be independent variables. Then the total differential $dy$ is defined by . $$ dy = nabla_ mathbf{x} f^T begin{bmatrix} dx_1 dx_2 vdots dx_n end{bmatrix}. $$ 1.2. Properties of Partial Derivatives . Theorem.1.3. . Let $f: mathbb{R}^n rightarrow mathbb{R}, , g: mathbb{R}^n rightarrow mathbb{R},$ and $c in mathbf{R}$. . Constant Multiple Rule: $ nabla_ mathbf{x} f(c mathbf{x}) = c nabla_ mathbf{x} f( mathbf{x})$ | Sum Rule: $ nabla_ mathbf{x} { f( mathbf{x}) + g( mathbf{x}) } = nabla_ mathbf{x} f( mathbf{x}) + nabla_ mathbf{x} g( mathbf{x})$ | Product Rule: $ nabla_ mathbf{x} { f( mathbf{x})g( mathbf{x}) } = ( nabla_ mathbf{x} f( mathbf{x}))g( mathbf{x}) + f( mathbf{x}) ( nabla_ mathbf{x} g( mathbf{x}))$ | Quotient Rule: $ nabla_ mathbf{x} left { frac{f( mathbf{x})}{g( mathbf{x})} right } = frac{( nabla_ mathbf{x} f( mathbf{x}))g( mathbf{x}) - f( mathbf{x}) ( nabla_ mathbf{x} g( mathbf{x}))}{g( mathbf{x})^2}$ | Proof. Trivials. . Theorem.1.4. (Chain Rule) . If $y = f( mathbf{x})$ and $ mathbf{x} = mathbf{g}(t)$ are differentiable functions, then . $$ frac{dy}{dt} = nabla_ mathbf{x} f^T begin{bmatrix} frac{dx_1}{dt} frac{dx_2}{dt} vdots frac{dx_n}{dt} end{bmatrix} $$Proof. . $$ begin{matrix} Delta y = nabla_ mathbf{x} f( mathbf{x})^T begin{bmatrix} Delta x_1 Delta x_2 vdots Delta x_n end{bmatrix} + mathbf{ epsilon}^T begin{bmatrix} Delta x_1 Delta x_2 vdots Delta x_n end{bmatrix} frac{ Delta y}{ Delta t} = nabla_ mathbf{x} f( mathbf{x})^T begin{bmatrix} frac{ Delta x_1}{ Delta t} frac{ Delta x_2}{ Delta t} vdots frac{ Delta x_n}{ Delta t} end{bmatrix} + mathbf{ epsilon}^T begin{bmatrix} frac{ Delta x_1}{ Delta t} frac{ Delta x_2}{ Delta t} vdots frac{ Delta x_n}{ Delta t} end{bmatrix} lim_{ Delta t rightarrow 0} frac{ Delta y}{ Delta t} = lim_{ Delta t rightarrow 0} left { nabla_ mathbf{x} f( mathbf{x})^T begin{bmatrix} frac{ Delta x_1}{ Delta t} frac{ Delta x_2}{ Delta t} vdots frac{ Delta x_n}{ Delta t} end{bmatrix} + mathbf{ epsilon}^T begin{bmatrix} frac{ Delta x_1}{ Delta t} frac{ Delta x_2}{ Delta t} vdots frac{ Delta x_n}{ Delta t} end{bmatrix} right } therefore quad frac{dy}{dt} = nabla_ mathbf{x} f( mathbf{x})^T begin{bmatrix} frac{dx_1}{dt} frac{dx_2}{dt} vdots frac{dx_n}{dt} end{bmatrix} qquad ( therefore , mathbf{ epsilon}, begin{bmatrix} Delta x_1 Delta x_2 vdots Delta x_n end{bmatrix} rightarrow mathbf{0} , text{as} , Delta t rightarrow 0) quad blacksquare end{matrix} $$ Theorem.1.5. (Chain Rule) . If $y = f( mathbf{x})$ and $ mathbf{x} = mathbf{g}( mathbf{t})$ are differentiable functions that $f: mathbb{R}^n rightarrow mathbb{R}, , mathbf{g}: mathbb{R}^m rightarrow mathbb{R}^n $, then . $$ nabla_ mathbf{t} f( mathbf{x}) = begin{bmatrix} frac{ partial}{ partial t_1}f( mathbf{x}) frac{ partial}{ partial t_t}f( mathbf{x}) vdots frac{ partial}{ partial t_m}f( mathbf{x}) end{bmatrix}_{m times 1}, quad frac{ partial}{ partial t_i}f( mathbf{x}) = nabla_ mathbf{x} f( mathbf{x})^T begin{bmatrix} frac{ partial x_1}{ partial t_i} frac{ partial x_2}{ partial t_i} vdots frac{ partial x_n}{ partial t_i} end{bmatrix} $$Proof. . Trivials. . Theorem.1.6. . Followings are true. . $ nabla_ mathbf{w} ( mathbf{x}^T mathbf{w}) = mathbf{x} $ | $ nabla_ mathbf{w} ( mathbf{w}^T R mathbf{w}) = (R + R^T) mathbf{w} $ | Proof. . Trivials. . . Application.1.1. . Solve a equation $ nabla_ mathbf{w} || mathbf{X}_{m times n} mathbf{w}_{n times 1} - hat{ mathbf{y}}_{m times 1} ||^2 = mathbf{0} $ . Solve. . $$ begin{aligned} &amp; nabla_ mathbf{w} (X mathbf{w} - hat{ mathbf{y}})^T (X mathbf{w} - hat{ mathbf{y}}) &amp;= nabla_ mathbf{w} ( mathbf{w}^T X^T - hat{ mathbf{y}}^T)^T (X mathbf{w} - hat{ mathbf{y}}) &amp;= nabla_ mathbf{w} ( mathbf{w}^T X^T X mathbf{w} - mathbf{w}^T X^T hat{ mathbf{y}} - hat{ mathbf{y}}^T X mathbf{w} + hat{ mathbf{y}}^T hat{ mathbf{y}}) &amp;= nabla_ mathbf{w} ( mathbf{w}^T X^T X mathbf{w} - 2 hat{ mathbf{y}}^T X mathbf{w} + hat{ mathbf{y}}^T hat{ mathbf{y}}) &amp;= 2X^TX mathbf{w} - 2X^T hat{ mathbf{y}} = mathbf{0} , &amp; qquad X^TX mathbf{w} = X^T hat{ mathbf{y}} &amp; therefore , mathbf{w} = (X^TX)^{-1}X^T hat{ mathbf{y}} end{aligned} $$ . 1.3. Direction of Gradients . Consider the directional vector $ mathbf{d}$ that maximizes $ mathcal{L}( mathbf{w} + eta mathbf{d}) - mathcal{L}( mathbf{w}) ( le 0)$. Geometically, $ mathbf{d}$ must be same direction of $ mathcal{L}( mathbf{w})$, therefore $ cos( phi) = 1$. . $$ therefore , mathbf{d} = frac{ nabla_ mathbf{w} mathcal{L}( mathbf{w})}{|| nabla_ mathbf{w} mathcal{L}( mathbf{w})||} $$that is, the direction of the gradient is the direction in which the function value increases. .",
            "url": "https://ndo04343.github.io/blog/calculus/2022/01/27/ch01-basic-calculus.html",
            "relUrl": "/calculus/2022/01/27/ch01-basic-calculus.html",
            "date": " • Jan 27, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "[OptimizationTheory] CH02. Search based Optimization",
            "content": "2.1. Grid Search . $$ mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , f( mathbf{x}) , text{where} , f:(0,1)^n rightarrow mathbb{R} , text{and} , mathbf{x} in {(a_1, a_2, cdots, a_n) : a_1, a_2, cdots, a_n in {0, frac{1}{m}, frac{2}{m}, cdots, frac{m - 1}{m} } }, , m in mathbb{N} $$Example) . In KNN classification, we can obtain $k$ with grid search. | In SVM classification with RBF kernel, we can obtain $c$ and $ gamma$. | . from sklearn.model_selection import GridSearchCV from sklearn import svm, datasets iris = datasets.load_iris() params = { &#39;kernel&#39;: (&#39;linear&#39;, &#39;rbf&#39;), &#39;C&#39;: [1, 10] } svc = svm.SVC() clf = GridSearchCV(svc, params) clf.fit(iris.data, iris.target) clf.cv_results_ . {&#39;mean_fit_time&#39;: array([0.00038528, 0.00042386, 0.00036063, 0.00038018]), &#39;std_fit_time&#39;: array([4.15368686e-05, 6.48989211e-06, 1.46394444e-05, 7.80994636e-06]), &#39;mean_score_time&#39;: array([0.00018463, 0.00020938, 0.00017271, 0.00018597]), &#39;std_score_time&#39;: array([1.26002358e-05, 1.30935003e-06, 6.32595976e-07, 3.16657214e-06]), &#39;param_C&#39;: masked_array(data=[1, 1, 10, 10], mask=[False, False, False, False], fill_value=&#39;?&#39;, dtype=object), &#39;param_kernel&#39;: masked_array(data=[&#39;linear&#39;, &#39;rbf&#39;, &#39;linear&#39;, &#39;rbf&#39;], mask=[False, False, False, False], fill_value=&#39;?&#39;, dtype=object), &#39;params&#39;: [{&#39;C&#39;: 1, &#39;kernel&#39;: &#39;linear&#39;}, {&#39;C&#39;: 1, &#39;kernel&#39;: &#39;rbf&#39;}, {&#39;C&#39;: 10, &#39;kernel&#39;: &#39;linear&#39;}, {&#39;C&#39;: 10, &#39;kernel&#39;: &#39;rbf&#39;}], &#39;split0_test_score&#39;: array([0.96666667, 0.96666667, 1. , 0.96666667]), &#39;split1_test_score&#39;: array([1. , 0.96666667, 1. , 1. ]), &#39;split2_test_score&#39;: array([0.96666667, 0.96666667, 0.9 , 0.96666667]), &#39;split3_test_score&#39;: array([0.96666667, 0.93333333, 0.96666667, 0.96666667]), &#39;split4_test_score&#39;: array([1., 1., 1., 1.]), &#39;mean_test_score&#39;: array([0.98 , 0.96666667, 0.97333333, 0.98 ]), &#39;std_test_score&#39;: array([0.01632993, 0.02108185, 0.03887301, 0.01632993]), &#39;rank_test_score&#39;: array([1, 4, 3, 1], dtype=int32)} . clf.best_params_ . {&#39;C&#39;: 1, &#39;kernel&#39;: &#39;linear&#39;} . 2.2. Random Search . Random search randomly selects a point to search for. It is generally faster than grid search. . from sklearn import svm, datasets from sklearn.model_selection import RandomizedSearchCV iris = datasets.load_iris() params = { &#39;kernel&#39;: (&#39;linear&#39;, &#39;rbf&#39;), &#39;C&#39;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] } svc = svm.SVC() clf = RandomizedSearchCV(svc, params, n_iter=5) clf.fit(iris.data, iris.target) clf.cv_results_ . {&#39;mean_fit_time&#39;: array([0.0004076 , 0.00036163, 0.00039191, 0.00040193, 0.00035834]), &#39;std_fit_time&#39;: array([2.62769725e-05, 5.79195235e-06, 4.55473096e-06, 1.88120569e-05, 1.21943587e-05]), &#39;mean_score_time&#39;: array([0.00020247, 0.00017519, 0.00019407, 0.00019431, 0.00019393]), &#39;std_score_time&#39;: array([1.38532479e-05, 6.97552626e-07, 2.92001932e-06, 2.64633883e-06, 3.75817868e-05]), &#39;param_kernel&#39;: masked_array(data=[&#39;rbf&#39;, &#39;linear&#39;, &#39;rbf&#39;, &#39;rbf&#39;, &#39;linear&#39;], mask=[False, False, False, False, False], fill_value=&#39;?&#39;, dtype=object), &#39;param_C&#39;: masked_array(data=[8, 4, 4, 5, 6], mask=[False, False, False, False, False], fill_value=&#39;?&#39;, dtype=object), &#39;params&#39;: [{&#39;kernel&#39;: &#39;rbf&#39;, &#39;C&#39;: 8}, {&#39;kernel&#39;: &#39;linear&#39;, &#39;C&#39;: 4}, {&#39;kernel&#39;: &#39;rbf&#39;, &#39;C&#39;: 4}, {&#39;kernel&#39;: &#39;rbf&#39;, &#39;C&#39;: 5}, {&#39;kernel&#39;: &#39;linear&#39;, &#39;C&#39;: 6}], &#39;split0_test_score&#39;: array([0.96666667, 0.96666667, 0.96666667, 0.96666667, 1. ]), &#39;split1_test_score&#39;: array([1., 1., 1., 1., 1.]), &#39;split2_test_score&#39;: array([1. , 0.93333333, 0.96666667, 1. , 0.9 ]), &#39;split3_test_score&#39;: array([0.96666667, 0.96666667, 0.96666667, 0.96666667, 0.96666667]), &#39;split4_test_score&#39;: array([1., 1., 1., 1., 1.]), &#39;mean_test_score&#39;: array([0.98666667, 0.97333333, 0.98 , 0.98666667, 0.97333333]), &#39;std_test_score&#39;: array([0.01632993, 0.02494438, 0.01632993, 0.01632993, 0.03887301]), &#39;rank_test_score&#39;: array([1, 4, 3, 1, 4], dtype=int32)} . clf.best_params_ . {&#39;kernel&#39;: &#39;rbf&#39;, &#39;C&#39;: 8} . 2.3. Baysian Optimization . Bayesian optimization is a sequential design strategy for global optimization of black-box functions that does not assume any functional forms. The above methods assume that the results of each trial are independent of each other. However, bayesian optimization optimizes by selecting the next candidate point using the results of each trial. . def black_box_function(x, y): return -x ** 2 - (y - 1) ** 2 + 1 . from bayes_opt import BayesianOptimization # Bounded region of parameter space pbounds = {&#39;x&#39;: (2, 4), &#39;y&#39;: (-3, 3)} optimizer = BayesianOptimization( f=black_box_function, pbounds=pbounds, random_state=1, ) . optimizer.maximize( init_points=2, n_iter=3, ) . | iter | target | x | y | - | 1 | -7.135 | 2.834 | 1.322 | | 2 | -7.78 | 2.0 | -1.186 | | 3 | -7.11 | 2.218 | -0.7867 | | 4 | -12.4 | 3.66 | 0.9608 | | 5 | -6.999 | 2.23 | -0.7392 | ================================================= . optimizer.res . [{&#39;target&#39;: -7.135455292718879, &#39;params&#39;: {&#39;x&#39;: 2.8340440094051482, &#39;y&#39;: 1.3219469606529488}}, {&#39;target&#39;: -7.779531005607566, &#39;params&#39;: {&#39;x&#39;: 2.0002287496346898, &#39;y&#39;: -1.1860045642089614}}, {&#39;target&#39;: -7.109925819441113, &#39;params&#39;: {&#39;x&#39;: 2.2175526295255183, &#39;y&#39;: -0.7867249801593896}}, {&#39;target&#39;: -12.397162416009818, &#39;params&#39;: {&#39;x&#39;: 3.660003815774634, &#39;y&#39;: 0.9608275029525108}}, {&#39;target&#39;: -6.999472814518675, &#39;params&#39;: {&#39;x&#39;: 2.2303920156083024, &#39;y&#39;: -0.7392021938893159}}] . optimizer.max . {&#39;target&#39;: -6.999472814518675, &#39;params&#39;: {&#39;x&#39;: 2.2303920156083024, &#39;y&#39;: -0.7392021938893159}} . 2.4. Golden-Section Search . Golden-section search algorithm is search algorithm for finding a minumum on an interval $[x_l, x_u]$ with a single minimum(unimodal interval). It uses the golden ratio $ phi = 1.6180 cdots $ to determine location of two interior points $x_1$ and $x_2$. By using the golden ratio, one of the interior points can be re-used in the next iteration. . $$ begin{matrix} text{Let} , d = ( phi - 1)(x_u - x_l) x_1 = x_l + d, , x_2 = x_u - d end{matrix} $$ Similarily, compute new $d$ about $x_1$ and $x_2$. Afterwards, it repeats the specified number of times or until the relative error is lower than the specified threshold. . from scipy import optimize def f(x): return (x - 1)**2 minimum = optimize.golden(f, brack=(0, 5)) minimum . 1.000000003917054 .",
            "url": "https://ndo04343.github.io/blog/optimization-theory/2022/01/26/ch02-search-based-optimization.html",
            "relUrl": "/optimization-theory/2022/01/26/ch02-search-based-optimization.html",
            "date": " • Jan 26, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "[OptimizationTheory] CH01. Introduction",
            "content": "1.0. Optimization . Optimization is the process of creating something that is as effective as possible. From a mathematical perspective, optimization deals with finding the maxima and minima of a function that depends on one or more variables. . For example, determin the optimum analytically . $$ z = z_0 + frac{m}{c}(v_0 + frac{mg}{c})(- exp(-(c/m)t)) - frac{mg}{c}t quad text{where} , g = 9.81, , z_0 = 100, , v_0 = 55, , m = 80, , c = 15 $$ Definition.1.1. Optimization Problem . Following problem that find optimal solution $ mathbf{x}$ are called optimization problem. . $$ mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , f( mathbf{x}) $$where $ mathbf{x}$ is optimization variable, $f : mathbb{R}^n rightarrow mathbb{R}$ is objective function. In this situation, $ mathbf{x}^*$ is called optimal solution. . It is very difficult to solve general optimization problem. Many optimization method involve some compromise, e.g., very long computation time, or not always finding the solution. However, certain problems can be solved efficiently and reliably. For example, . Least-squares problems | Linaer programming problems | Convex optimization problems | . 1.1. Classification of Optimization Method . There are some kinds of optimization method like . Search | Least-Squares | Linear Programming/Nonlinear Optimization | Convex Optimization | Gradient based Optimization | . Above methods are not separated by analytical method and numerical method. . 1.2. Search based Algorithm . Search based algorithms are simplest method in optimization theory. It just computes the objective function value for many $x$ candidates and finds the minimum point. Examples of algorithms are as follows. . Grid Search | Golden-Section Search | . These algorithms can be used effectively for single variable functions, but for high-dimensional multivariate functions, the amount of computation increases exponentially. And also, the solution does not guarantee a global optimum. . 1.3. Least-Squares(Mature technology) . Form like . $$ mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , || A mathbf{x} - mathbf{b} ||_2^2 $$are called least-squares problem. Optimal solution can be obtained analytically e.g., $ mathbf{x}^* = (A^T A)^{-1} A^T mathbf{b}$. There are reliable and efficient algorithms and software that have $n^2k , (A in mathbb{R}^{k times n})$ time complexity. It can increase flexibility by few standard techniques like including weights, adding regularization terms. . 1.4. Linear Programming(Mature technology)/Nonlinear Programming . Form like . $$ mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , mathbf{c}^T mathbf{x} quad s.t. quad mathbf{a}_i^T mathbf{x} le mathbf{b}_i, , i = 1, cdots,m $$are called linear programming. There is no analytical formula for solution, but there are reliable and efficient algorithms and software that have $n^2m$ time complexity. . Form like . $$ mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , f( mathbf{x}) quad s.t. quad g_i( mathbf{x}) le 0, , h_j( mathbf{x}) = 0, , i=1, cdots,p, , j=1, cdots,q $$are called nonlinear programming. There are local optimization methods and global optimization methods. . 1.5. Convex Optimization . Form like . $$ mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , f( mathbf{x}) quad s.t. quad g_i( mathbf{x}) le 0, , h_j( mathbf{x}) = 0, , i=1, cdots,p, , j=1, cdots,q, , f , text{is a convex function.} $$This problem includes least-squares problems and linear programming as special cases. $g_i$ is called an inequality constraint function and $h_j$ is called an equality constraint function. There are no analytical solutions, but reliable and efficient algorithms that have $ text{max} {n^3, n^2p, n^2q }$ time complexity roughly. . Using convex optimization often difficult to recognize. However, there are many tricks for transforming problems into convex form. Many problems can be solved via convex optimization. . 1.6. Gradient based Optimization . A gradient based optimization is an algorithm to solve problems of the form $$ mathbf{x}^* = underset{ mathbf{x}}{ mathrm{argmin}} , f( mathbf{x}) $$ with the search directions defined by the gradient of the function at the current point. .",
            "url": "https://ndo04343.github.io/blog/optimization-theory/2022/01/26/ch01-introduction.html",
            "relUrl": "/optimization-theory/2022/01/26/ch01-introduction.html",
            "date": " • Jan 26, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "[LinearAlgebra] CH02. Linear Transformation",
            "content": "2.0. Elementary Definitions and Theorems . Definition.2.1. Linear Transformation . $ mathbb{K} $상의 벡터공간 $V$와 $W$에 대해서, 다음 두 조건을 만족하는 함수 $T , : , V rightarrow W$를 linear transformation이라고 한다. . $ ^ forall mathbf{x} mathbf{y} in V, , ^ forall alpha in mathbb{K}, $ . $ T( mathbf{x} + mathbf{y}) = T( mathbf{x}) + T( mathbf{y}) $ | $ T( alpha mathbf{x}) = alpha T( mathbf{x}) $ | 특히, 일차변환 $T , : , V rightarrow W$가 전단사일때, 이를 linear isomorphism이라 한다. 그리고 이 때의 벡터공간 $V$와 $W$는 linear isomorphic이라고 하고, $ V cong W $로 나타낸다. . EX) . Zero transformation | Identity transformation | Inverse transformation of linear transformation $ T $(Not always). | Matrix transformation | . $ text{Let} , A = [a_{ij}]_{m times n} , text{for} , a_{ij} in mathbb{K} $. $ text{If} , ^ forall mathbf{x} in mathbb{K}^n, , T_A( mathbf{x}) = A mathbf{x} $, . $$ T_A , : , mathbb{K}^n rightarrow mathbb{K}^m $$ Differential transformation | Definite integral transformation | . 2.1. Symmetric Matrix and Orthogonal Matrix . Definition.2.2. Symmetric Matrix and Orthogonal Matrix . $$ text{If} , A^T = A , text{, then} , A , text{is a symetric matrix.} $$$$ text{If} , A^T = A^{-1} , text{, then} , A , text{is an orthogonal matrix.} $$ Theorem.2.1. . $ text{Let} , Q_1 = [q_{ij}^{(1)}]_{n times n} , text{and} , Q_2 = [q_{ij}^{(2)}]_{n times n}$. . $$ text{If} , Q_1, , Q_2 , text{are orthogonal matrices, then the followings are true.} $$ $ text{Every pairs of columns are orthogonal.} $ | $ text{Every columns are unit vector.} $ | $ Q_1Q_2 , text{is also orthogonal matrix.} $ | $ |Q| = 1 , text{or} , |Q| = -1 $ | . Proof. Trivial. . Theorem.2.2. . $ text{Let} , A , text{be a matrix.}$. . $$ text{Then,} , A^TA , text{is a symmetric matrix.} $$ $ (i, i) text{-element of} , A^TA = A_{C_i}^TA_{C_i} ge 0 $ | . Proof. Trivial. . 2.2. Eigenvalue and Eigenvector . Consider $T_A , : , V rightarrow V$ and following vector equation. . $$ T_A( mathbf{x}) = A mathbf{x} = lambda mathbf{x} quad text{for} , lambda in mathbb{R}. $$$ text{Since} , (A - lambda I_n) mathbf{x} = mathbf{0} Leftrightarrow A mathbf{x} = lambda mathbf{x}, $ $ text{by Basic Theorem of Algebra, above equation have} , n , text{complex solutions in} , V. $ . Definition.2.3. Eigenvalue and Eigenvector . $ text{Let} , T_A , : , V rightarrow V , text{and} , text{correspond with matrix} , A$. . For $ lambda in mathbb{K}, , T( mathbf{x}) = lambda mathbf{x}$ is called characteristic equation of linear transform $T$ . | In $T( mathbf{x}) = lambda mathbf{x}$, $ lambda$ is called eigenvalue, and corresponding $ mathbf{x}( neq mathbf{0}, in V)$ is called eigenvector . | Theorem.2.3. . $n$차 정방행렬 A와 정칙행렬 $N$에 대해서 $A, A^T, N^{-1}AN$의 고유치는 일치한다. . Proof. Trivial. . Theorem.2.4. . $ text{Let} , A = [a_{ij}]_{n times n}.$ . $$ begin{matrix} prod_{k = 1}^{n} lambda_k = |A| sum_{k = 1}^{n} lambda_k = sum_{k = 1}^{n} a_{kk} end{matrix} $$Proof. Trivial. . 2.3. Diagonalization . 유한차원 벡터공간 $V$의 일차변환 $T:V rightarrow V$에 대해서, $T$의 행렬을 대각행렬로 만드는 $V$의 기저가 존재할까? . 위의 문제는 theorem.2.3. 에 의해서 다음 문제와 동치이다. . 주어진 실정방행렬 $A$에 대해서, $N^{-1}AN$이 대각행렬이 되는 정칙행렬 $N$이 존재하는가? (복소정방행렬 $A$에 대해서는 $ bar{N}^{-1}AN$에 대해서 따진다.) . Definition.2.3. Diagonalizable . $ text{Let} , A = [a_{ij}]_{n times n} , text{for} , a_{ij} in mathbb{R}$. $ text{If} , exists , text{invertible matrix} , N in mathbb{M}_{n times n}( mathbb{R}) quad s.t. quad N^{-1}AN = diag(d_1, d_2, cdots, d_n),$ $ text{then} , A , text{is diagonalizable by} , N$. . Theorem.2.5. . $ text{Let} , A = [a_{ij}]_{n times n} , text{for} , a_{ij} in mathbb{R}, , lambda_1, lambda_2, cdots, lambda_n , text{are eigenvalues of matrix} , A, , text{and} , mathbf{x}_1, mathbf{x}_2, cdots, mathbf{x}_n , text{are corresponding eigenvectors of eigenvalues}$. $ text{Assume that} , { mathbf{x}_1, mathbf{x}_2, cdots, mathbf{x}_n} , text{are ordered basis of} , mathbb{R}^n$. $ text{Let} , N = [ mathbf{x}_1 , mathbf{x}_2 , cdots , mathbf{x}_n]$. $ text{Then} , N , text{is invertible and} , A , text{is diagonalizable by} , N$. $ text{That is}$ . $$ N^{-1}AN = diag( lambda_1, lambda_2, cdots, lambda_n) $$. . $ text{If} , lambda_1, lambda_2, cdots, lambda_n , text{are different with each other, then the eigenvectors} , mathbf{x}_1, mathbf{x}_2, cdots, mathbf{x}_n , text{are linearly independent and diagonalizable.} $ | $ text{Eigenvector can be multiplied any scalar except zero.} $ | . Proof. Trivial. . Theorem.2.6. . $ text{Let} , A , text{be a diagonalizable real matrix.}$ $ text{Then}$ . $$ A^k = N^{-1}D^kN quad text{for} , k in mathbb{Z}. $$Proof. Trivial. . 유한차원 내적공간 $V$의 일차변환 $T:V rightarrow V$에 대해서, $T$의 행렬을 대각행렬로 만드는 $V$의 정규직교기저가 존재할까? . 위의 문제는 theorem.2.3. 에 의해서 다음 문제와 동치이다. . 주어진 실정방행렬 $A$에 대해서, $P^{-1}AP$이 대각행렬이 되는 직교행렬 $P$가 존재하는가? (복소정방행렬 $A$에 대해서는 $ bar{P}^{-1}AP$에 대해서 따진다.) . Definition.2.4. Orthogonally Diagonalizable . 실정방행렬 $A$가 직교행렬 $P$에 의해서 대각화되면 A는 orthogonally diagonalizable이라고 한다. . Theorem.2.7. . 실대칭행렬 $A$에 대해서, 서로 다른 고유치에 대응되는 고유벡터는 직교한다. . Proof. $ text{Let} , A , text{be orthogonally diagonalizable and} , lambda_1, lambda_2, cdots, lambda_n, mathbf{x}_1, mathbf{x}_2, cdots, mathbf{x}_n , text{are eigenvalues and corresponding eigenvectors.} $ $ text{For} , i neq j, $ $$ begin{matrix} lambda_i mathbf{x}_i^T mathbf{x}_j &amp;= (A mathbf{x}_i)^T mathbf{x}_j &amp;= mathbf{x}_i^T A^T mathbf{x}_j &amp;= mathbf{x}_i(A mathbf{x}_j) &amp;= mathbf{x}_i^T ( lambda_j mathbf{x}_j) end{matrix} $$ . $ text{Therefore,} , ( lambda_i - lambda_j) mathbf{x}_i^T mathbf{x}_j = 0. $ $ text{Since} , lambda_i - lambda_j neq 0, mathbf{x}_i^T mathbf{x}_j = 0.$ . $$ therefore quad mathbf{x}_i perp mathbf{x}_j , text{for} , i neq j quad blacksquare$$ . Theorem.2.8. . $n$차 실정방행렬 $A$에 대해서, $A$가 직교대각화가능일 필요충분조건은 $A$가 대칭행렬이다. . Proof. Trivial. . 2.3. Singular Value Decomposition . Definition.2.5. Positive Definite . $ text{Let} , A , text{be a symmetric matrix.} $ . $$ text{If} , ^ forall mathbf{x} neq mathbf{0}, , mathbf{x}^T A mathbf{x} &gt; 0, , text{then} , A , text{is called positive definite.} $$$$ text{If} , ^ forall mathbf{x} neq mathbf{0}, , mathbf{x}^T A mathbf{x} ge 0, , text{then} , A , text{is called positive semidefinite.} $$$$ text{If} , ^ forall mathbf{x} neq mathbf{0}, , mathbf{x}^T A mathbf{x} &lt; 0, , text{then} , A , text{is called negative definite.} $$$$ text{If} , ^ forall mathbf{x} neq mathbf{0}, , mathbf{x}^T A mathbf{x} le 0, , text{then} , A , text{is called negative semidefinite.} $$ Theorem.2.9. . 대칭행렬 $A$에 대하여 다음이 성립한다. . $A$가 Positive definite면 모든 $A$의 모든 고윳값은 양수이다. | $A$가 Positive semidefinite면 모든 $A$의 모든 고윳값은 음이 아닌 수수이다. | $A$가 Negative definite면 모든 $A$의 모든 고윳값은 음수이다. | $A$가 Negative semidefinite면 모든 $A$의 모든 고윳값은 양이 아닌 실수이다. | . Proof. Chapter03 이후 . . Application.2.1. Eigenvalue Decomposition(EVD) . By above theorems. . Application.2.2. Singular Value Decomposition(SVD) . Any matrix $A_{m times n}$ can be decomposed as $ A = U_{m times m} Sigma_{m times n} {V_{n times n}}^T $ where $ AA^T = U Sigma Sigma^T U^T, , A^TA = V Sigma^T Sigma V^T $ . .",
            "url": "https://ndo04343.github.io/blog/linear-algebra/2022/01/25/ch02-linear-transformation.html",
            "relUrl": "/linear-algebra/2022/01/25/ch02-linear-transformation.html",
            "date": " • Jan 25, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "[LinearAlgebra] CH01. Linear Equations and Inverse Matrices",
            "content": "1.0. Elementary Definitions and Theorems . 이 노트에서 사용하는 수학 표기는 표기법을 따른다. | 기본적인 행렬의 연산과 성질들은 생략한다. | . Definition.1.1. Matrix . 임의의 자연수 $m$과 $n$에 대하여 $mn$개의 수 $(a_{ij} in mathbb{K}, , i = 1, 2, cdots, m, , j = 1, 2, cdots, n)$를 다음과 같이 배열한 도식(diagram) $A$를 $m times n$ 행렬(matrix)이라고 한다. . $$ A = begin{bmatrix} a_{11} &amp; a_{12} &amp; cdots &amp; a_{1j} &amp; cdots &amp; a_{1n} a_{21} &amp; a_{22} &amp; cdots &amp; a_{2j} &amp; cdots &amp; a_{2n} vdots &amp; vdots &amp; ddots &amp; vdots &amp; &amp; vdots a_{i1} &amp; a_{i2} &amp; cdots &amp; a_{ij} &amp; cdots &amp; a_{in} vdots &amp; vdots &amp; &amp; vdots &amp; ddots &amp; vdots a_{m1} &amp; a_{m2} &amp; cdots &amp; a_{mj} &amp; cdots &amp; a_{mn} end{bmatrix} $$행렬 $A$를 간단히 $A = [a_{ij}]$ 또는 $A = [a_{ij}]_{m times n}$과 같이 나타낸다. . 이 글에서 행렬과 관련된 기본 정의와 정리들은 위의 표기를 따른다. . 1.1. System of Linear Equations . Definition.1.2. Augmented Matrices and Coefficient Matrices . $n$개의 미지수 $x_1, , x_2, , cdots, , x_n$에 관한 연립일차방정식 . $$ begin{cases} a_{11}x_1 + a_{12}x_2 + cdots + a_{1n}x_n = b_1 a_{21}x_1 + a_{22}x_2 + cdots + a_{2n}x_n = b_2 ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; vdots a_{m1}x_1 + a_{m2}x_2 + cdots + a_{mn}x_n = b_m end{cases}, $$에 대하여 . $$ A = begin{bmatrix} a_{11} &amp; a_{12} &amp; cdots &amp; a_{1n} a_{21} &amp; a_{22} &amp; cdots &amp; a_{2n} vdots &amp; vdots &amp; ddots &amp; vdots a_{m1} &amp; a_{m2} &amp; cdots &amp; a_{mn} end{bmatrix}_{m times n}, mathbf{x} = begin{bmatrix} x_{1} x_{2} vdots x_{n} end{bmatrix}_{n times 1}, text{and} , mathbf{b} = begin{bmatrix} b_{1} b_{2} vdots b_{m} end{bmatrix}_{m times 1} $$로 둘때, 연립일차방정식을 행렬을 써서 나타내면 . $$ A mathbf{x} = mathbf{b} $$이다. 이때, 연립일차방정식에 대응되는 행렬 $[A quad mathbf{b}]_{m times (n + 1)}$를 주어진 연립일차방정식의 첨가행렬(Augmented matrix)이라고 하고 $A$를 연립일차방정식의 계수행렬(Coeffficient matrix)이라고 한다. . Theorem.1.1. . $A = [a_{ij}]_{n times n}, text{and} , B = [b_{ij}]_{n times n}$에 대하여 다음이 성립한다. . $$ qquad (AB)^ top = B^ top A^ top $$Proof. Trivial. . Theorem.1.2. . Let $A$ is invertible and have $n$-degree. . $$ qquad (A^ top)^{-1} = (A^{-1})^ top $$Proof. Trivial. . Theorem.1.3. . Square matrix $A$에 대하여, 다음이 성립한다. . $$ text{(1)} quad text{If} , [A quad I] cong_R [I quad P], , text{then} , P = A^{-1} text{(2)} quad text{If} , begin{bmatrix} A I end{bmatrix} cong_C begin{bmatrix} I P end{bmatrix}, , text{then} , P = A^{-1} $$Proof. Trivial. . Theorem.1.4. . Square matrix $A, , B, , C$에 대하여, $A$가 가역행렬일때, . $$ text{If} , [A quad B] cong_R [I quad C], , text{then} , C = A^{-1}B $$Proof. Trivial. . Theorem.1.5. . $ text{If} , A , text{and} , B , text{are} , n text{-degree square matrices, then followings are true.} $ . $$ text{(1)} quad Tr(AB) = Tr(BA) text{(2)} quad Tr(PAP^{-1}) = Tr(A) $$Proof. Trivial. . Definition.1.3. Gauss-Jordan Elimination and Gauss Elimination . Forward substitution을 통해서 어떤 행렬을 ref(row echelon form)으로 변환하는 과정을 Gauss Elimination이라고 하고, Back substitution까지 진행하여 어떤 행렬을 rref(reduced row echelon form으로 변환하는 과정을 Gauss-Jordan Elimination이라고 한다. . Definition.1.4. Matrix Decomposition . Matrix Decomposition은 어떤 Matrix를 여러 행렬들의 곱으로 표현하는 것을 의미한다. 이는 Computational convenience, analytic simplicity를 목적으로 가진다. . . Application.1.1. LU Decomposition . $ text{Let} , m times n , text{matrix} , A.$ $ text{By Gauss Elimination, We can get matrix} , U. $ $ text{Then} , E_p cdots E_1 A = U $ $ L = (E_p cdots E_1)^{-1} $ . Ex) . $$ A = begin{bmatrix} 2 &amp; 4 &amp; -1 &amp; 5 &amp; -2 -4 &amp; -5 &amp; 3 &amp; -8 &amp; 1 2 &amp; -5 &amp; -4 &amp; 1 &amp; 8 -6 &amp; 0 &amp; 7 &amp; -3 &amp; 1 end{bmatrix} cong begin{bmatrix} 2 &amp; 4 &amp; -1 &amp; 5 &amp; -2 0 &amp; 3 &amp; 1 &amp; 2 &amp; -3 0 &amp; -9 &amp; -3 &amp; -4 &amp; 10 0 &amp; 12 &amp; 4 &amp; 12 &amp; -5 end{bmatrix} cong begin{bmatrix} 2 &amp; 4 &amp; -1 &amp; 5 &amp; -2 0 &amp; 3 &amp; 1 &amp; 2 &amp; -3 0 &amp; 0 &amp; 0 &amp; 2 &amp; 1 0 &amp; 0 &amp; 0 &amp; 4 &amp; 7 end{bmatrix} cong begin{bmatrix} 2 &amp; 4 &amp; -1 &amp; 5 &amp; -2 0 &amp; 3 &amp; 1 &amp; 2 &amp; -3 0 &amp; 0 &amp; 0 &amp; 2 &amp; 1 0 &amp; 0 &amp; 0 &amp; 0 &amp; 5 end{bmatrix} = U $$$$ L_{C_1} = frac{1}{2} begin{bmatrix} 2 -4 2 -6 end{bmatrix}, , L_{C_2} = frac{1}{3} begin{bmatrix} 0 3 -9 12 end{bmatrix}, , L_{C_3} = frac{1}{2} begin{bmatrix} 0 0 2 4 end{bmatrix}, , L_{C_4} = frac{1}{5} begin{bmatrix} 0 0 0 5 end{bmatrix}. $$ $$ text{Therefore, } , L = begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 -2 &amp; 1 &amp; 0 &amp; 0 1 &amp; -3 &amp; 1 &amp; 0 -3 &amp; 4 &amp; 2 &amp; 1 end{bmatrix} $$Suppose we have to solve $ A mathbf{x} = mathbf{b} $. We can get $ A = LU $. Then . $$ L mathbf{d} = mathbf{b} U mathbf{x} = mathbf{d} $$ import numpy as np from scipy.linalg import lu A = np.array([[2, 4, -1, 5, -2], [-4, -5, 3, -8, 1], [2, -5, -4, 1, 8], [-6, 0, 7, -3, 1]]) P, L, U = lu(A) # P is permutating matrix(pivoting) P, L, U . (array([[0., 0., 0., 1.], [0., 1., 0., 0.], [0., 0., 1., 0.], [1., 0., 0., 0.]]), array([[ 1. , 0. , 0. , 0. ], [ 0.66666667, 1. , 0. , 0. ], [-0.33333333, 1. , 1. , 0. ], [-0.33333333, -0.8 , -0. , 1. ]]), array([[-6.00000000e+00, 0.00000000e+00, 7.00000000e+00, -3.00000000e+00, 1.00000000e+00], [ 0.00000000e+00, -5.00000000e+00, -1.66666667e+00, -6.00000000e+00, 3.33333333e-01], [ 0.00000000e+00, 0.00000000e+00, -8.88178420e-16, 6.00000000e+00, 8.00000000e+00], [ 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, -8.00000000e-01, -1.40000000e+00]])) . . 1.2. Determinants and Inverse Matrices . Theorem.1.6. . $n$차 정방행렬 $A = [a_{ij}]$에 대하여 $|A| = |A^ top|$ . Proof. Trivial. . Theorem.1.7. . $n$차 정방행렬 $A = [a_{ij}]$와 수 $k$에 대하여 $|kA| = k^n|A|$ . Proof. Trivial. . Theorem.1.8. . 두 행렬의 곱의 행렬식은 각각의 행렬식의 곱과 같다. . Proof. Trivial. . Definition.1.5. Minor Determinant and Cofactor . $n$차 정방행렬 $A = [a_{ij}]$의 $i$번째 행과 $j$번째 열을 삭제하고 남은 $(n - 1)$차 정방행렬의 행렬식을 행렬 $A$의 $(i, j)-$Minor determinant이라고 하고 $M_{ij}$로 쓴다. 또한 $M_{ij}$에 부호 $(-1)^{i + j}$를 곱한 $(-1)^{i + j}M_{ij}$를 행렬 $A$의 $(i, j)$-Cofactor라고 하고 $C_{ij}$로 쓴다. 즉, . $$ C_{ij} = (-1)^{i + j}M_{ij} $$ Definition.1.6. Adjoint Matrix and Cofactor Matrix . $n$차 정방행렬 $A = [a_{ij}]$에서 $(i, j)$-Cofactor $C_{ij}$를 $(i, j)$ 성분으로 하는 행렬 $C = [C_{ij}]$의 전치행렬 $C^ top$를 $A$의 Adjoint matrix또는 여인수 행렬(Cofactor matrix)이라고 하며, $ text{adj}(A)$로 나타낸다. . $$ text{adj}(A) = [C_{ij}]^ top = begin{bmatrix} C_{11} &amp; C_{21} &amp; cdots &amp; C_{n1} C_{12} &amp; C_{22} &amp; cdots &amp; C_{n2} vdots &amp; vdots &amp; ddots &amp; vdots C_{1n} &amp; C_{2n} &amp; cdots &amp; C_{nn} end{bmatrix} $$ Theorem.1.9. . $n$차 정방행렬 $A = [a_{ij}]$에 대하여 다음이 성립한다. . $ text{(1)} quad text{adj}(A)A = A text{adj}(A) = |A|I_n$ $ text{(2)} quad A$가 정칙행렬일 필요충분조건은 $|A| neq 0$이다. $ text{(3)} quad A$가 정칙행렬이면 $A^{-1} = frac{1}{|A|} text{adj}(A)$이다. . Proof. Trivial. . Definition.1.7. Minor Determinant and Rank . $m times n$행렬 $A$의 $m - r$개 행과 $n - r$개의 열을 제거하고 남은 $r$차의 정방행렬에 대한 행렬식을 $A$의 $r$차의 소행렬식(Minor determinant)이라고 한다. 만약 $A$의 $r$차 소행렬식 중에서 $0$이 아닌 것이 적어도 하나 존재하고, $r$보다 큰 모든 $s(r &lt; s)$에 대하여 소행렬식이 모두 $0$일때, $r$을 행렬 $A$의 계수(rank)라고 하고 $$ text{rank}(A) = r $$ 이라고 나타낸다. . Theorem.1.10. . $ text{If} , A cong A^ prime, , text{then} , text{rank}(A) = text{rank}(A^ prime)$ . Proof. Trivial. . Theorem.1.11. . 행렬의 계수(rank)는 그 행렬의 rref에서 leading 1의 개수와 같다. . Proof. Trivial. . 1.3. Rank and Solution of System of Linear Equation . 연립 일차 방정식의 해의 종류는 다음과 같다. . No solution | Particular solution(Only one solution) | Infinitely many solutions | . Definition.1.8. Homogeneous System of Linear Equations . 미지수 $x_1, , x_2, , cdots, , x_n$에 관한 연립일차방정식 . $$ begin{cases} a_{11}x_1 + a_{12}x_2 + cdots + a_{1n}x_n = 0 a_{21}x_1 + a_{22}x_2 + cdots + a_{2n}x_n = 0 ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; vdots a_{m1}x_1 + a_{m2}x_2 + cdots + a_{mn}x_n = 0 end{cases} $$을 Homogeneous System of Linear Euation이라고 한다. . Homogeneous System of Linear Equation은 다음과 같은 해를 가진다. . Trivial solution | Infinitely many solution including trivial solution. | . Theorem.1.12. . $ text{In homogeneous system of linear equations, if} , m &lt; n, , text{then the system have infinitely many solutions.} $ . Proof. Trivial. . Theorem.1.13. . 행렬 $A_{m times n}, , mathbf{b}_{m times 1} = [b_1 , b_2 , cdots , b_m]^ top, , mathbf{x}_{n times 1} = [x_1 , x_2 , cdots , x_n]^ top$에 대하여 연립일차방정식 $A mathbf{x} = mathbf{b}$가 해를 가질 필요충분조건은 . $$ text{rank}(A) = text{rank}([A quad B]) $$이며, $A mathbf{x} = mathbf{b}$가 해를 가지는 경우는 다음과 같다. . $ text{If} , text{rank}(A) = text{rank}([A quad B]) = n, , text{then} , A mathbf{x} = mathbf{b} , text{have a particular solution.}$ | $ text{If} , text{rank}(A) = text{rank}([A quad B]) &lt; n, , text{then} , A mathbf{x} = mathbf{b} , text{have infinitely many solutions.}$ | . Remark) . $ text{If} , text{rank}(A) neq text{rank}([A quad B]), , text{then} , nexists , text{solution of} , A mathbf{x} = mathbf{b}$. | $ text{If} , text{rank}(A) = text{rank}([A quad B]) = n, , text{then} , exists^1 , text{solution of} , A mathbf{x} = mathbf{b}$. | $ text{If} , text{rank}(A) = text{rank}([A quad B]) &lt; n, , text{then} , exists^ infty , text{solution of} , A mathbf{x} = mathbf{b}$. | Proof. Trivial. . Theorem.1.14. . $ m times n $행렬 $A$에 대하여 만약 $m &lt; n$이면 동차연립일차방정식 $A mathbf{x} = mathbf{0}$는 무한히 많은 해를 가진다. . Proof. Trivial. . Theorem.1.15. . $n$차 정방행렬 $A$를 계수행렬로 가지는 동차연립일차방정식 $A mathbf{x} = mathbf{0}$가 자명해가 아닌 해를 가질 필요충분조건은 $ text{rank}(A) &lt; n$이다. . Proof. Trivial. .",
            "url": "https://ndo04343.github.io/blog/linear-algebra/2022/01/25/ch01-linear-equations-and-inverse-matrices.html",
            "relUrl": "/linear-algebra/2022/01/25/ch01-linear-equations-and-inverse-matrices.html",
            "date": " • Jan 25, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "HEESUNG YANG .",
          "url": "https://ndo04343.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  
  

  

  
  

  

  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ndo04343.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}