---
keywords: fastai
description: Optimization theory summary note.
title: "[OptimizationTheory] CH05. Constrained Optimizations"
toc: false
badges: false
comments: false
categories: [optimization-theory]
hide_{github,colab,binder,deepnote}_badge: true
nb_path: _notebooks/ch05-constrained-optimizations.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/ch05-constrained-optimizations.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="5.1.-Introduction">5.1. Introduction<a class="anchor-link" href="#5.1.-Introduction"> </a></h4><p>Many practical optimization problems have constraints like equality and inequality constraint. Consider following problem:</p>
$$
\mathbf{w}^* = \underset{\mathbf{w}}{\mathrm{argmin}} || \mathbf{y} - X\mathbf{w} ||_2^2 \quad s.t. \quad ||\mathbf{w}||_2^2 \le 1. 
$$<p>Above problem is inequality constrained optimization problem, and luckly $MSE$ cost function is convex function. Therefore, we can obtain optimal solution analytically. However, if obtained optimal solution doesn't satisfy the inequality condition, we have to find another solution. We can define general constained optimization problem like following.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Definition.5.1.-Constrained-Optimization-Problem">Definition.5.1. Constrained Optimization Problem<a class="anchor-link" href="#Definition.5.1.-Constrained-Optimization-Problem"> </a></h5><p>Following problem that find optimal solution $\mathbf{x}$ are called <strong>constrained optimization problem</strong>.</p>
$$
\begin{matrix}
\mathbf{x}^* = \underset{\mathbf{x}}{\mathrm{argmin}} \,\ f(\mathbf{x}) \\ 
\quad s.t. \quad \\
g_i(\mathbf{x}) \le 0, \,\ i=1,\cdots,m, \\
h_j(\mathbf{x}) = 0, \,\ j=1,\cdots,k \\
\end{matrix}
$$<p>where $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is a objective function, $\mathbf{g} : \mathbb{R}^n \rightarrow \mathbb{R}^m$ is a inequality constraint function, and $\mathbf{h}: \mathbb{R}^n \rightarrow \mathbb{R}^k $ is a equality constraint function.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="5.2.-Lagrange-Multiplier">5.2. Lagrange Multiplier<a class="anchor-link" href="#5.2.-Lagrange-Multiplier"> </a></h4><h5 id="Definition.5.2.-Lagrangian-Function">Definition.5.2. Lagrangian Function<a class="anchor-link" href="#Definition.5.2.-Lagrangian-Function"> </a></h5><p>In above constrained optimization problem, there is corresponding Lagrangian function</p>
$$
\begin{matrix}
\mathcal{L}(\mathbf{x}, \mathbf{\lambda}, \mathbf{\mu}) &amp;= f(\mathbf{x}) + \sum_{i = 1}^{m} \lambda_ig_i(\mathbf{x}) + \sum_{j = 1}^{k} \mu_j h_j(\mathbf{x}) \\
                                                        &amp;= f(\mathbf{x}) + \mathbf{\lambda}^T \mathbf{g}(\mathbf{x}) + \mathbf{\mu}^T \mathbf{h}(\mathbf{x}) \\
\end{matrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>where $\lambda_i, \mu_j$ for $i=1,\cdots,m, \,\ j = 1,\cdots,k$ are dual variables.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Theorem.5.1.-Lagrangian-Multiplier">Theorem.5.1. Lagrangian Multiplier<a class="anchor-link" href="#Theorem.5.1.-Lagrangian-Multiplier"> </a></h5><p>In equality constrained optimization problem, if $ \mathbf{x}^* \in \mathbb{R}^n $ is a local minimum, then there exist $ \mathbf{\mu}^* \in \mathbb{R}^k $ such that</p>
$$
\begin{cases}
\frac{\partial}{\partial \mathbf{x}} \mathcal{L}(\mathbf{x}, \mathbf{\mu}) = \mathbf{0} \\
\frac{\partial}{\partial \mathbf{\mu}} \mathcal{L}(\mathbf{x}, \mathbf{\mu}) = \mathbf{0} \\
\end{cases}.
$$<p>And system of equations contain $n + k$ equation.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Consider following problem from algebra.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Ex)</strong><br></p>
<blockquote><p>2차원 평면상의 원 $x^2 + y^2 = k$과 직선 $y = \sqrt{3}x + 4\sqrt{3}$을 지날때, k의 최소를 구하시오.</p>
</blockquote>
<p><strong>sol)</strong><br></p>
<p>$ \text{Let} \,\ \mathbf{x} = \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \in \mathbb{R}^2. $<br>
$ \text{Then, this problem can be convert following constrained optimization problem.} $<br></p>
$$
\mathbf{x}^* = \underset{\mathbf{x}}{\mathrm{argmin}} \,\ \mathbf{x}^T \mathbf{x} \quad s.t. \quad g(\mathbf{x}) = [\sqrt{3} \,\ - 1]\mathbf{x} + 4\sqrt{3} = 0.
$$<p>$\text{Then the Lagrangian function of above constrained optimization problem is} $<br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\mathcal{L}(\mathbf{x}, \mu) = \mathbf{x}^T \mathbf{x} + \mu ([\sqrt{3} \,\ - 1] \mathbf{x} + 4 \sqrt{3}).
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\frac{\partial}{\partial \mathbf{x}} \mathcal{L}(\mathbf{x}, \mu) = 2 \mathbf{x} + \mu [\sqrt{3} \,\  -1]^T = \mathbf{0} \qquad \cdots \,\ (1)
$$$$
\frac{\partial}{\partial \mu} \mathcal{L}(\mathbf{x}, \mu) = [\sqrt{3} \,\ -1] \mathbf{x} + 4 \sqrt{3} = 0 \qquad \cdots \,\ (2)
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$\text{By theorem.5.1., solution of above equation is a local minimum of optimization problem.}$<br>
$\text{Also, above Lagrangian function is convex.} \quad (\because \,\ \text{Property.3.3})$ <br>
$\text{By, theorem.3.1.,}$<br></p>
$$
\therefore \quad \mathbf{x}^* = \begin{bmatrix} - \frac{\sqrt{3}\mu}{2} \\ \frac{\mu}{2} \end{bmatrix}, \,\ \min k = 12
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Of course, it is reasonable to use gradient based optimization when we optimize the Lagrangian function. Above solution is obtained just analytically, not numerical method.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="5.3.-Karush-Kuhn-Tucker(KKT)-Conditions">5.3. Karush-Kuhn-Tucker(KKT) Conditions<a class="anchor-link" href="#5.3.-Karush-Kuhn-Tucker(KKT)-Conditions"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Consider following problem.</p>
$$
\min x_1^2 + x_2^2 \quad s.t. \quad x_1 + x_2 = 0
$$<p>In above problem, the value of $\mu$ is zero. This means that the equality constraint doesn't dependent on optimization problem. In this situation, above constrained optimization problem has equivalence relation with unconstrained optimization; $\min x_1^2 + x_2^2 $.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Theorem.5.2.-Karush-Kuhn-Tucker(KKT)-Conditions">Theorem.5.2. Karush-Kuhn-Tucker(KKT) Conditions<a class="anchor-link" href="#Theorem.5.2.-Karush-Kuhn-Tucker(KKT)-Conditions"> </a></h5><p>If $ \mathbf{x}^* \in \mathbb{R}^n $ is a local minimum, then there exist $\mathbf{\lambda}^* \in \mathbb{R}^m $ and $ \mathbf{\mu}^* \in \mathbb{R}^k $ such that</p>
<p>$ (1) \,\ \text{Stationarity} \quad \frac{\partial}{\partial \mathbf{x}} \mathcal{L}(\mathbf{x}, \mathbf{\lambda}, \mathbf{\mu}) = \mathbf{0} $<br></p>
<p>$ (2) \,\ \text{Primal feasibility} \quad ^\forall i, \,\ g_i(\mathbf{x}^*) \le 0, \,\ \mathbf{h}(\mathbf{x}^*) = \mathbf{0} $<br></p>
<p>$ (3) \,\ \text{Complementary slackness} \quad ^\forall i, \,\ \lambda_i^* g_i(\mathbf{x}^*) = 0 $<br></p>
<p>$ (4) \,\ \text{Dual feasibility} \quad ^\forall i, \,\ \lambda_i^* \ge 0 $<br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Remark that if optimal solution isn't dependent with inequality constraint, then it is same with unconstrained optimization problem, and if optimal solution is dependent with inequality constraint, then optimal solution is on the boundary line of inequatlity constraint. Consider following problems.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\min x_1^2 + x_2^2 \quad s.t. \quad x_1 + x_2 - 1 \le 0 \qquad \cdots \,\ (1)
$$$$
\min x_1^2 + x_2^2 \quad s.t. \quad x_1 + x_2 - 1 \ge 0 \qquad \cdots \,\ (2)
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>All constraint conditions are divided into those that affect the result and those that do not. In $(1)$, that is same with</p>
$$
\min x_1^2 + x_2^2. 
$$<p></p>
<p>Furthermore, since optimal solution can be in boundary line of inequality constraint, the inequality constraint that affect the result can be converted equality constraint. Therefore, $(2)$ is same with</p>
$$
\min x_1^2 + x_2^2 \quad s.t. \quad x_1 + x_2 - 1 = 0
$$<p>Remark followings. <em>Stationarity</em> condition is because of <em>Lagrange multiplier</em>. <em>Primal feasibility</em> condition is trivial. <em>Complementary slackness</em> condition is derived from Lagrangian multiplier. We can understand that</p>
$$
\frac{\partial }{\partial \lambda_i} \mathcal{L}(\mathbf{x}, \mathbf{\lambda}, \mathbf{\mu}) = g_i(\mathbf{x})
$$<p>, and since it can be constraint that doesn't affect the result, $\lambda_i$ can be zero or not. Therefore,</p>
$$
^\forall i, \,\ \lambda_i^* g_i(\mathbf{x}^*) = 0.
$$<p><em>Dual feasibility</em> condition is a condition that guarantees that the KKT condition is the same problem as the inequality constrained optimization problem.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="5.4.-Duality">5.4. Duality<a class="anchor-link" href="#5.4.-Duality"> </a></h4><p>Consider following optimization problem</p>
$$
\begin{matrix}
\mathbf{x}^* = \underset{\mathbf{x}}{\mathrm{argmin}} \,\ f(\mathbf{x}) \\ 
\quad s.t. \quad \\
g_i(\mathbf{x}) \le 0, \,\ i=1,\cdots,m, \\
h_j(\mathbf{x}) = 0, \,\ j=1,\cdots,k \\
\end{matrix}
$$<p>where $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is a convex function, $\mathbf{g} : \mathbb{R}^n \rightarrow \mathbb{R}^m$ is a convex function for each $g_i$, and $\mathbf{h}: \mathbb{R}^n \rightarrow \mathbb{R}^k $ is an affine function. Above optimization problem is called convex. For convex problems. KKT conditions becomes necessary and also sufficient for global optimality. <br><br></p>
<p>From now on, we consider above problem with duality. Duality means that the primal problem of optimization problem can view the dual problem. In Lagrangian method, it is called <strong>Lagrangian dual problem</strong>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Definition.5.2.-Lagrangian-Dual-Function">Definition.5.2. Lagrangian Dual Function<a class="anchor-link" href="#Definition.5.2.-Lagrangian-Dual-Function"> </a></h5>$$
\mathcal{D}(\mathbf{\lambda}, \mathbf{\mu}) = \underset{\mathbf{x}}{\min} \mathcal{L}(\mathbf{x}, \mathbf{\lambda}, \mathbf{\mu}) = \underset{\mathbf{x}}{\min} \left\{ f(\mathbf{x} + \mathbf{\lambda}^T \mathbf{h}(\mathbf{x}) + \mathbf{\mu}^T \mathbf{g}(\mathbf{x}) \right\}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Theorem.5.3.-Lagrange-Dual-Problem">Theorem.5.3. Lagrange Dual Problem<a class="anchor-link" href="#Theorem.5.3.-Lagrange-Dual-Problem"> </a></h5><p>The problem
$$
\begin{matrix}
\mathbf{x}^* = \underset{\mathbf{x}}{\mathrm{argmin}} \,\ f(\mathbf{x}) \\ 
\quad s.t. \quad \\
g_i(\mathbf{x}) \le 0, \,\ i=1,\cdots,m, \\
h_j(\mathbf{x}) = 0, \,\ j=1,\cdots,k \\
\end{matrix}
$$
where $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is a convex function, $\mathbf{g} : \mathbb{R}^n \rightarrow \mathbb{R}^m$ is a convex function for each $g_i$, and $\mathbf{h}: \mathbb{R}^n \rightarrow \mathbb{R}^k $ is an affine function, is equivalent with</p>
$$
\underset{\mathbf{\lambda}, \mathbf{\mu}}{\max} \mathcal{D}(\mathbf{\lambda}, \mathbf{\mu}) \quad \text{for} \,\ \lambda_i \ge 0, \,\ i=1,\cdots,m.
$$<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In above problem, $ \mathcal{D}(\mathbf{\lambda}, \mathbf{\mu}) \le \mathcal{L}(\mathbf{x}^*, \mathbf{\lambda}, \mathbf{\mu}) \le f(\mathbf{x}^*) $ where $\mathbf{x}^*$ is primal optimal.<br>
Therefore, $ \mathcal{D}(\mathbf{\lambda}, \mathbf{\mu}) \le \underset{\lambda_i \ge 0, \mathbf{\mu}, \,\ \text{for} \,\ i=1,\cdots,m}{\max} \mathcal{D}(\mathbf{\lambda}, \mathbf{\mu}) \le f(\mathbf{x}^*)$.<br>
$f(\mathbf{x}^*)$ is called primal optimal, and $\underset{\lambda_i \ge 0, \mathbf{\mu}, \,\ \text{for} \,\ i=1,\cdots,m}{\max} \mathcal{D}(\mathbf{\lambda}, \mathbf{\mu})$ is called dual optimal.<br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Definition.5.3.-Duality">Definition.5.3. Duality<a class="anchor-link" href="#Definition.5.3.-Duality"> </a></h5><p>Let $p^*$ be primal optimal and $d^*$ be dual optimal of a dual problem.<br>
If $p^* \ge d^*$, it is called weak duality, and if $p^* = d^*$, it is called strong duality.<br>
And $p^* - d^*$ is called duality gap.
<br><br></p>
<p><strong>Remark)</strong><br></p>
<ul>
<li>Dual function is a concave function(proved by definition of concave).</li>
<li>For a convex optimization problem, the strong duality usually holds (not always, i.e., When Slater’s condition is not satisfied.)</li>
</ul>
<p><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Ex)</strong><br></p>
$$
\mathbf{x}^* = \underset{\mathbf{x}}{\mathrm{argmin}} \mathbf{x}^T \mathbf{x} \quad s.t. \quad A\mathbf{x} = \mathbf{b}
$$
</div>
</div>
</div>
</div>
 

