---
keywords: fastai
description: Information theory summary note.
title: "[InformationTheory] CH01. Entropy"
toc: false
badges: false
comments: false
categories: [information-theory]
hide_{github,colab,binder,deepnote}_badge: true
nb_path: _notebooks/ch01-entropy.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/ch01-entropy.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="1.0.-Overview">1.0. Overview<a class="anchor-link" href="#1.0.-Overview"> </a></h4><p>Information theory was developed to study the problem of sending messages by sending discrete alphabets over noised channel like wireless communication. In machine learning, its context lies in the application of information theory to continuous variables. _TM Cover, 2006<strong>, </strong>MacKay, 2003__<br><br></p>
<p>The following are the core intuitions of information theory.</p>
<blockquote><p>Learning an event that is less likely to occur is more informative than learning an event that is more likely to occur.</p>
</blockquote>
<p>For example, "there was a solar eclipse this morning" than "the sun rose this morning" means that you have more information. That is, to quantify the amount of information, the following properties must be satisfied.</p>
<ul>
<li>Events with a high probability of occurrence should have less information.</li>
<li>There is no information about an event that must occur.</li>
<li>Events with a low probability of occurrence should have more information.</li>
<li>The amount of information for individual events should be additive.<ul>
<li>Ex) In coin toss, $I([H, H]) = 2I([H])$</li>
</ul>
</li>
</ul>
<h5 id="Definition.1.1.-Self-information-of-Event-x=$x$">Definition.1.1. Self-information of Event x=$x$<a class="anchor-link" href="#Definition.1.1.-Self-information-of-Event-x=$x$"> </a></h5>$$
I_p(x) = -\log P(x).
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since $\log$ is a natural logarithm, the unit of $I(x)$ above is nat. nat is the amount of information obtained by observing an event with probability $\frac{1}{e}$. If $\log_2$ is used instead of the natural logarithm, the unit is called bit or shannon, which means the amount of information obtained by observing an event with probability $\frac{1}{2}$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Consider the binomial distribution of the probability that a coin is tossed 3 times and heads are x. The expression is:</p>
$$
p(x) = {3 \choose x}p^x (1 - p)^{3 - x} \quad \text{for} \,\ x=0,1,2,3.
$$<p>In the above equation, for each event $x=0, x=1, x=2, x=3$, the probability value is as follows.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\begin{matrix}
p(x=0) = 0.125 \\
p(x=1) = 0.375 \\
p(x=2) = 0.375 \\
p(x=3) = 0.125 \\
\end{matrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The self-information for it is as follows.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\begin{matrix}
I_p(x=0) = 3 \\
I_p(x=1) = 1.415037\cdots \\
I_p(x=2) = 1.415037\cdots \\
I_p(x=3) = 3 \\
\end{matrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The information amount value is a structure that receives the probability value as $\log$, so it is not negative and has a relatively large value at a low probability value.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="1.1.-Shannon-Entropy">1.1. Shannon Entropy<a class="anchor-link" href="#1.1.-Shannon-Entropy"> </a></h4><p>The above case of self-information deals with only one event. The uncertainty of the entire probability distribution can be quantified with the Shannon entropy.</p>
<h5 id="Definition.1.2.-Shannon-Entropy">Definition.1.2. Shannon Entropy<a class="anchor-link" href="#Definition.1.2.-Shannon-Entropy"> </a></h5>$$
H[p] = \mathbb{E}_{x \sim  p}[I(x)] = - \mathbb{E}_{x \sim  p}[\log p(x)].
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is the average amount of information for the events in the distribution. This value tells the lower bound of the average number of bits required to encode information drawn from the distribution $p$. In the example above, the Shannon entropy value is calculated as follows.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
H[p] = \frac{1}{4} (0.125 \cdot 3 + 0.375 \cdot 1.415037 + 0.375 \cdot 1.415037 + 0.125 \cdot 3) = 0.4528194.
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The value of Shannon's entropy is low in the deterministic case, and the closer it is to an even distribution(i.e., like uniform distribution), the higher the entropy. In particular, when $x$ is a continuous variable, the Shannon entropy is called differential entropy.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h5 id="Application.1.1.-Maximization-of-Shannon-Entropy">Application.1.1. Maximization of Shannon Entropy<a class="anchor-link" href="#Application.1.1.-Maximization-of-Shannon-Entropy"> </a></h5><p>Let probability density function $p$ be for $x\in [a, b]$.<br>
Then,</p>
$$
\begin{matrix}
\int_{a}^{b} p(x)dx = 1 \\
\end{matrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Consider following problem</p>
$$
\max_{p(x)} H[p(x)] = \max_{p(x)} - \mathbb{E}[\log p(x)] = \max_{p(x)} - \int_a^b p(x) \log p(x) dx
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Above problem is equality constraind optimization problem. Therefore we have to get lagrangian function $\mathcal{L}$.</p>
$$
\mathcal{L} = - \int_a^b p(x) \log p(x) dx + \lambda_1 \left( \int_a^b p(x) dx - 1 \right)
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Above $\mathcal{L}$ is functional, then</p>
$$
\frac{\delta \mathcal{L}}{\delta p(x)} = - \log p(x) - 1 + \lambda_1 = 0
$$$$
\therefore \,\ p(x) = \exp(-1 + \lambda_1) = c \,\ \text{for some constant} \,\ c.
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By equality constraint, 
$$
\int_a^b c dx = c(b - a) = 1.
$$
Then</p>
$$
\therefore \,\ c = \frac{1}{b - a} = p(x) \sim U(a, b)
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In above result, we can know that the uniform distribution is maximized shannon entropy distribution.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h5 id="Application.1.2.-Maximization-of-Shannon-Entropy-with-Fixed-Variation">Application.1.2. Maximization of Shannon Entropy with Fixed Variation<a class="anchor-link" href="#Application.1.2.-Maximization-of-Shannon-Entropy-with-Fixed-Variation"> </a></h5><p>Let the expectation of probability density function $p$ be $\mu$ and variation be $\sigma^2$.<br>
Then,</p>
$$
\begin{matrix}
\int_{-\infty}^{\infty} p(x)dx = 1 \\
\int_{-\infty}^{\infty} xp(x)dx = \mu \\
\int_{-\infty}^{\infty} (x - \mu)^2p(x)dx = \sigma^2 \\
\end{matrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For maximization of $H[p]$, we have to use lagrange multiplier.<br>
Then,</p>
$$
\max \mathcal{L}[p](\mathbf{\lambda}) = \max - \int_{-\infty}^{\infty} p(x) \log p(x) dx + \mathbf{\lambda}^T 
\begin{bmatrix}
\int_{-\infty}^{\infty} p(x)dx - 1 \\
\int_{-\infty}^{\infty} xp(x)dx - \mu \\
\int_{-\infty}^{\infty} (x-\mu)^2 p(x)dx - \sigma^2 \\
\end{bmatrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then,</p>
$$
\frac{\delta \mathcal{L}}{\delta p(x)} = - \log p(x) - 1 + \lambda_1  + \lambda_2x + \lambda_3(x-\mu)^2  = 0
$$$$
\therefore \,\ p(x) = \exp(-1 + \lambda_1 + \lambda_2x + \lambda_3(x-\mu)^2)
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By first equality constraint,</p>
$$
\begin{matrix}
\int_{-\infty}^{\infty} p(x) dx = \int_{-\infty}^{\infty} \exp(-1 + \lambda_1 + \lambda_2x + \lambda_3(x - \mu)^2) dx \\
= \int_{-\infty}^{\infty} \exp( \lambda_3x^2 + (\lambda_2 - 2\mu)x + \mu^2 \lambda_3 + \lambda_1 - 1) dx \quad (\lambda_3 &lt; 0)\\ 
\end{matrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\begin{matrix}
- |\lambda_3|(x^2 + \frac{\lambda_2 - 2\mu}{|\lambda_3|}x + \frac{(\lambda_2 - 2\mu)^2}{4\lambda_3^2} ) - \frac{(\lambda_2 - 2\mu)^2}{4|\lambda_3|} + \mu^2 \lambda_3 + \lambda_1 - 1 \\
= - |\lambda_3|( x + \frac{\lambda_2 - 2\mu}{2\lambda_3} )^2 - \frac{(\lambda_2 - 2\mu)^2}{4|\lambda_3|} + \mu^2 \lambda_3 + \lambda_1 - 1 \\
\end{matrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\begin{matrix}
\int_{-\infty}^{\infty} \exp( \lambda_3x^2 + (\lambda_2 - 2\mu)x + \mu^2 \lambda_3 + \lambda_1 - 1) dx \\
= \int_{-\infty}^{\infty} \exp(- |\lambda_3|( x + \frac{\lambda_2 - 2\mu}{2\lambda_3} )^2 - \frac{(\lambda_2 - 2\mu)^2}{4|\lambda_3|} + \mu^2 \lambda_3 + \lambda_1 - 1) dx \\
\end{matrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let $\sqrt{|\lambda_3|}(x + \frac{\lambda_2}{2\lambda_3}) = t$.<br>
Then</p>
$$
\begin{matrix}
\int_{-\infty}^{\infty} \exp(- |\lambda_3|( x + \frac{\lambda_2 - 2\mu}{2\lambda_3} )^2 - \frac{(\lambda_2 - 2\mu)^2}{4|\lambda_3|} + \mu^2 \lambda_3 + \lambda_1 - 1) dx \\
= \frac{1}{\sqrt{|\lambda_3|}} \exp\left\{ - \frac{(\lambda_2 - 2\mu)^2}{4|\lambda_3|} + \mu^2\lambda_3 + \lambda_1 - 1 \right\} \int_{-\infty}^{\infty} \exp(-t^2) dt
\end{matrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\therefore \,\ \frac{\sqrt{\pi}}{\sqrt{|\lambda_3|}} \exp\left\{ - \frac{(\lambda_2 - 2\mu)^2}{4|\lambda_3|} + \mu^2\lambda_3 + \lambda_1 - 1 \right\} = 1 \qquad \cdots \,\ (1) 
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In similar way by second equality constraint,</p>
$$
- \frac{(\lambda_2 - 2\mu)\sqrt{\pi}}{2\lambda_3 \sqrt{|\lambda_3|}} \exp\left\{ - \frac{(\lambda_2 - 2\mu)^2}{4|\lambda_3|} + \mu^2\lambda_3 + \lambda_1 - 1 \right\} = \mu \qquad \cdots \,\ (2)
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In similar way by third equality constraint,</p>
$$
\frac{\sqrt{\pi}}{\sqrt{|\lambda_3|}} \left( \frac{1}{2|\lambda_3|} + \frac{(\lambda_2 - 2\mu + 2\lambda_3 \mu)^2}{4\lambda_3^2} \right) = \sigma^2 \qquad \cdots \,\ (3)
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By $(1), \,\ (2), \,\ \text{and} \,\ (3)$,</p>
$$
\therefore \,\ p(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left\{ - \frac{(x - \mu)^2}{2\sigma^2} \right\}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
</div>
 

