---
keywords: fastai
description: Optimization theory summary note.
title: "[OptimizationTheory] CH04. Gradient based Optimizations"
toc: false
badges: false
comments: false
categories: [optimization-theory]
hide_{github,colab,binder,deepnote}_badge: true
nb_path: _notebooks/ch04-gradient-based-optimizations.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/ch04-gradient-based-optimizations.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="4.1.-Introduction">4.1. Introduction<a class="anchor-link" href="#4.1.-Introduction"> </a></h4><p>Gradient descent is an iterative first-order optimisation algorithm used to find a local minimum of a given function.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="4.2.-Gradient-Descent">4.2. Gradient Descent<a class="anchor-link" href="#4.2.-Gradient-Descent"> </a></h4><h5 id="Algorithm.4.1.-Gradient-Descent">Algorithm.4.1. Gradient Descent<a class="anchor-link" href="#Algorithm.4.1.-Gradient-Descent"> </a></h5><p>Gradient descent is an iterative method to find a stationary point of an unconstraint optimization problem : <br>
$$
 \theta^* = \underset{\mathbf{\theta}} {\arg\min} L (\mathbf{\theta}) 
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
L(\mathbf{\theta} + \eta \mathbf{d}) \approx L(\mathbf{\theta}) + \eta \bigtriangledown  _\mathbf{\theta} ^ T L( \mathbf{\theta} ) \mathbf{d} \quad where \quad \eta &gt; 0, \,\  \left \| \mathbf{d}  \right \| = 1 
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
L(\mathbf{\theta} + \eta \mathbf{d}) - L(\mathbf{\theta}) \approx \eta \bigtriangledown  _\mathbf{\theta} ^ T L( \mathbf{\theta} ) \mathbf{d} = \eta \cos{(\phi)} \left \| \bigtriangledown  _\mathbf{\theta} ^ T L( \mathbf{\theta} ) \right \| 
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Find the directional vector $\mathbf{d}$ that minimizes $ L(\mathbf{\theta} + \eta \mathbf{d} ) - L(\mathbf{\theta}) \le 0 $</p>
$$
\cos{(\phi)} = -1 \,\ \rightarrow \,\ \mathbf{d} = - \frac{\bigtriangledown_\mathbf{\theta} L( \mathbf{\theta} ) }{ \left \| \bigtriangledown  _\mathbf{\theta} L( \mathbf{\theta} ) \right \| } 
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\therefore \mathbf{\theta} + \eta \mathbf{d} = \mathbf{\theta} - \eta \frac{\bigtriangledown_\mathbf{\theta} L( \mathbf{\theta} ) }{ \left \| \bigtriangledown  _\mathbf{\theta} L( \mathbf{\theta} ) \right \| } = \mathbf{\theta} - \alpha \bigtriangledown_\mathbf{\theta} L( \mathbf{\theta} )
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="4.3.-4-Types-of-Gradient-Descent">4.3. 4 Types of Gradient Descent<a class="anchor-link" href="#4.3.-4-Types-of-Gradient-Descent"> </a></h4><p>$ (i) \,\ \text{Standard (or steepest) Gradient Descent} $
{% raw %}
$$ \mathbf{w} \leftarrow \mathbf{w} - \eta \bigtriangledown \mathbb{E}[J(\mathbf{w})] $$
{% endraw %}</p>
<ul>
<li>Practically infeasible</li>
<li>Thus, we need distribution about data $\mathbf{x}$ (Contradiction)</li>
<li>So, We can use sample mean
<br><br></li>
</ul>
<p>$ (ii) \,\ \text{Stochastic(online) Gradient Descent} $
{% raw %}
$$ \mathbf{w} \leftarrow \mathbf{w} - \eta \bigtriangledown J_i(\mathbf{w}) $$
{% endraw %}</p>
<ul>
<li>Simple to implement </li>
<li>Effective for large-scale problem</li>
<li>Much less memory</li>
<li>Unstable(zigzaging)</li>
<li>Purpose : We just consider one of data</li>
<li>It can be convergent. But there is little unstable.
<br><br></li>
</ul>
<p>$ (iii) \,\ \text{Batch gradient Descent} $
{% raw %}
$$ \mathbf{w} \leftarrow \eta \bigtriangledown \sum_{i=1}^{N} J_i (\mathbf{w}) $$
{% endraw %}</p>
<ul>
<li>Accurate estimation of gradients</li>
<li>Parallelization of learning</li>
<li>Large memory</li>
<li>Big time-complexity can be problem in this method.(So slow)</li>
<li>But, there isn't problem in convergence. </li>
<li>Purpose : We consider all of data!
<br><br></li>
</ul>
<p>$ (vi) \,\ \text{Mini-Batch Gradient Descent} $
{% raw %}
$$ \mathbf{w} \leftarrow \mathbf{w} - \eta \bigtriangledown \sum_{i \in \mathfrak{I}}^{N} J_i (\mathbf{w}), \quad 1 \le \left | \mathfrak{I} \right | \le N $$
{% endraw %}</p>
<ul>
<li>Most generalized version</li>
<li>Effective to deal with large</li>
<li>Amount of training data</li>
<li>Purpose : We just consider seveal datas.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="4.4.-Newton's-Method">4.4. Newton's Method<a class="anchor-link" href="#4.4.-Newton's-Method"> </a></h4><p>Newton's method is zero finding algorithm. Many equations can be solved by this algorithm and bisection search algorithm in numerical analysis. We use this method too because of gradient necessary condition, which is $\nabla L = \mathbf{0}$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Algorithm.4.2.-Newton-Rapson-Method-in-Multivariate-Function">Algorithm.4.2. Newton-Rapson Method in Multivariate Function<a class="anchor-link" href="#Algorithm.4.2.-Newton-Rapson-Method-in-Multivariate-Function"> </a></h5><p>In gradient updating context, we can find hyperplane of $L$ at $\mathbf{w}_0$</p>
$$
\mathbf{y} = \nabla^2 L(\mathbf{w})^T (\mathbf{w} - \mathbf{w}_0) + \nabla L(\mathbf{w}_0)
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And we have to find next $\mathbf{w}$ by obtaining solution of following eqation:</p>
$$
\nabla^2 L(\mathbf{w})^T (\mathbf{w} - \mathbf{w}_0) + \nabla L(\mathbf{w}_0) = \mathbf{0}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Therefore,</p>
$$
\mathbf{w}_1 = \mathbf{w}_0 - H(\mathbf{w}_0)^{-1} \nabla L(\mathbf{w}_0)
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Actually, we can consider too polynomial approximation like Taylor series expansion. The result is surprising.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
L(\mathbf{w} + \Delta \mathbf{w}) \approx L(\mathbf{w}) + \nabla L (\mathbf{w})^T \Delta \mathbf{w} + \frac{1}{2} \Delta \mathbf{w}^T H(\mathbf{w}) \Delta \mathbf{w}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\frac{\partial}{\partial \Delta \mathbf{w}} L(\mathbf{w} + \Delta \mathbf{w}) \approx \nabla L(\mathbf{w}) + H(\mathbf{w}) \Delta \mathbf{w} = \mathbf{0}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\therefore \,\ \Delta \mathbf{w} = H(\mathbf{w})^{-1} \nabla L(\mathbf{w})
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The above result is the same as the result of the Newton Method.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="4.4.-Quasi-Newton-Method">4.4. Quasi-Newton Method<a class="anchor-link" href="#4.4.-Quasi-Newton-Method"> </a></h4><p>The inverse of the Hessian matrix appearing in Newton's method is difficult to use because of its too much computation. By replacing this with an average gradient, the amount of computation can be reduced. Explore the BFGS method.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="4.5.-Update-Rule-with-Momentum">4.5. Update Rule with Momentum<a class="anchor-link" href="#4.5.-Update-Rule-with-Momentum"> </a></h4><p>We can add a momentum term to the update equation to prevent slowing down of learning or reduce instability of learning. Basic update rule is following:</p>
$$
\mathbf{w}_{k + 1} = \mathbf{w}_k - \eta \nabla_\mathbf{w} L(\mathbf{w}_k) + \gamma \mathbf{w}_{k - 1} \,\ \text{for} \,\ k \ge 2.
$$<p>There are various variants of the gradient update algorithm using momentum.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Algorithm.4.4.-Nesterov-Accelerated-Gradient(NAG)">Algorithm.4.4. Nesterov Accelerated Gradient(NAG)<a class="anchor-link" href="#Algorithm.4.4.-Nesterov-Accelerated-Gradient(NAG)"> </a></h5><p>When using Momentum, the direction of the gradient is also slightly shifted in the previous direction.</p>
$$
\mathbf{w}_{k + 1} = \mathbf{w}_k - \eta \nabla_\mathbf{w} L(\mathbf{w}_k + \gamma \mathbf{w}_{k - 1}) + \gamma \mathbf{w}_{k - 1} \,\ \text{for} \,\ k \ge 2.
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="4.6.-Update-Rule-with-Adaptive-Leaning-Rate">4.6. Update Rule with Adaptive Leaning Rate<a class="anchor-link" href="#4.6.-Update-Rule-with-Adaptive-Leaning-Rate"> </a></h4><p>If the learning rate is too small, the learning time is too long, and if the learning rate is too large, it diverges(zigzagging) and learning is not performed properly. <br>
AdaGrad solves this problem through learning rate decay. However, this also has a problem (zero convergence problem), so the following methods are used.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Algorithm.4.5.-Adaptive-Gradient(AdaGrad)">Algorithm.4.5. Adaptive Gradient(AdaGrad)<a class="anchor-link" href="#Algorithm.4.5.-Adaptive-Gradient(AdaGrad)"> </a></h5>$$
\mathbf{w}_{k + 1} = \mathbf{w}_k - \frac{\eta}{\sqrt{\epsilon + \mathbf{d}_k}} \odot \nabla_\mathbf{w} L(\mathbf{w}_k), \,\ \mathbf{d}_k = \mathbf{d}_{k - 1} +  \nabla_\mathbf{w} L(\mathbf{w}_k) \odot \nabla_\mathbf{w} L(\mathbf{w}_k)
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The above algorithm has a fatal flaw. Since $d$ is infinitely increasing, the amount of change in the gradient will converge to zero.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Algorithm.4.6.-Root-Mean-Square-Propagation(RMSProp)">Algorithm.4.6. Root Mean Square Propagation(RMSProp)<a class="anchor-link" href="#Algorithm.4.6.-Root-Mean-Square-Propagation(RMSProp)"> </a></h5>$$
\mathbf{w}_{k + 1} = \mathbf{w}_k - \frac{\eta}{\sqrt{\epsilon + \mathbf{d}_k}} \odot \nabla_\mathbf{w} L(\mathbf{w}_k), \,\ \mathbf{d}_k = \gamma \mathbf{d}_{k - 1} + (1 - \gamma) \nabla_\mathbf{w} L(\mathbf{w}_k) \odot \nabla_\mathbf{w} L(\mathbf{w}_k)
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Algorithm.4.7.-Adaptive-Delta(AdaDelta)">Algorithm.4.7. Adaptive Delta(AdaDelta)<a class="anchor-link" href="#Algorithm.4.7.-Adaptive-Delta(AdaDelta)"> </a></h5>$$
\mathbf{w}_{k + 1} = \mathbf{w}_k - \frac{\sqrt{\epsilon + \mathbf{u}_k}}{\sqrt{\epsilon + \mathbf{d}_k}} \odot \nabla_\mathbf{w} L(\mathbf{w}_k), \,\ \mathbf{d}_k = \gamma \mathbf{d}_{k - 1} + (1 - \gamma) \nabla_\mathbf{w} L(\mathbf{w}_k) \odot \nabla_\mathbf{w} L(\mathbf{w}_k), \,\ \mathbf{u}_k = \gamma \mathbf{u}_{k - 1} - (1 - \gamma) \frac{\sqrt{\epsilon + \mathbf{u}_{k-1}}}{\sqrt{\epsilon + \mathbf{d}_{k-1}}} \odot \nabla_\mathbf{w} L(\mathbf{w}_{k-1}) \,\ \text{for} \,\ k \ge 2.
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="4.6.-Hybrid-Update-Rule-with-Momentum-and-Adaptive-Learning-Rate">4.6. Hybrid Update Rule with Momentum and Adaptive Learning Rate<a class="anchor-link" href="#4.6.-Hybrid-Update-Rule-with-Momentum-and-Adaptive-Learning-Rate"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Adaptive Moment Estimation(Adam) : Momentum + RMSProp</li>
<li>Nesterov-accelerated Adaptive Moment Estimation(NAdam) : NAG + Adam</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="4.7.-Laerning-Rate-Scheduler">4.7. Laerning Rate Scheduler<a class="anchor-link" href="#4.7.-Laerning-Rate-Scheduler"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In implementations of neural network, the optimizer is important, but the learning rate scheduler is also important. In pytorch implementation, the followings are a commonly used learning rate scheduler.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li>Constant Learning Rate</li>
<li>LambdaLR</li>
</ol>
<div class="highlight"><pre><span></span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">LambdaLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">lr_lambda</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">epoch</span><span class="p">:</span> <span class="mf">0.95</span> <span class="o">**</span> <span class="n">epoch</span><span class="p">)</span>
</pre></div>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">&lt;</span> <span class="mi">40</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">0.5</span>
    <span class="k">elif</span> <span class="n">epoch</span> <span class="o">&lt;</span> <span class="mi">70</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">0.5</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="k">elif</span> <span class="n">epoch</span> <span class="o">&lt;</span> <span class="mi">90</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">0.5</span> <span class="o">**</span> <span class="mi">3</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">0.5</span> <span class="o">**</span> <span class="mi">4</span>

<span class="n">scheduler</span> <span class="o">=</span> <span class="n">LambdaLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">lr_lambda</span> <span class="o">=</span> <span class="n">func</span>
</pre></div>
<ol>
<li>StepLR</li>
</ol>
<div class="highlight"><pre><span></span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
<ol>
<li>MultiStepLR</li>
</ol>
<div class="highlight"><pre><span></span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">MultiStepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">milestones</span><span class="o">=</span><span class="p">[</span><span class="mi">200</span><span class="p">,</span> <span class="mi">350</span><span class="p">],</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
<ol>
<li>ExponentialLR</li>
</ol>
<div class="highlight"><pre><span></span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
</pre></div>
<ol>
<li>CosineAnnealingLR</li>
</ol>
<div class="highlight"><pre><span></span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">eta_min</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</pre></div>
<ol>
<li>CosineAnnealingWarmRestarts</li>
</ol>
<div class="highlight"><pre><span></span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">CosineAnnealingWarmRestarts</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">T_0</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">T_mult</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eta_min</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</pre></div>
<ol>
<li>Custom CosineAnnealingWarmRestarts</li>
</ol>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">_LRScheduler</span>

<span class="k">class</span> <span class="nc">CosineAnnealingWarmUpRestarts</span><span class="p">(</span><span class="n">_LRScheduler</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">T_0</span><span class="p">,</span> <span class="n">T_mult</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eta_max</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">T_up</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">last_epoch</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">T_0</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">T_0</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected positive integer T_0, but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">T_0</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">T_mult</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">T_mult</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected integer T_mult &gt;= 1, but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">T_mult</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">T_up</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">T_up</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected positive integer T_up, but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">T_up</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">T_0</span> <span class="o">=</span> <span class="n">T_0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">T_mult</span> <span class="o">=</span> <span class="n">T_mult</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base_eta_max</span> <span class="o">=</span> <span class="n">eta_max</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eta_max</span> <span class="o">=</span> <span class="n">eta_max</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">T_up</span> <span class="o">=</span> <span class="n">T_up</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">T_i</span> <span class="o">=</span> <span class="n">T_0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cycle</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">T_cur</span> <span class="o">=</span> <span class="n">last_epoch</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CosineAnnealingWarmUpRestarts</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">last_epoch</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_cur</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_lrs</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_cur</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_up</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[(</span><span class="bp">self</span><span class="o">.</span><span class="n">eta_max</span> <span class="o">-</span> <span class="n">base_lr</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">T_cur</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_up</span> <span class="o">+</span> <span class="n">base_lr</span> <span class="k">for</span> <span class="n">base_lr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_lrs</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">base_lr</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eta_max</span> <span class="o">-</span> <span class="n">base_lr</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">T_cur</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">T_up</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">T_i</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_up</span><span class="p">)))</span> <span class="o">/</span> <span class="mi">2</span>
                    <span class="k">for</span> <span class="n">base_lr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_lrs</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">T_cur</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_cur</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_cur</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_i</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">cycle</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">T_cur</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_cur</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_i</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">T_i</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">T_i</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_up</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_mult</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_up</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">epoch</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_0</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_mult</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">T_cur</span> <span class="o">=</span> <span class="n">epoch</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_0</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">cycle</span> <span class="o">=</span> <span class="n">epoch</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_0</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">n</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">((</span><span class="n">epoch</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_0</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">T_mult</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_mult</span><span class="p">))</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">cycle</span> <span class="o">=</span> <span class="n">n</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">T_cur</span> <span class="o">=</span> <span class="n">epoch</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_0</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">T_mult</span> <span class="o">**</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">T_mult</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">T_i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_0</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_mult</span> <span class="o">**</span> <span class="p">(</span><span class="n">n</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">T_i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_0</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">T_cur</span> <span class="o">=</span> <span class="n">epoch</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">eta_max</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_eta_max</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">cycle</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">param_group</span><span class="p">,</span> <span class="n">lr</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()):</span>
            <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>


<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">CosineAnnealingWarmUpRestarts</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">T_0</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">T_mult</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eta_max</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>  <span class="n">T_up</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>
</div>
 

