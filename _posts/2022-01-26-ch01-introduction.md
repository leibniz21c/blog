---
keywords: fastai
description: Optimization theory summary note.
title: "[OptimizationTheory] CH01. Introduction"
toc: false
badges: false
comments: false
categories: [optimization-theory]
hide_{github,colab,binder,deepnote}_badge: true
nb_path: _notebooks/ch01-introduction.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/ch01-introduction.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="1.0.-Optimization">1.0. Optimization<a class="anchor-link" href="#1.0.-Optimization"> </a></h4><blockquote><p>Optimization is the process of creating something that is as effective as possible. From a mathematical perspective, optimization deals with finding the maxima and minima of a function that depends on one or more variables.</p>
</blockquote>
<p>For example, determin the optimum analytically</p>
$$
z = z_0 + \frac{m}{c}(v_0 + \frac{mg}{c})(-\exp(-(c/m)t)) - \frac{mg}{c}t \quad \text{where} \,\ g = 9.81, \,\ z_0 = 100, \,\ v_0 = 55, \,\ m = 80, \,\ c = 15
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Definition.1.1.-Optimization-Problem">Definition.1.1. Optimization Problem<a class="anchor-link" href="#Definition.1.1.-Optimization-Problem"> </a></h5><p>Following problem that find optimal solution $\mathbf{x}$ are called <strong>optimization problem</strong>.</p>
$$
\mathbf{x}^* = \underset{\mathbf{x}}{\mathrm{argmin}} \,\ f(\mathbf{x})
$$<p>where $\mathbf{x}$ is optimization variable, $f : \mathbb{R}^n \rightarrow \mathbb{R}$ is objective function.<br>
In this situation, $\mathbf{x}^*$ is called optimal solution.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It is very difficult to solve general optimization problem. Many optimization method involve some compromise, e.g., very long computation time, or not always finding the solution. However, certain problems can be solved efficiently and reliably. For example,</p>
<ul>
<li>Least-squares problems</li>
<li>Linaer programming problems</li>
<li>Convex optimization problems</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="1.1.-Classification-of-Optimization-Method">1.1. Classification of Optimization Method<a class="anchor-link" href="#1.1.-Classification-of-Optimization-Method"> </a></h4><p>There are some kinds of optimization method like</p>
<ul>
<li>Search</li>
<li>Least-Squares</li>
<li>Linear Programming/Nonlinear Optimization</li>
<li>Convex Optimization</li>
<li>Gradient based Optimization</li>
</ul>
<p>Above methods are not separated by analytical method and numerical method.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="1.2.-Search-based-Algorithm">1.2. Search based Algorithm<a class="anchor-link" href="#1.2.-Search-based-Algorithm"> </a></h4><p>Search based algorithms are simplest method in optimization theory. It just computes the objective function value for many $x$ candidates and finds the minimum point. Examples of algorithms are as follows.</p>
<ul>
<li>Grid Search</li>
<li>Golden-Section Search</li>
</ul>
<p>These algorithms can be used effectively for single variable functions, but for high-dimensional multivariate functions, the amount of computation increases exponentially. And also, the solution does not guarantee a global optimum.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="1.3.-Least-Squares(Mature-technology)">1.3. Least-Squares(Mature technology)<a class="anchor-link" href="#1.3.-Least-Squares(Mature-technology)"> </a></h4><p>Form like</p>
$$
\mathbf{x}^* = \underset{\mathbf{x}}{\mathrm{argmin}} \,\ || A\mathbf{x} - \mathbf{b} ||_2^2
$$<p>are called least-squares problem. Optimal solution can be obtained analytically e.g., $\mathbf{x}^* = (A^T A)^{-1} A^T \mathbf{b}$. There are reliable and efficient algorithms and software that have $n^2k \,\ (A \in \mathbb{R}^{k \times n})$ time complexity. It can increase flexibility by few standard techniques like including weights, adding regularization terms.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="1.4.-Linear-Programming(Mature-technology)/Nonlinear-Programming">1.4. Linear Programming(Mature technology)/Nonlinear Programming<a class="anchor-link" href="#1.4.-Linear-Programming(Mature-technology)/Nonlinear-Programming"> </a></h4><p>Form like</p>
$$
\mathbf{x}^* = \underset{\mathbf{x}}{\mathrm{argmin}} \,\ \mathbf{c}^T \mathbf{x} \quad s.t. \quad \mathbf{a}_i^T \mathbf{x} \le \mathbf{b}_i, \,\ i = 1,\cdots,m
$$<p>are called linear programming. There is no analytical formula for solution, but there are reliable and efficient algorithms and software that have $n^2m$ time complexity.<br><br></p>
<p>Form like</p>
$$
\mathbf{x}^* = \underset{\mathbf{x}}{\mathrm{argmin}} \,\ f(\mathbf{x}) \quad s.t. \quad g_i(\mathbf{x}) \le 0, \,\ h_j(\mathbf{x}) = 0, \,\ i=1,\cdots,p, \,\ j=1,\cdots,q
$$<p>are called nonlinear programming. There are local optimization methods and global optimization methods.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="1.5.-Convex-Optimization">1.5. Convex Optimization<a class="anchor-link" href="#1.5.-Convex-Optimization"> </a></h4><p>Form like</p>
$$
\mathbf{x}^* = \underset{\mathbf{x}}{\mathrm{argmin}} \,\ f(\mathbf{x}) \quad s.t. \quad g_i(\mathbf{x}) \le 0, \,\ h_j(\mathbf{x}) = 0, \,\ i=1,\cdots,p, \,\ j=1,\cdots,q, \,\ f \,\ \text{is a convex function.}
$$<p>This problem includes least-squares problems and linear programming as special cases. $g_i$ is called an inequality constraint function and $h_j$ is called an equality constraint function. There are no analytical solutions, but reliable and efficient algorithms that have $\text{max}\{n^3, n^2p, n^2q\}$ time complexity roughly.<br><br></p>
<p>Using convex optimization often difficult to recognize. However, there are many tricks for transforming problems into convex form. Many problems can be solved via convex optimization.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="1.6.-Gradient-based-Optimization">1.6. Gradient based Optimization<a class="anchor-link" href="#1.6.-Gradient-based-Optimization"> </a></h4><p>A gradient based optimization is an algorithm to solve problems of the form
$$
\mathbf{x}^* = \underset{\mathbf{x}}{\mathrm{argmin}} \,\ f(\mathbf{x}) 
$$
with the search directions defined by the gradient of the function at the current point.</p>

</div>
</div>
</div>
</div>
 

