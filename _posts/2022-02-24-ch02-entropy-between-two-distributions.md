---
keywords: fastai
description: Information theory summary note.
title: "[InformationTheory] CH02. Entropy Between Two Distributions"
toc: false
badges: false
comments: false
categories: [information-theory]
hide_{github,colab,binder,deepnote}_badge: true
nb_path: _notebooks/ch02-entropy-between-two-distributions.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/ch02-entropy-between-two-distributions.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="2.1.-Cross-Entropy">2.1. Cross Entropy<a class="anchor-link" href="#2.1.-Cross-Entropy"> </a></h4><h5 id="Definition.2.1.-Cross-Entropy">Definition.2.1. Cross Entropy<a class="anchor-link" href="#Definition.2.1.-Cross-Entropy"> </a></h5><p>For probability mess functions $p$ and $q$, the cross entropy $H[p, q]$ is
$$
H[p, q] = \mathbb{E}_{x \sim p}[\log q(x)] = - \sum_{i = 1}^N p(x_i) \log_2 q(x_i) dx.
$$</p>
<p>Also, for probability density functions $p$ and $q$, the cross entropy $H[p, q]$ is
$$
H[p, q] = \mathbb{E}_{x \sim p}[\log q(x)] = - \int_x p(x) \log_2 q(x) dx.
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Ex)</strong> <br>
$$
\begin{matrix}
p = [\frac{1}{3} \,\ \frac{1}{3} \,\ \frac{1}{3}] \\
q_1 = [\frac{1}{3} \,\ \frac{1}{3} \,\ \frac{1}{3}] \\
q_2 = [\frac{1}{6} \,\ \frac{2}{3} \,\ \frac{1}{6}] \\
q_3 = [\frac{1}{12} \,\ \frac{5}{6} \,\ \frac{1}{12}] \\
r_1 = [\frac{1}{6} \,\ \frac{1}{6} \,\ \frac{2}{3}] \\
r_2 = [\frac{1}{12} \,\ \frac{1}{12} \,\ \frac{5}{6}] \\
\end{matrix}
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\begin{matrix}
H[p, q_1] = 1.584962500721156 \\
H[p, q_2] = 1.9182958340544896 \\
H[p, q_3] = 2.477653135758702 \\
H[p, r_1] = 1.9182958340544896 \\
H[p, r_2] = 2.477653135758702 \\
\end{matrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Theorem.2.1.">Theorem.2.1.<a class="anchor-link" href="#Theorem.2.1."> </a></h5><p>If $p = q$,</p>
$$
H[p, q] = H[p]
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="2.2.-Kullback-Leibler-Divergence">2.2. Kullback-Leibler Divergence<a class="anchor-link" href="#2.2.-Kullback-Leibler-Divergence"> </a></h4><h5 id="Definition.2.2.-Kullback-Leibler-Divergence">Definition.2.2. Kullback-Leibler Divergence<a class="anchor-link" href="#Definition.2.2.-Kullback-Leibler-Divergence"> </a></h5><p>For probability mess functions $p$ and $q$, the KL divergence $D_{KL}(p||q)$ is
$$
D_{KL}(p||q) = \mathbb{E}_{x \sim p}\left[\frac{\log p(x)}{\log q(x)}\right] = H[p, q] - H[p] = \sum_{i = 1}^N p(x_i) \log_2 \frac{p(x_i)}{q(x_i)}.
$$</p>
<p>For probability density functions $p$ and $q$, the KL divergence $D_{KL}(p||q)$ is
$$
D_{KL}(p||q) = \mathbb{E}_{x \sim p}\left[\frac{\log p(x)}{\log q(x)}\right] = H[p, q] - H[p] = \int_x p(x) \log_2 \frac{p(x)}{q(x)}.
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Ex)</strong> <br>
$$
\begin{matrix}
p = [\frac{1}{3} \,\ \frac{1}{3} \,\ \frac{1}{3}] \\
q_1 = [\frac{1}{3} \,\ \frac{1}{3} \,\ \frac{1}{3}] \\
q_2 = [\frac{1}{6} \,\ \frac{2}{3} \,\ \frac{1}{6}] \\
q_3 = [\frac{1}{12} \,\ \frac{5}{6} \,\ \frac{1}{12}] \\
r_1 = [\frac{1}{6} \,\ \frac{1}{6} \,\ \frac{2}{3}] \\
r_2 = [\frac{1}{12} \,\ \frac{1}{12} \,\ \frac{5}{6}] \\
\end{matrix}
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\begin{matrix}
D_{KL}(p||q_1) = 0.0 \\
D_{KL}(p||q_2) = 0.3333333333333333 \\
D_{KL}(p||q_3) = 0.8926906350375459 \\
D_{KL}(p||r_1) = 0.3333333333333333 \\
D_{KL}(p||r_2) = 0.8926906350375459 \\
\end{matrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Theorem.2.2.">Theorem.2.2.<a class="anchor-link" href="#Theorem.2.2."> </a></h5><p>If $p = q$,</p>
$$
D_{KL}(p||q) = H[p, q] - H[p] = 0
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since both operation doesn't allow commutative property, remark that cross entropy and kl divergence can't be used to distance metric.</p>

</div>
</div>
</div>
</div>
 

