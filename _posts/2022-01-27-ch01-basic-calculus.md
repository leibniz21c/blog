---
keywords: fastai
description: Calculus summary note.
title: "[Calculus] CH01. Basic Calculus"
toc: false
badges: false
comments: false
categories: [calculus]
hide_{github,colab,binder,deepnote}_badge: true
nb_path: _notebooks/ch01-basic-calculus.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/ch01-basic-calculus.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="1.0.-Functions">1.0. Functions<a class="anchor-link" href="#1.0.-Functions"> </a></h4><p>We'll use <strong>numerator-layout notation</strong>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Definition.1.1.-A-Function-$f$-of-$n$-Variables">Definition.1.1. A Function $f$ of $n$ Variables<a class="anchor-link" href="#Definition.1.1.-A-Function-$f$-of-$n$-Variables"> </a></h5><p>A function $f$ of $n$ variables is a rule that assigns a number $z = f(x_1, x_2, \cdots, x_n)$ to an $n$-tuple $(x_1, x_2, \cdots, x_n)$ of real numbers.</p>
$$
f : \mathbb{R}^n \rightarrow \mathbb{R}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Definition.1.2.-Limits-of-a-Function-of-$n$-Variables">Definition.1.2. Limits of a Function of $n$ Variables<a class="anchor-link" href="#Definition.1.2.-Limits-of-a-Function-of-$n$-Variables"> </a></h5><p>Let $f$ be a function of $n$ variables whose domain $D$ includes points arbitrarily close to $\mathbf{a}$.<br>
Then we say that the limit of $f(\mathbf{x})$ as $\mathbf{x}$ approaches $\mathbf{a}$ is $L$ and we write</p>
$$
\lim_{\mathbf{x} \rightarrow \mathbf{a}} f(\mathbf{x}) = L
$$<p>if for every number $\epsilon &gt; 0$ there is a corresponding number $\delta &gt; 0$ such that if $\mathbf{x} \in D$ and $0 &lt; ||\mathbf{x} - \mathbf{a}|| &lt; \delta$ then $|f(\mathbf{x}) - L| &lt; \epsilon $.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Definition.1.3.-Continuity">Definition.1.3. Continuity<a class="anchor-link" href="#Definition.1.3.-Continuity"> </a></h5><p>A function $f$ of two variables is called continuous at $\mathbf{a}$ if</p>
$$
\lim_{\mathbf{x} \rightarrow \mathbf{a}} f(\mathbf{x}) = f(\mathbf{a}).
$$<p>We say $f$ is continuous on $D$ if $f$ is continuous at every point $\mathbf{a}$ in $D$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="1.1.-Partial-Derivatives-and-Gradients">1.1. Partial Derivatives and Gradients<a class="anchor-link" href="#1.1.-Partial-Derivatives-and-Gradients"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Definition.1.4.-Partial-Derivative-of-a-Function-of-$n$-Variables">Definition.1.4. Partial Derivative of a Function of $n$ Variables<a class="anchor-link" href="#Definition.1.4.-Partial-Derivative-of-a-Function-of-$n$-Variables"> </a></h5><p>If $f$ is a function of $n$ variables, its partial derivatives are</p>
$$
\frac{\partial}{\partial x_i} f(\mathbf{x}) = \lim_{h \rightarrow 0} \frac{f(x_1, \cdots, x_i + h, \cdots, x_n) - f(x_1, \cdots, x_i, \cdots, x_n)}{h} \quad \text{for} \,\ i=1,2,\cdots,n
$$<p>Also, a vector of partial derivatives as above is called gradient;</p>
$$
\nabla_\mathbf{x} f(\mathbf{x}) = (\frac{\partial}{\partial x_1} f(\mathbf{x}), \cdots, \frac{\partial}{\partial x_n} f(\mathbf{x}))
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Theorem.1.1.">Theorem.1.1.<a class="anchor-link" href="#Theorem.1.1."> </a></h5><p>Let $V = \{f | f: \mathbb{R}^n \rightarrow \mathbb{R} \,\ \text{and} \,\ f \,\ \text{is differentiable}\}, \,\ W = \{f | f: \mathbb{R}^n \rightarrow \mathbb{R}^n\}$, and $D: V \rightarrow W, \,\ D(f) = \nabla f$.<br>
Then the differential transformation $D$ is a linear transformation and it is called differential operator $D$.</p>
<p><strong>Proof.</strong> <br>
$$
\text{For} \,\ 0 \in V, \,\ D(0) = \mathbf{0} \in W. \qquad \cdots \,\ (1)
$$</p>
$$
\begin{align}
\text{For} \,\ f, g \in V, \,\ D(f + g) &amp;= \nabla_\mathbf{x} (f + g) \\
                                        &amp;= \nabla_\mathbf{x} f + \nabla_\mathbf{x} g \\
                                        &amp;= D(f) + D(g) \qquad \cdots \,\ (2) \\
\end{align}
$$$$
\begin{align}
\text{For} \,\ c \in \mathbb{R}, \,\ f \in V, \,\ D(cf) &amp;= \nabla_\mathbf{x} f \\
                                                        &amp;= c \nabla_\mathbf{x} f \\
                                                        &amp;= c D(f) \qquad \cdots \,\ (3) \\
\end{align}
$$$$
\therefore \,\ D \,\ \text{is a linear transformation} \quad (\because \,\ (1), \,\ (2), \,\ \text{and} \,\ (3)) \quad \blacksquare
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Definition.1.5.-Linear-Approximation">Definition.1.5. Linear Approximation<a class="anchor-link" href="#Definition.1.5.-Linear-Approximation"> </a></h5><p>The linear approximation of $f$ at $\mathbf{x}_0$ is given as</p>
$$
f(\mathbf{x}) \approx f(\mathbf{x}_0) + \nabla_\mathbf{x} f(\mathbf{x}_0)^T (\mathbf{x} - \mathbf{x}_0).
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Definition.1.6.-Differentiable">Definition.1.6. Differentiable<a class="anchor-link" href="#Definition.1.6.-Differentiable"> </a></h5><p>If $y = f(\mathbf{x})$, $f$ is differentiable at $\mathbf{x}_0$ if $\Delta u$ can be expressed in the form</p>
$$
\Delta y = \nabla_\mathbf{x} f(\mathbf{x}_0)^T \begin{bmatrix} \Delta x_1 \\ \Delta x_2 \\ \vdots \\ \Delta x_n \end{bmatrix} + \mathbf{\epsilon}^T \begin{bmatrix} \Delta x_1 \\ \Delta x_2 \\ \vdots \\ \Delta x_n \end{bmatrix} \,\ \text{where} \,\ \mathbf{\epsilon} \rightarrow \mathbf{0} \,\ \text{as} \,\ \begin{bmatrix} \Delta x_1 \\ \Delta x_2 \\ \vdots \\ \Delta x_n \end{bmatrix} \rightarrow \mathbf{0}.
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Theorem.1.2.">Theorem.1.2.<a class="anchor-link" href="#Theorem.1.2."> </a></h5><p>If the gradient $\nabla_\mathbf{x} f$ exist near $\mathbf{x}_0$ and are continuous at $\mathbf{x}_0$, then $f$ is differentiable at $\mathbf{x}_0$.</p>
<p><strong>Proof.</strong> <br></p>
<p>Trivial.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Definition.1.7.-Differentials">Definition.1.7. Differentials<a class="anchor-link" href="#Definition.1.7.-Differentials"> </a></h5><p>For a differentiable function $y = f(\mathbf{x})$, let the differentials $dx_i$ for $i=1,2,\cdots,n$ be independent variables.<br>
Then the total differential $dy$ is defined by</p>
$$
dy = \nabla_\mathbf{x} f^T \begin{bmatrix} dx_1 \\ dx_2 \\ \vdots \\ dx_n \end{bmatrix}.
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="1.2.-Properties-of-Partial-Derivatives">1.2. Properties of Partial Derivatives<a class="anchor-link" href="#1.2.-Properties-of-Partial-Derivatives"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Theorem.1.3.">Theorem.1.3.<a class="anchor-link" href="#Theorem.1.3."> </a></h5><p>Let $f: \mathbb{R}^n \rightarrow \mathbb{R}, \,\ g: \mathbb{R}^n \rightarrow \mathbb{R},$ and $c \in \mathbf{R}$.</p>
<ol>
<li>Constant Multiple Rule: $\nabla_\mathbf{x} f(c\mathbf{x}) = c\nabla_\mathbf{x} f(\mathbf{x})$</li>
<li>Sum Rule: $\nabla_\mathbf{x} \{ f(\mathbf{x}) + g(\mathbf{x})\} = \nabla_\mathbf{x} f(\mathbf{x}) + \nabla_\mathbf{x} g(\mathbf{x})$</li>
<li>Product Rule: $\nabla_\mathbf{x} \{ f(\mathbf{x})g(\mathbf{x}) \} = (\nabla_\mathbf{x} f(\mathbf{x}))g(\mathbf{x}) + f(\mathbf{x}) (\nabla_\mathbf{x} g(\mathbf{x}))$</li>
<li>Quotient Rule: $\nabla_\mathbf{x} \left\{ \frac{f(\mathbf{x})}{g(\mathbf{x})} \right\} = \frac{(\nabla_\mathbf{x} f(\mathbf{x}))g(\mathbf{x}) - f(\mathbf{x}) (\nabla_\mathbf{x} g(\mathbf{x}))}{g(\mathbf{x})^2}$</li>
</ol>
<p><strong>Proof.</strong> <br>
Trivials.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Theorem.1.4.-(Chain-Rule)">Theorem.1.4. (Chain Rule)<a class="anchor-link" href="#Theorem.1.4.-(Chain-Rule)"> </a></h5><p>If $y = f(\mathbf{x})$ and $\mathbf{x} = \mathbf{g}(t)$ are differentiable functions, then</p>
$$
\frac{dy}{dt} = \nabla_\mathbf{x} f^T \begin{bmatrix} \frac{dx_1}{dt} \\ \frac{dx_2}{dt} \\ \vdots \\ \frac{dx_n}{dt} \end{bmatrix}
$$<p><strong>Proof.</strong> <br></p>
$$
\begin{matrix}
\Delta y = \nabla_\mathbf{x} f(\mathbf{x})^T \begin{bmatrix} \Delta x_1 \\ \Delta x_2 \\ \vdots \\ \Delta x_n \end{bmatrix} + \mathbf{\epsilon}^T \begin{bmatrix} \Delta x_1 \\ \Delta x_2 \\ \vdots \\ \Delta x_n \end{bmatrix} \\
\frac{\Delta y}{\Delta t} = \nabla_\mathbf{x} f(\mathbf{x})^T \begin{bmatrix} \frac{\Delta x_1}{\Delta t} \\ \frac{\Delta x_2}{\Delta t} \\ \vdots \\ \frac{\Delta x_n}{\Delta t} \end{bmatrix} + \mathbf{\epsilon}^T \begin{bmatrix} \frac{\Delta x_1}{\Delta t} \\ \frac{\Delta x_2}{\Delta t} \\ \vdots \\ \frac{\Delta x_n}{\Delta t} \end{bmatrix} \\
\lim_{\Delta t \rightarrow 0} \frac{\Delta y}{\Delta t} = \lim_{\Delta t \rightarrow 0} \left\{ \nabla_\mathbf{x} f(\mathbf{x})^T \begin{bmatrix} \frac{\Delta x_1}{\Delta t} \\ \frac{\Delta x_2}{\Delta t} \\ \vdots \\ \frac{\Delta x_n}{\Delta t} \end{bmatrix} + \mathbf{\epsilon}^T \begin{bmatrix} \frac{\Delta x_1}{\Delta t} \\ \frac{\Delta x_2}{\Delta t} \\ \vdots \\ \frac{\Delta x_n}{\Delta t} \end{bmatrix} \right\} \\
\therefore \quad \frac{dy}{dt} = \nabla_\mathbf{x} f(\mathbf{x})^T \begin{bmatrix} \frac{dx_1}{dt} \\ \frac{dx_2}{dt} \\ \vdots \\ \frac{dx_n}{dt} \end{bmatrix} \qquad (\therefore \,\ \mathbf{\epsilon}, \begin{bmatrix} \Delta x_1 \\ \Delta x_2 \\ \vdots \\ \Delta x_n \end{bmatrix} \rightarrow \mathbf{0} \,\ \text{as} \,\ \Delta t \rightarrow 0) \quad \blacksquare
\end{matrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Theorem.1.5.-(Chain-Rule)">Theorem.1.5. (Chain Rule)<a class="anchor-link" href="#Theorem.1.5.-(Chain-Rule)"> </a></h5><p>If $y = f(\mathbf{x})$ and $\mathbf{x} = \mathbf{g}(\mathbf{t})$ are differentiable functions that $f: \mathbb{R}^n \rightarrow \mathbb{R}, \,\ \mathbf{g}: \mathbb{R}^m \rightarrow \mathbb{R}^n $, then</p>
$$
\nabla_\mathbf{t} f(\mathbf{x}) = \begin{bmatrix} \frac{\partial}{\partial t_1}f(\mathbf{x}) \\ \frac{\partial}{\partial t_t}f(\mathbf{x}) \\ \vdots \\ \frac{\partial}{\partial t_m}f(\mathbf{x}) \end{bmatrix}_{m \times 1}, \quad \frac{\partial}{\partial t_i}f(\mathbf{x}) = \nabla_\mathbf{x} f(\mathbf{x})^T \begin{bmatrix} \frac{\partial x_1}{\partial t_i} \\ \frac{\partial x_2}{\partial t_i} \\ \vdots \\ \frac{\partial x_n}{\partial t_i} \end{bmatrix}
$$<p><strong>Proof.</strong> <br></p>
<p>Trivials.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="Theorem.1.6.">Theorem.1.6.<a class="anchor-link" href="#Theorem.1.6."> </a></h5><p>Followings are true.</p>
<ol>
<li>$ \nabla_\mathbf{w} (\mathbf{x}^T \mathbf{w}) = \mathbf{x} $</li>
<li>$ \nabla_\mathbf{w} (\mathbf{w}^T R \mathbf{w}) = (R + R^T) \mathbf{w} $</li>
</ol>
<p><strong>Proof.</strong> <br></p>
<p>Trivials.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h5 id="Application.1.1.">Application.1.1.<a class="anchor-link" href="#Application.1.1."> </a></h5><p>Solve a equation $\nabla_\mathbf{w} || \mathbf{X}_{m \times n} \mathbf{w}_{n \times 1} - \hat{\mathbf{y}}_{m \times 1} ||^2 = \mathbf{0} $</p>
<p><strong>Solve.</strong> <br></p>
$$
\begin{align}
&amp;\nabla_\mathbf{w} (X\mathbf{w} - \hat{\mathbf{y}})^T (X\mathbf{w} - \hat{\mathbf{y}}) \\
&amp;= \nabla_\mathbf{w} (\mathbf{w}^T X^T - \hat{\mathbf{y}}^T)^T (X\mathbf{w} - \hat{\mathbf{y}}) \\
&amp;= \nabla_\mathbf{w} (\mathbf{w}^T X^T X \mathbf{w} - \mathbf{w}^T X^T \hat{\mathbf{y}} - \hat{\mathbf{y}}^T X \mathbf{w} + \hat{\mathbf{y}}^T \hat{\mathbf{y}}) \\
&amp;= \nabla_\mathbf{w} (\mathbf{w}^T X^T X \mathbf{w} - 2\hat{\mathbf{y}}^T X \mathbf{w} + \hat{\mathbf{y}}^T \hat{\mathbf{y}}) \\
&amp;= 2X^TX\mathbf{w} - 2X^T\hat{\mathbf{y}} = \mathbf{0} \\
\,\ \\
&amp;\qquad X^TX \mathbf{w} = X^T \hat{\mathbf{y}} \\
&amp;\therefore \,\ \mathbf{w} = (X^TX)^{-1}X^T\hat{\mathbf{y}}
\end{align}
$$<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="1.3.-Direction-of-Gradients">1.3. Direction of Gradients<a class="anchor-link" href="#1.3.-Direction-of-Gradients"> </a></h4><p>Consider the directional vector $\mathbf{d}$ that maximizes $\mathcal{L}(\mathbf{w} + \eta \mathbf{d}) - \mathcal{L}(\mathbf{w}) (\le 0)$.<br>
Geometically, $\mathbf{d}$ must be same direction of $\mathcal{L}(\mathbf{w})$, therefore $\cos(\phi) = 1$.</p>
$$
\therefore \,\ \mathbf{d} = \frac{\nabla_\mathbf{w} \mathcal{L}(\mathbf{w})}{||\nabla_\mathbf{w} \mathcal{L}(\mathbf{w})||}
$$<p>that is, the direction of the gradient is the direction in which the function value increases.</p>

</div>
</div>
</div>
</div>
 

