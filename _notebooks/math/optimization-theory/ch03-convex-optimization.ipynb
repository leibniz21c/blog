{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "251fd9a4-0f29-4862-aa66-857c313e79aa",
   "metadata": {},
   "source": [
    "# \"[OptimizationTheory] CH03. Convex Optimization\"\n",
    "> Optimization theory summary note.\n",
    "\n",
    "- toc: false\n",
    "- badges: false\n",
    "- comments: false\n",
    "- categories: [optimization-theory]\n",
    "- hide_{github,colab,binder,deepnote}_badge: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1a44f0-4db4-4574-ae40-b586ca3fa045",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.1. Introduction\n",
    "Consider the objective function that is differentiable in optimization problem. If $\\mathbf{w}^*$ is optimal, $\\nabla_\\mathbf{w} L$ must be zero vector. This condition is called gradient necessary condition. In optimization problem, we have to find global optimal. However, we only find local optimal everytime. Also, if we find the solution of $\\nabla_\\mathbf{w} L = \\mathbf{0}$, there is no guarantee that the solution is a global optimal. <br><br>\n",
    "\n",
    "Here is something to think about.\n",
    "\n",
    "> [Dauphin14] Y. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, Y. Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization.\n",
    "\n",
    "Above paper suggest that local minima problem is actually a very rare case that does not occur in a high dimensional space. First reason is gradient necessary condition. Every elements in gradient must be a zero at some $\\mathbf{w}^*$. But this is very rare case. Also, in every direction, gradient must form a convex shape in a high-dimensional space. But, the probability of that happening is close to zero.\n",
    "\n",
    "> Intuitively, in high dimensions, the chance that all the directions around a critical point lead upward is exponentially small w.r.t. the number of dimensions, unless the critical point is the global minimum or stands at an error level close to it, i.e., it is unlikely one can find a way to go further down.\n",
    "\n",
    "\n",
    "In this context, since both local minima and the global minima are same, it can be seen that a convex function can be good loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bfb94b-2506-4cd7-b5d7-6904d6933adc",
   "metadata": {},
   "source": [
    "#### 3.2. Convex Optimization\n",
    "\n",
    "##### Definition.3.1. Convex Set\n",
    "A set $S$ is said to be convex if\n",
    "\n",
    "$$\n",
    "\\mathbf{x}, \\mathbf{y} \\in S, \\,\\ \\text{then} \\,\\ t\\mathbf{x} + (1 - t)\\mathbf{y} \\in S, \\,\\ \\text{for} \\,\\ t \\in [0, 1]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9d423e-4482-40c0-9877-5bc3659c1e3c",
   "metadata": {},
   "source": [
    "##### Definition.3.2. Convex Function\n",
    "A function $f: S \\rightarrow \\mathbb{R}$ is said to be convex if\n",
    "\n",
    "$$\n",
    "^\\forall \\mathbf{x}_1, \\mathbf{x}_2 \\in S, \\,\\ f(t\\mathbf{x}_1 + (1 - t)\\mathbf{x}_2) \\le tf(\\mathbf{x}_1) + (1 - t)f(\\mathbf{x}_2) \\,\\ \\text{for} \\,\\ t \\in [0, 1]\n",
    "$$\n",
    "where $S$ is convex subset of a real vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e7c21a-cd9f-4fbb-91a6-1d4deb100cb6",
   "metadata": {},
   "source": [
    "##### Theorem.3.1. \n",
    "In convex function, some local minimum is a global minimum.\n",
    "\n",
    "__Proof.__<br>\n",
    "Trivial(proof by contradiction)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d8c491-3473-475a-b7bc-a03fa920f874",
   "metadata": {},
   "source": [
    "##### Theorem.3.2.\n",
    "A twice differentiable function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is a convex function if and only if $\\nabla^2 f \\succeq 0$.\n",
    "\n",
    "__Proof.__<br>\n",
    "Suppose $f$ has a positive semidefinite hessian matrix.<br>\n",
    "Then for some $\\mathbf{x}_0, \\mathbf{x}_1$ in the domain, and $t \\in [0, 1]$, we have\n",
    "\n",
    "$$\n",
    "g(t) = f(t\\mathbf{x}_0 + (1- t)\\mathbf{x}_1)\n",
    "$$\n",
    "\n",
    "which have the first and second derivative "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd4fd4-4710-4f87-bbc2-5c4363ef6b55",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{matrix}\n",
    "\\frac{dg}{dt} = (\\mathbf{x}_0 - \\mathbf{x}_1)^T \\nabla_\\mathbf{x} f(t\\mathbf{x}_0 + (1 - t)\\mathbf{x}_1) \\\\\n",
    "\\frac{d^2g}{dt^2} = (\\mathbf{x}_0 - \\mathbf{x}_1)^T \\nabla_\\mathbf{x}^2 f(t\\mathbf{x}_0 + (1 - t)\\mathbf{x}_1)(\\mathbf{x}_0 - \\mathbf{x}_1) \\\\\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf4a9df-d30b-4dcd-ad3e-d97785201986",
   "metadata": {},
   "source": [
    "Since the hessian matrix of $f$ is positive semidefinite, $\\frac{d^2g}{dt^2} \\ge 0$ for $t \\in [0, 1]$.<br>\n",
    "Then we can get\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "g(0) \\ge g(t) + g^\\prime(t)(-t) \\\\\n",
    "g(1) \\ge g(t) + g^\\prime(t)(1 - t) \\\\\n",
    "\\end{matrix} \\quad (\\because \\,\\ \\text{Taylor's theorem})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8daab0-d376-42a7-af62-6a1ef5f5ddc4",
   "metadata": {},
   "source": [
    "Then \n",
    "$$\n",
    "g(t) \\le tg(1) + (1 - t)g(0)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\therefore \\,\\ ^\\forall \\mathbf{x}_0, \\mathbf{x}_1 \\in D, \\,\\ f(t\\mathbf{x}_0 + (1 - t)\\mathbf{x}_1) \\le tf(\\mathbf{x}_0) + (1 - t)\\mathbf{x}_1 \\quad \\blacksquare\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b733c8-508b-45e3-b958-f74398a6b8df",
   "metadata": {},
   "source": [
    "##### Definition.3.3. Convex Optimization Problem\n",
    "A convex optimization problem is an optimization problem in which the objective function is a convex function and the feasible set is a convex set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96edccbf-8e26-40d7-a5d6-7a8d29110fbf",
   "metadata": {},
   "source": [
    "#### 3.3. Properties\n",
    "\n",
    "Followings are convex function:\n",
    "\n",
    "- Exponential function\n",
    "- Power function(in some case)\n",
    "- Logarithmic function\n",
    "- Affine function\n",
    "- Quadratic function \n",
    "- Mean square error\n",
    "- Max function\n",
    "- Norm function\n",
    "- Softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b65458-1c0e-4f54-9b80-fad41d250e7a",
   "metadata": {},
   "source": [
    "##### Theorem.3.3.\n",
    "Let $f(\\mathbf{x}) = h(g(\\mathbf{x})) = h(g_1(\\mathbf{x}), \\cdots, g_k(\\mathbf{x}))$ <br>\n",
    "where $g: \\mathbb{R}^n \\rightarrow \\mathbb{R}^k, \\,\\ h : \\mathbb{R}^k \\rightarrow \\mathbb{R}, \\,\\ \\text{and} \\,\\ f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$.<br>\n",
    "Then \n",
    "\n",
    "- $f$ is convex if $h$ is convex and nondecreasing in each argument, g is convex.\n",
    "- $f$ is convex if $h$ is convex and nonincreasing in each argument, g is concave.\n",
    "\n",
    "__Proof.__<br>\n",
    "Trivial(by chain rule and above theorems)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
