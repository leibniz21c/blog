{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "251fd9a4-0f29-4862-aa66-857c313e79aa",
   "metadata": {},
   "source": [
    "# \"[OptimizationTheory] CH01. Introduction\"\n",
    "> Optimization theory summary note.\n",
    "\n",
    "- toc: false\n",
    "- badges: false\n",
    "- comments: false\n",
    "- categories: [optimization-theory]\n",
    "- hide_{github,colab,binder,deepnote}_badge: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1a44f0-4db4-4574-ae40-b586ca3fa045",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.0. Optimization\n",
    "> Optimization is the process of creating something that is as effective as possible. From a mathematical perspective, optimization deals with finding the maxima and minima of a function that depends on one or more variables.\n",
    "\n",
    "For example, determin the optimum analytically\n",
    "\n",
    "$$\n",
    "z = z_0 + \\frac{m}{c}(v_0 + \\frac{mg}{c})(-\\exp(-(c/m)t)) - \\frac{mg}{c}t \\quad \\text{where} \\,\\ g = 9.81, \\,\\ z_0 = 100, \\,\\ v_0 = 55, \\,\\ m = 80, \\,\\ c = 15\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e4cf65-347a-49af-bd29-db489a16d9b8",
   "metadata": {},
   "source": [
    "##### Definition.1.1. Optimization Problem\n",
    "Following problem that find optimal solution $\\mathbf{x}$ are called __optimization problem__.\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^* = \\underset{\\mathbf{x}}{\\mathrm{argmin}} \\,\\ f(\\mathbf{x})\n",
    "$$\n",
    "where $\\mathbf{x}$ is optimization variable, $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is objective function.<br>\n",
    "In this situation, $\\mathbf{x}^*$ is called optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22d9746-2089-4ae3-b22b-5ce82e0aa60f",
   "metadata": {},
   "source": [
    "It is very difficult to solve general optimization problem. Many optimization method involve some compromise, e.g., very long computation time, or not always finding the solution. However, certain problems can be solved efficiently and reliably. For example,\n",
    "\n",
    "- Least-squares problems\n",
    "- Linaer programming problems\n",
    "- Convex optimization problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3ee185-1885-4e91-99e6-bf478fa32835",
   "metadata": {},
   "source": [
    "#### 1.1. Classification of Optimization Method\n",
    "\n",
    "There are some kinds of optimization method like\n",
    "\n",
    "- Search\n",
    "- Least-Squares\n",
    "- Linear Programming/Nonlinear Optimization\n",
    "- Convex Optimization\n",
    "- Gradient based Optimization\n",
    "\n",
    "Above methods are not separated by analytical method and numerical method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee39da5-73bd-4eed-812c-0e95fe01bd15",
   "metadata": {},
   "source": [
    "#### 1.2. Search based Algorithm\n",
    "\n",
    "Search based algorithms are simplest method in optimization theory. It just computes the objective function value for many $x$ candidates and finds the minimum point. Examples of algorithms are as follows.\n",
    "\n",
    "- Grid Search\n",
    "- Golden-Section Search\n",
    "\n",
    "These algorithms can be used effectively for single variable functions, but for high-dimensional multivariate functions, the amount of computation increases exponentially. And also, the solution does not guarantee a global optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb1367b-5f93-4c73-99cf-58023fdf0f7f",
   "metadata": {},
   "source": [
    "#### 1.3. Least-Squares(Mature technology)\n",
    "\n",
    "Form like\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^* = \\underset{\\mathbf{x}}{\\mathrm{argmin}} \\,\\ || A\\mathbf{x} - \\mathbf{b} ||_2^2\n",
    "$$\n",
    "\n",
    "are called least-squares problem. Optimal solution can be obtained analytically e.g., $\\mathbf{x}^* = (A^T A)^{-1} A^T \\mathbf{b}$. There are reliable and efficient algorithms and software that have $n^2k \\,\\ (A \\in \\mathbb{R}^{k \\times n})$ time complexity. It can increase flexibility by few standard techniques like including weights, adding regularization terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b0cb56-27ab-456a-9383-a9ab4faaef5f",
   "metadata": {},
   "source": [
    "#### 1.4. Linear Programming(Mature technology)/Nonlinear Programming\n",
    "\n",
    "Form like\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^* = \\underset{\\mathbf{x}}{\\mathrm{argmin}} \\,\\ \\mathbf{c}^T \\mathbf{x} \\quad s.t. \\quad \\mathbf{a}_i^T \\mathbf{x} \\le \\mathbf{b}_i, \\,\\ i = 1,\\cdots,m\n",
    "$$\n",
    "\n",
    "are called linear programming. There is no analytical formula for solution, but there are reliable and efficient algorithms and software that have $n^2m$ time complexity.<br><br>\n",
    "\n",
    "Form like\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^* = \\underset{\\mathbf{x}}{\\mathrm{argmin}} \\,\\ f(\\mathbf{x}) \\quad s.t. \\quad g_i(\\mathbf{x}) \\le 0, \\,\\ h_j(\\mathbf{x}) = 0, \\,\\ i=1,\\cdots,p, \\,\\ j=1,\\cdots,q\n",
    "$$\n",
    "are called nonlinear programming. There are local optimization methods and global optimization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19d0869-72b7-4af2-a721-175d553b571a",
   "metadata": {},
   "source": [
    "#### 1.5. Convex Optimization\n",
    "\n",
    "Form like\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^* = \\underset{\\mathbf{x}}{\\mathrm{argmin}} \\,\\ f(\\mathbf{x}) \\quad s.t. \\quad g_i(\\mathbf{x}) \\le 0, \\,\\ h_j(\\mathbf{x}) = 0, \\,\\ i=1,\\cdots,p, \\,\\ j=1,\\cdots,q, \\,\\ f \\,\\ \\text{is a convex function.}\n",
    "$$\n",
    "\n",
    "This problem includes least-squares problems and linear programming as special cases. $g_i$ is called an inequality constraint function and $h_j$ is called an equality constraint function. There are no analytical solutions, but reliable and efficient algorithms that have $\\text{max}\\{n^3, n^2p, n^2q\\}$ time complexity roughly.<br><br>\n",
    "\n",
    "Using convex optimization often difficult to recognize. However, there are many tricks for transforming problems into convex form. Many problems can be solved via convex optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c510f86-9199-4e43-971b-cd48e9ba2204",
   "metadata": {},
   "source": [
    "#### 1.6. Gradient based Optimization\n",
    "A gradient based optimization is an algorithm to solve problems of the form\n",
    "$$\n",
    "\\mathbf{x}^* = \\underset{\\mathbf{x}}{\\mathrm{argmin}} \\,\\ f(\\mathbf{x}) \n",
    "$$\n",
    "with the search directions defined by the gradient of the function at the current point."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
