{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "251fd9a4-0f29-4862-aa66-857c313e79aa",
   "metadata": {},
   "source": [
    "# \"[OptimizationTheory] CH05. Constrained Optimizations\"\n",
    "> Optimization theory summary note.\n",
    "\n",
    "- toc: false\n",
    "- badges: false\n",
    "- comments: false\n",
    "- categories: [optimization-theory]\n",
    "- hide_{github,colab,binder,deepnote}_badge: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1a44f0-4db4-4574-ae40-b586ca3fa045",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 5.1. Introduction\n",
    "Many practical optimization problems have constraints like equality and inequality constraint. Consider following problem:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\mathrm{argmin}} || \\mathbf{y} - X\\mathbf{w} ||_2^2 \\quad s.t. \\quad ||\\mathbf{w}||_2^2 \\le 1. \n",
    "$$\n",
    "\n",
    "Above problem is inequality constrained optimization problem, and luckly $MSE$ cost function is convex function. Therefore, we can obtain optimal solution analytically. However, if obtained optimal solution doesn't satisfy the inequality condition, we have to find another solution. We can define general constained optimization problem like following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b30414f-39b1-4904-a9b7-d6e6e27a8a08",
   "metadata": {},
   "source": [
    "##### Definition.5.1. Constrained Optimization Problem\n",
    "Following problem that find optimal solution $\\mathbf{x}$ are called __constrained optimization problem__.\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\mathbf{x}^* = \\underset{\\mathbf{x}}{\\mathrm{argmin}} \\,\\ f(\\mathbf{x}) \\\\ \n",
    "\\quad s.t. \\quad \\\\\n",
    "g_i(\\mathbf{x}) \\le 0, \\,\\ i=1,\\cdots,m, \\\\\n",
    "h_j(\\mathbf{x}) = 0, \\,\\ j=1,\\cdots,k \\\\\n",
    "\\end{matrix}\n",
    "$$\n",
    "where $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is a objective function, $\\mathbf{g} : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ is a inequality constraint function, and $\\mathbf{h}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^k $ is a equality constraint function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af0833b-9f2f-4962-aae3-41566ed11f9e",
   "metadata": {},
   "source": [
    "#### 5.2. Lagrange Multiplier\n",
    "\n",
    "##### Definition.5.2. Lagrangian Function\n",
    "In above constrained optimization problem, there is corresponding Lagrangian function\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\mathcal{L}(\\mathbf{x}, \\mathbf{\\lambda}, \\mathbf{\\mu}) &= f(\\mathbf{x}) + \\sum_{i = 1}^{m} \\lambda_ig_i(\\mathbf{x}) + \\sum_{j = 1}^{k} \\mu_j h_j(\\mathbf{x}) \\\\\n",
    "                                                        &= f(\\mathbf{x}) + \\mathbf{\\lambda}^T \\mathbf{g}(\\mathbf{x}) + \\mathbf{\\mu}^T \\mathbf{h}(\\mathbf{x}) \\\\\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb92325-ebea-4740-b154-0016c6ae9ab0",
   "metadata": {},
   "source": [
    "where $\\lambda_i, \\mu_j$ for $i=1,\\cdots,m, \\,\\ j = 1,\\cdots,k$ are dual variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beadcfdc-9f14-48a3-883e-0607a5aa38d7",
   "metadata": {},
   "source": [
    "##### Theorem.5.1. Lagrangian Multiplier\n",
    "In equality constrained optimization problem, if $ \\mathbf{x}^* \\in \\mathbb{R}^n $ is a local minimum, then there exist $ \\mathbf{\\mu}^* \\in \\mathbb{R}^k $ such that\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\frac{\\partial}{\\partial \\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\mathbf{\\mu}) = \\mathbf{0} \\\\\n",
    "\\frac{\\partial}{\\partial \\mathbf{\\mu}} \\mathcal{L}(\\mathbf{x}, \\mathbf{\\mu}) = \\mathbf{0} \\\\\n",
    "\\end{cases}.\n",
    "$$\n",
    "And system of equations contain $n + k$ equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0fe019-64ea-43fa-a713-98d501704de1",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cb41f7-5253-4db7-8285-c931a8bf6d00",
   "metadata": {},
   "source": [
    "Consider following problem from algebra.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca203533-15b7-4679-8055-8c8132c40c43",
   "metadata": {},
   "source": [
    "__Ex)__<br>\n",
    "\n",
    ">2차원 평면상의 원 $x^2 + y^2 = k$과 직선 $y = \\sqrt{3}x + 4\\sqrt{3}$을 지날때, k의 최소를 구하시오.\n",
    "\n",
    "__sol)__<br>\n",
    "\n",
    "$ \\text{Let} \\,\\ \\mathbf{x} = \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\in \\mathbb{R}^2. $<br>\n",
    "$ \\text{Then, this problem can be convert following constrained optimization problem.} $<br>\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^* = \\underset{\\mathbf{x}}{\\mathrm{argmin}} \\,\\ \\mathbf{x}^T \\mathbf{x} \\quad s.t. \\quad g(\\mathbf{x}) = [\\sqrt{3} \\,\\ - 1]\\mathbf{x} + 4\\sqrt{3} = 0.\n",
    "$$\n",
    "\n",
    "$\\text{Then the Lagrangian function of above constrained optimization problem is} $<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fd3502-ba4e-4d04-9306-f75ad2d39ec7",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L}(\\mathbf{x}, \\mu) = \\mathbf{x}^T \\mathbf{x} + \\mu ([\\sqrt{3} \\,\\ - 1] \\mathbf{x} + 4 \\sqrt{3}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0087e229-40ca-486c-9670-8c79c679bcc0",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial}{\\partial \\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\mu) = 2 \\mathbf{x} + \\mu [\\sqrt{3} \\,\\  -1]^T = \\mathbf{0} \\qquad \\cdots \\,\\ (1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mu} \\mathcal{L}(\\mathbf{x}, \\mu) = [\\sqrt{3} \\,\\ -1] \\mathbf{x} + 4 \\sqrt{3} = 0 \\qquad \\cdots \\,\\ (2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2f2161-730f-4cea-b747-5b32d5bc8227",
   "metadata": {},
   "source": [
    "$\\text{By theorem.5.1., solution of above equation is a local minimum of optimization problem.}$<br>\n",
    "$\\text{Also, above Lagrangian function is convex.} \\quad (\\because \\,\\ \\text{Property.3.3})$ <br>\n",
    "$\\text{By, theorem.3.1.,}$<br>\n",
    "\n",
    "$$\n",
    "\\therefore \\quad \\mathbf{x}^* = \\begin{bmatrix} - \\frac{\\sqrt{3}\\mu}{2} \\\\ \\frac{\\mu}{2} \\end{bmatrix}, \\,\\ \\min k = 12\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9397735-0893-4dd7-8e05-ea86b481b1c2",
   "metadata": {},
   "source": [
    "Of course, it is reasonable to use gradient based optimization when we optimize the Lagrangian function. Above solution is obtained just analytically, not numerical method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90b438a-689d-45e2-9fa1-41cf1ee478cb",
   "metadata": {},
   "source": [
    "#### 5.3. Karush-Kuhn-Tucker(KKT) Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94ddaef-a04a-4664-9685-e5cfa9f5b009",
   "metadata": {},
   "source": [
    "Consider following problem.\n",
    "\n",
    "$$\n",
    "\\min x_1^2 + x_2^2 \\quad s.t. \\quad x_1 + x_2 = 0\n",
    "$$\n",
    "\n",
    "In above problem, the value of $\\mu$ is zero. This means that the equality constraint doesn't dependent on optimization problem. In this situation, above constrained optimization problem has equivalence relation with unconstrained optimization; $\\min x_1^2 + x_2^2 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac1b48b-4202-4741-a6cc-14494ee21a1e",
   "metadata": {},
   "source": [
    "##### Theorem.5.2. Karush-Kuhn-Tucker(KKT) Conditions\n",
    "If $ \\mathbf{x}^* \\in \\mathbb{R}^n $ is a local minimum, then there exist $\\mathbf{\\lambda}^* \\in \\mathbb{R}^m $ and $ \\mathbf{\\mu}^* \\in \\mathbb{R}^k $ such that\n",
    "\n",
    "$ (1) \\,\\ \\text{Stationarity} \\quad \\frac{\\partial}{\\partial \\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\mathbf{\\lambda}, \\mathbf{\\mu}) = \\mathbf{0} $<br>\n",
    "\n",
    "$ (2) \\,\\ \\text{Primal feasibility} \\quad ^\\forall i, \\,\\ g_i(\\mathbf{x}^*) \\le 0, \\,\\ \\mathbf{h}(\\mathbf{x}^*) = \\mathbf{0} $<br>\n",
    "\n",
    "$ (3) \\,\\ \\text{Complementary slackness} \\quad ^\\forall i, \\,\\ \\lambda_i^* g_i(\\mathbf{x}^*) = 0 $<br>\n",
    "\n",
    "$ (4) \\,\\ \\text{Dual feasibility} \\quad ^\\forall i, \\,\\ \\lambda_i^* \\ge 0 $<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9014d7d5-6d55-43ca-b8fb-3f2a031db76a",
   "metadata": {},
   "source": [
    "Remark that if optimal solution isn't dependent with inequality constraint, then it is same with unconstrained optimization problem, and if optimal solution is dependent with inequality constraint, then optimal solution is on the boundary line of inequatlity constraint. Consider following problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58abe1c-6bb0-4ef9-8953-b81005b714f4",
   "metadata": {},
   "source": [
    "$$\n",
    "\\min x_1^2 + x_2^2 \\quad s.t. \\quad x_1 + x_2 - 1 \\le 0 \\qquad \\cdots \\,\\ (1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\min x_1^2 + x_2^2 \\quad s.t. \\quad x_1 + x_2 - 1 \\ge 0 \\qquad \\cdots \\,\\ (2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2837cf3e-01bc-4a8b-b40e-79adb2159ca9",
   "metadata": {},
   "source": [
    "All constraint conditions are divided into those that affect the result and those that do not. In $(1)$, that is same with \n",
    "\n",
    "$$\n",
    "\\min x_1^2 + x_2^2. \n",
    "$$ \n",
    "\n",
    "Furthermore, since optimal solution can be in boundary line of inequality constraint, the inequality constraint that affect the result can be converted equality constraint. Therefore, $(2)$ is same with\n",
    "\n",
    "$$\n",
    "\\min x_1^2 + x_2^2 \\quad s.t. \\quad x_1 + x_2 - 1 = 0\n",
    "$$\n",
    "\n",
    "Remark followings. _Stationarity_ condition is because of _Lagrange multiplier_. _Primal feasibility_ condition is trivial. _Complementary slackness_ condition is derived from Lagrangian multiplier. We can understand that \n",
    "\n",
    "$$\n",
    "\\frac{\\partial }{\\partial \\lambda_i} \\mathcal{L}(\\mathbf{x}, \\mathbf{\\lambda}, \\mathbf{\\mu}) = g_i(\\mathbf{x})\n",
    "$$\n",
    ", and since it can be constraint that doesn't affect the result, $\\lambda_i$ can be zero or not. Therefore, \n",
    "\n",
    "$$\n",
    "^\\forall i, \\,\\ \\lambda_i^* g_i(\\mathbf{x}^*) = 0.\n",
    "$$\n",
    "\n",
    "_Dual feasibility_ condition is a condition that guarantees that the KKT condition is the same problem as the inequality constrained optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f09073-65a1-4e16-a26e-d92f9bd0c60d",
   "metadata": {},
   "source": [
    "#### 5.4. Duality\n",
    "Consider following optimization problem\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\mathbf{x}^* = \\underset{\\mathbf{x}}{\\mathrm{argmin}} \\,\\ f(\\mathbf{x}) \\\\ \n",
    "\\quad s.t. \\quad \\\\\n",
    "g_i(\\mathbf{x}) \\le 0, \\,\\ i=1,\\cdots,m, \\\\\n",
    "h_j(\\mathbf{x}) = 0, \\,\\ j=1,\\cdots,k \\\\\n",
    "\\end{matrix}\n",
    "$$\n",
    "where $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is a convex function, $\\mathbf{g} : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ is a convex function for each $g_i$, and $\\mathbf{h}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^k $ is an affine function. Above optimization problem is called convex. For convex problems. KKT conditions becomes necessary and also sufficient for global optimality. <br><br>\n",
    "\n",
    "From now on, we consider above problem with duality. Duality means that the primal problem of optimization problem can view the dual problem. In Lagrangian method, it is called __Lagrangian dual problem__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4518a19c-a6e1-4c99-b679-c5f135a4c85a",
   "metadata": {},
   "source": [
    "##### Definition.5.2. Lagrangian Dual Function\n",
    "\n",
    "$$\n",
    "\\mathcal{D}(\\mathbf{\\lambda}, \\mathbf{\\mu}) = \\underset{\\mathbf{x}}{\\min} \\mathcal{L}(\\mathbf{x}, \\mathbf{\\lambda}, \\mathbf{\\mu}) = \\underset{\\mathbf{x}}{\\min} \\left\\{ f(\\mathbf{x} + \\mathbf{\\lambda}^T \\mathbf{h}(\\mathbf{x}) + \\mathbf{\\mu}^T \\mathbf{g}(\\mathbf{x}) \\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35140787-0e1a-4d36-82b7-471aef3d9f13",
   "metadata": {},
   "source": [
    "##### Theorem.5.3. Lagrange Dual Problem\n",
    "The problem\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\mathbf{x}^* = \\underset{\\mathbf{x}}{\\mathrm{argmin}} \\,\\ f(\\mathbf{x}) \\\\ \n",
    "\\quad s.t. \\quad \\\\\n",
    "g_i(\\mathbf{x}) \\le 0, \\,\\ i=1,\\cdots,m, \\\\\n",
    "h_j(\\mathbf{x}) = 0, \\,\\ j=1,\\cdots,k \\\\\n",
    "\\end{matrix}\n",
    "$$\n",
    "where $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is a convex function, $\\mathbf{g} : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ is a convex function for each $g_i$, and $\\mathbf{h}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^k $ is an affine function, is equivalent with \n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{\\lambda}, \\mathbf{\\mu}}{\\max} \\mathcal{D}(\\mathbf{\\lambda}, \\mathbf{\\mu}) \\quad \\text{for} \\,\\ \\lambda_i \\ge 0, \\,\\ i=1,\\cdots,m.\n",
    "$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe8081a-19b2-472c-9789-938fc313951c",
   "metadata": {},
   "source": [
    "In above problem, $ \\mathcal{D}(\\mathbf{\\lambda}, \\mathbf{\\mu}) \\le \\mathcal{L}(\\mathbf{x}^*, \\mathbf{\\lambda}, \\mathbf{\\mu}) \\le f(\\mathbf{x}^*) $ where $\\mathbf{x}^*$ is primal optimal.<br>\n",
    "Therefore, $ \\mathcal{D}(\\mathbf{\\lambda}, \\mathbf{\\mu}) \\le \\underset{\\lambda_i \\ge 0, \\mathbf{\\mu}, \\,\\ \\text{for} \\,\\ i=1,\\cdots,m}{\\max} \\mathcal{D}(\\mathbf{\\lambda}, \\mathbf{\\mu}) \\le f(\\mathbf{x}^*)$.<br>\n",
    "$f(\\mathbf{x}^*)$ is called primal optimal, and $\\underset{\\lambda_i \\ge 0, \\mathbf{\\mu}, \\,\\ \\text{for} \\,\\ i=1,\\cdots,m}{\\max} \\mathcal{D}(\\mathbf{\\lambda}, \\mathbf{\\mu})$ is called dual optimal.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53046d8-ddd9-46f3-8a49-839d26d59cbe",
   "metadata": {},
   "source": [
    "##### Definition.5.3. Duality\n",
    "Let $p^*$ be primal optimal and $d^*$ be dual optimal of a dual problem.<br>\n",
    "If $p^* \\ge d^*$, it is called weak duality, and if $p^* = d^*$, it is called strong duality.<br>\n",
    "And $p^* - d^*$ is called duality gap.\n",
    "<br><br>\n",
    "\n",
    "__Remark)__<br>\n",
    "- Dual function is a concave function(proved by definition of concave).\n",
    "- For a convex optimization problem, the strong duality usually holds (not always, i.e., When Slater’s condition is not satisfied.)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af80edf-f350-4add-8016-8aa6b5f9de07",
   "metadata": {},
   "source": [
    "__Ex)__<br>\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^* = \\underset{\\mathbf{x}}{\\mathrm{argmin}} \\mathbf{x}^T \\mathbf{x} \\quad s.t. \\quad A\\mathbf{x} = \\mathbf{b}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
