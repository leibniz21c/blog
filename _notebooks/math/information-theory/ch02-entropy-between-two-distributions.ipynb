{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3e6342",
   "metadata": {},
   "source": [
    "# \"[InformationTheory] CH02. Entropy Between Two Distributions\"\n",
    "> Information theory summary note.\n",
    "\n",
    "- toc: false\n",
    "- badges: false\n",
    "- comments: false\n",
    "- categories: [information-theory]\n",
    "- hide_{github,colab,binder,deepnote}_badge: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1a44f0-4db4-4574-ae40-b586ca3fa045",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1. Cross Entropy\n",
    "##### Definition.2.1. Cross Entropy\n",
    "For probability mess functions $p$ and $q$, the cross entropy $H[p, q]$ is\n",
    "$$\n",
    "H[p, q] = \\mathbb{E}_{x \\sim p}[\\log q(x)] = - \\sum_{i = 1}^N p(x_i) \\log_2 q(x_i) dx.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Also, for probability density functions $p$ and $q$, the cross entropy $H[p, q]$ is\n",
    "$$\n",
    "H[p, q] = \\mathbb{E}_{x \\sim p}[\\log q(x)] = - \\int_x p(x) \\log_2 q(x) dx.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fce85b-5b8b-4ceb-abce-8d52f2f028d6",
   "metadata": {},
   "source": [
    "__Ex)__ <br>\n",
    "$$\n",
    "\\begin{matrix}\n",
    "p = [\\frac{1}{3} \\,\\ \\frac{1}{3} \\,\\ \\frac{1}{3}] \\\\\n",
    "q_1 = [\\frac{1}{3} \\,\\ \\frac{1}{3} \\,\\ \\frac{1}{3}] \\\\\n",
    "q_2 = [\\frac{1}{6} \\,\\ \\frac{2}{3} \\,\\ \\frac{1}{6}] \\\\\n",
    "q_3 = [\\frac{1}{12} \\,\\ \\frac{5}{6} \\,\\ \\frac{1}{12}] \\\\\n",
    "r_1 = [\\frac{1}{6} \\,\\ \\frac{1}{6} \\,\\ \\frac{2}{3}] \\\\\n",
    "r_2 = [\\frac{1}{12} \\,\\ \\frac{1}{12} \\,\\ \\frac{5}{6}] \\\\\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80236b20-66f4-4261-b622-f7fabfafec05",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{matrix}\n",
    "H[p, q_1] = 1.584962500721156 \\\\\n",
    "H[p, q_2] = 1.9182958340544896 \\\\\n",
    "H[p, q_3] = 2.477653135758702 \\\\\n",
    "H[p, r_1] = 1.9182958340544896 \\\\\n",
    "H[p, r_2] = 2.477653135758702 \\\\\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81aa232-9105-4332-98b3-1aa4dece3c35",
   "metadata": {},
   "source": [
    "##### Theorem.2.1. \n",
    "If $p = q$, \n",
    "\n",
    "$$\n",
    "H[p, q] = H[p]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce4b952-406b-45a4-ac4f-bb38ee3fc1f8",
   "metadata": {},
   "source": [
    "#### 2.2. Kullback-Leibler Divergence\n",
    "\n",
    "##### Definition.2.2. Kullback-Leibler Divergence\n",
    "For probability mess functions $p$ and $q$, the KL divergence $D_{KL}(p||q)$ is\n",
    "$$\n",
    "D_{KL}(p||q) = \\mathbb{E}_{x \\sim p}\\left[\\frac{\\log p(x)}{\\log q(x)}\\right] = H[p, q] - H[p] = \\sum_{i = 1}^N p(x_i) \\log_2 \\frac{p(x_i)}{q(x_i)}.\n",
    "$$\n",
    "\n",
    "For probability density functions $p$ and $q$, the KL divergence $D_{KL}(p||q)$ is\n",
    "$$\n",
    "D_{KL}(p||q) = \\mathbb{E}_{x \\sim p}\\left[\\frac{\\log p(x)}{\\log q(x)}\\right] = H[p, q] - H[p] = \\int_x p(x) \\log_2 \\frac{p(x)}{q(x)}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43d685f-d405-4bc8-b54d-012072e2869d",
   "metadata": {},
   "source": [
    "__Ex)__ <br>\n",
    "$$\n",
    "\\begin{matrix}\n",
    "p = [\\frac{1}{3} \\,\\ \\frac{1}{3} \\,\\ \\frac{1}{3}] \\\\\n",
    "q_1 = [\\frac{1}{3} \\,\\ \\frac{1}{3} \\,\\ \\frac{1}{3}] \\\\\n",
    "q_2 = [\\frac{1}{6} \\,\\ \\frac{2}{3} \\,\\ \\frac{1}{6}] \\\\\n",
    "q_3 = [\\frac{1}{12} \\,\\ \\frac{5}{6} \\,\\ \\frac{1}{12}] \\\\\n",
    "r_1 = [\\frac{1}{6} \\,\\ \\frac{1}{6} \\,\\ \\frac{2}{3}] \\\\\n",
    "r_2 = [\\frac{1}{12} \\,\\ \\frac{1}{12} \\,\\ \\frac{5}{6}] \\\\\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27be682a-5179-4846-b1c9-a811333e061f",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{matrix}\n",
    "D_{KL}(p||q_1) = 0.0 \\\\\n",
    "D_{KL}(p||q_2) = 0.3333333333333333 \\\\\n",
    "D_{KL}(p||q_3) = 0.8926906350375459 \\\\\n",
    "D_{KL}(p||r_1) = 0.3333333333333333 \\\\\n",
    "D_{KL}(p||r_2) = 0.8926906350375459 \\\\\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162544ac-28ac-43e4-ab5b-cdedda30ebcf",
   "metadata": {},
   "source": [
    "##### Theorem.2.2. \n",
    "If $p = q$, \n",
    "\n",
    "$$\n",
    "D_{KL}(p||q) = H[p, q] - H[p] = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ef0fd5-fb8f-420b-bd77-c73e22baeecc",
   "metadata": {},
   "source": [
    "Since both operation doesn't allow commutative property, remark that cross entropy and kl divergence can't be used to distance metric."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
