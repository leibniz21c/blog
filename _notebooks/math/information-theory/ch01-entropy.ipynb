{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3e6342",
   "metadata": {},
   "source": [
    "# \"[InformationTheory] CH01. Entropy\"\n",
    "> Information theory summary note.\n",
    "\n",
    "- toc: false\n",
    "- badges: false\n",
    "- comments: false\n",
    "- categories: [information-theory]\n",
    "- hide_{github,colab,binder,deepnote}_badge: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1a44f0-4db4-4574-ae40-b586ca3fa045",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.0. Overview\n",
    "Information theory was developed to study the problem of sending messages by sending discrete alphabets over noised channel like wireless communication. In machine learning, its context lies in the application of information theory to continuous variables. _TM Cover, 2006__, __MacKay, 2003__<br><br>\n",
    "\n",
    "The following are the core intuitions of information theory. \n",
    "\n",
    "> Learning an event that is less likely to occur is more informative than learning an event that is more likely to occur.\n",
    "\n",
    "For example, \"there was a solar eclipse this morning\" than \"the sun rose this morning\" means that you have more information. That is, to quantify the amount of information, the following properties must be satisfied.\n",
    "\n",
    "- Events with a high probability of occurrence should have less information.\n",
    "- There is no information about an event that must occur.\n",
    "- Events with a low probability of occurrence should have more information.\n",
    "- The amount of information for individual events should be additive.\n",
    "    - Ex) In coin toss, $I([H, H]) = 2I([H])$\n",
    "  \n",
    "##### Definition.1.1. Self-information of Event x=$x$\n",
    "$$\n",
    "I_p(x) = -\\log P(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c111f3e2-f4ca-4708-87e5-760b9381ccc9",
   "metadata": {},
   "source": [
    "Since $\\log$ is a natural logarithm, the unit of $I(x)$ above is nat. nat is the amount of information obtained by observing an event with probability $\\frac{1}{e}$. If $\\log_2$ is used instead of the natural logarithm, the unit is called bit or shannon, which means the amount of information obtained by observing an event with probability $\\frac{1}{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f6a05e-e70c-4dde-aa9a-a95c9249a6df",
   "metadata": {},
   "source": [
    "Consider the binomial distribution of the probability that a coin is tossed 3 times and heads are x. The expression is:\n",
    "\n",
    "$$\n",
    "p(x) = {3 \\choose x}p^x (1 - p)^{3 - x} \\quad \\text{for} \\,\\ x=0,1,2,3.\n",
    "$$\n",
    "\n",
    "In the above equation, for each event $x=0, x=1, x=2, x=3$, the probability value is as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89a7d97-4469-49cd-b2ee-d07812e1078c",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{matrix}\n",
    "p(x=0) = 0.125 \\\\\n",
    "p(x=1) = 0.375 \\\\\n",
    "p(x=2) = 0.375 \\\\\n",
    "p(x=3) = 0.125 \\\\\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52b5348-800c-43a5-a432-9688b86f2900",
   "metadata": {},
   "source": [
    "The self-information for it is as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237d09ae-1e82-4510-9137-58ad1c772a21",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{matrix}\n",
    "I_p(x=0) = 3 \\\\\n",
    "I_p(x=1) = 1.415037\\cdots \\\\\n",
    "I_p(x=2) = 1.415037\\cdots \\\\\n",
    "I_p(x=3) = 3 \\\\\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2fd9fc-c015-4b03-9228-48a8b86668c3",
   "metadata": {},
   "source": [
    "The information amount value is a structure that receives the probability value as $\\log$, so it is not negative and has a relatively large value at a low probability value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b5f46-2764-4e2c-a912-894baad8fca4",
   "metadata": {},
   "source": [
    "#### 1.1. Shannon Entropy\n",
    "The above case of self-information deals with only one event. The uncertainty of the entire probability distribution can be quantified with the Shannon entropy.\n",
    "\n",
    "##### Definition.1.2. Shannon Entropy\n",
    "$$\n",
    "H[p] = \\mathbb{E}_{x \\sim  p}[I(x)] = - \\mathbb{E}_{x \\sim  p}[\\log p(x)].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12123916-4e0f-458e-8802-1d0f9fcdf1a0",
   "metadata": {},
   "source": [
    "This is the average amount of information for the events in the distribution. This value tells the lower bound of the average number of bits required to encode information drawn from the distribution $p$. In the example above, the Shannon entropy value is calculated as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f026b746-e282-443a-8113-3ef0a580fde3",
   "metadata": {},
   "source": [
    "$$\n",
    "H[p] = \\frac{1}{4} (0.125 \\cdot 3 + 0.375 \\cdot 1.415037 + 0.375 \\cdot 1.415037 + 0.125 \\cdot 3) = 0.4528194.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da88ec0-ee40-466e-83d2-086bbbc83b85",
   "metadata": {},
   "source": [
    "The value of Shannon's entropy is low in the deterministic case, and the closer it is to an even distribution(i.e., like uniform distribution), the higher the entropy. In particular, when $x$ is a continuous variable, the Shannon entropy is called differential entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96aa883-9917-48a6-af13-9cb89c252125",
   "metadata": {},
   "source": [
    "---\n",
    "##### Application.1.1. Maximization of Shannon Entropy\n",
    "Let probability density function $p$ be for $x\\in [a, b]$.<br>\n",
    "Then,\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\int_{a}^{b} p(x)dx = 1 \\\\\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa6230d-dc84-459e-9d80-6256e6ad24b5",
   "metadata": {},
   "source": [
    "Consider following problem\n",
    "\n",
    "$$\n",
    "\\max_{p(x)} H[p(x)] = \\max_{p(x)} - \\mathbb{E}[\\log p(x)] = \\max_{p(x)} - \\int_a^b p(x) \\log p(x) dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b76a9b-449f-4fc5-9871-2657d4bc5a1c",
   "metadata": {},
   "source": [
    "Above problem is equality constraind optimization problem. Therefore we have to get lagrangian function $\\mathcal{L}$.\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\int_a^b p(x) \\log p(x) dx + \\lambda_1 \\left( \\int_a^b p(x) dx - 1 \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479a8bbf-7206-4148-b57c-c006606696c5",
   "metadata": {},
   "source": [
    "Above $\\mathcal{L}$ is functional, then\n",
    "\n",
    "$$\n",
    "\\frac{\\delta \\mathcal{L}}{\\delta p(x)} = - \\log p(x) - 1 + \\lambda_1 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\therefore \\,\\ p(x) = \\exp(-1 + \\lambda_1) = c \\,\\ \\text{for some constant} \\,\\ c.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44ab15b-c369-4b70-8845-f8b166bdfa58",
   "metadata": {},
   "source": [
    "By equality constraint, \n",
    "$$\n",
    "\\int_a^b c dx = c(b - a) = 1.\n",
    "$$\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\therefore \\,\\ c = \\frac{1}{b - a} = p(x) \\sim U(a, b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fcb44e-c063-4bb5-a3f7-71f6936d6e81",
   "metadata": {},
   "source": [
    "In above result, we can know that the uniform distribution is maximized shannon entropy distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34242c64-6422-49bd-9d6a-396a9156fba9",
   "metadata": {},
   "source": [
    "---\n",
    "##### Application.1.2. Maximization of Shannon Entropy with Fixed Variation\n",
    "Let the expectation of probability density function $p$ be $\\mu$ and variation be $\\sigma^2$.<br>\n",
    "Then,\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\int_{-\\infty}^{\\infty} p(x)dx = 1 \\\\\n",
    "\\int_{-\\infty}^{\\infty} xp(x)dx = \\mu \\\\\n",
    "\\int_{-\\infty}^{\\infty} (x - \\mu)^2p(x)dx = \\sigma^2 \\\\\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6792440-78df-475f-b21c-3000d9d97771",
   "metadata": {},
   "source": [
    "For maximization of $H[p]$, we have to use lagrange multiplier.<br>\n",
    "Then,\n",
    "\n",
    "$$\n",
    "\\max \\mathcal{L}[p](\\mathbf{\\lambda}) = \\max - \\int_{-\\infty}^{\\infty} p(x) \\log p(x) dx + \\mathbf{\\lambda}^T \n",
    "\\begin{bmatrix}\n",
    "\\int_{-\\infty}^{\\infty} p(x)dx - 1 \\\\\n",
    "\\int_{-\\infty}^{\\infty} xp(x)dx - \\mu \\\\\n",
    "\\int_{-\\infty}^{\\infty} (x-\\mu)^2 p(x)dx - \\sigma^2 \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9685bef8-29b9-4093-8f40-860fdf4e82b8",
   "metadata": {},
   "source": [
    "Then,\n",
    "\n",
    "$$\n",
    "\\frac{\\delta \\mathcal{L}}{\\delta p(x)} = - \\log p(x) - 1 + \\lambda_1  + \\lambda_2x + \\lambda_3(x-\\mu)^2  = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\therefore \\,\\ p(x) = \\exp(-1 + \\lambda_1 + \\lambda_2x + \\lambda_3(x-\\mu)^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc0e927-c73e-429e-9aa7-1544feee5317",
   "metadata": {},
   "source": [
    "By first equality constraint, \n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\int_{-\\infty}^{\\infty} p(x) dx = \\int_{-\\infty}^{\\infty} \\exp(-1 + \\lambda_1 + \\lambda_2x + \\lambda_3(x - \\mu)^2) dx \\\\\n",
    "= \\int_{-\\infty}^{\\infty} \\exp( \\lambda_3x^2 + (\\lambda_2 - 2\\mu)x + \\mu^2 \\lambda_3 + \\lambda_1 - 1) dx \\quad (\\lambda_3 < 0)\\\\ \n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0872be-107f-4a6a-8c97-925a73987f8e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{matrix}\n",
    "- |\\lambda_3|(x^2 + \\frac{\\lambda_2 - 2\\mu}{|\\lambda_3|}x + \\frac{(\\lambda_2 - 2\\mu)^2}{4\\lambda_3^2} ) - \\frac{(\\lambda_2 - 2\\mu)^2}{4|\\lambda_3|} + \\mu^2 \\lambda_3 + \\lambda_1 - 1 \\\\\n",
    "= - |\\lambda_3|( x + \\frac{\\lambda_2 - 2\\mu}{2\\lambda_3} )^2 - \\frac{(\\lambda_2 - 2\\mu)^2}{4|\\lambda_3|} + \\mu^2 \\lambda_3 + \\lambda_1 - 1 \\\\\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeff4f0-19ff-4de7-b36c-cbe6c7d6ad62",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{matrix}\n",
    "\\int_{-\\infty}^{\\infty} \\exp( \\lambda_3x^2 + (\\lambda_2 - 2\\mu)x + \\mu^2 \\lambda_3 + \\lambda_1 - 1) dx \\\\\n",
    "= \\int_{-\\infty}^{\\infty} \\exp(- |\\lambda_3|( x + \\frac{\\lambda_2 - 2\\mu}{2\\lambda_3} )^2 - \\frac{(\\lambda_2 - 2\\mu)^2}{4|\\lambda_3|} + \\mu^2 \\lambda_3 + \\lambda_1 - 1) dx \\\\\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0653ca31-a37e-4da6-b0cc-fc5953a0fd7b",
   "metadata": {},
   "source": [
    "Let $\\sqrt{|\\lambda_3|}(x + \\frac{\\lambda_2}{2\\lambda_3}) = t$.<br>\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\int_{-\\infty}^{\\infty} \\exp(- |\\lambda_3|( x + \\frac{\\lambda_2 - 2\\mu}{2\\lambda_3} )^2 - \\frac{(\\lambda_2 - 2\\mu)^2}{4|\\lambda_3|} + \\mu^2 \\lambda_3 + \\lambda_1 - 1) dx \\\\\n",
    "= \\frac{1}{\\sqrt{|\\lambda_3|}} \\exp\\left\\{ - \\frac{(\\lambda_2 - 2\\mu)^2}{4|\\lambda_3|} + \\mu^2\\lambda_3 + \\lambda_1 - 1 \\right\\} \\int_{-\\infty}^{\\infty} \\exp(-t^2) dt\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a7528a-1f58-4877-9f62-3571595d7f39",
   "metadata": {},
   "source": [
    "$$\n",
    "\\therefore \\,\\ \\frac{\\sqrt{\\pi}}{\\sqrt{|\\lambda_3|}} \\exp\\left\\{ - \\frac{(\\lambda_2 - 2\\mu)^2}{4|\\lambda_3|} + \\mu^2\\lambda_3 + \\lambda_1 - 1 \\right\\} = 1 \\qquad \\cdots \\,\\ (1) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9af484-e6a0-4447-81d6-9769db4218f6",
   "metadata": {},
   "source": [
    "In similar way by second equality constraint, \n",
    "\n",
    "$$\n",
    "- \\frac{(\\lambda_2 - 2\\mu)\\sqrt{\\pi}}{2\\lambda_3 \\sqrt{|\\lambda_3|}} \\exp\\left\\{ - \\frac{(\\lambda_2 - 2\\mu)^2}{4|\\lambda_3|} + \\mu^2\\lambda_3 + \\lambda_1 - 1 \\right\\} = \\mu \\qquad \\cdots \\,\\ (2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa430f12-2870-4d9e-8355-a9d335c3de5f",
   "metadata": {},
   "source": [
    "In similar way by third equality constraint, \n",
    "\n",
    "$$\n",
    "\\frac{\\sqrt{\\pi}}{\\sqrt{|\\lambda_3|}} \\left( \\frac{1}{2|\\lambda_3|} + \\frac{(\\lambda_2 - 2\\mu + 2\\lambda_3 \\mu)^2}{4\\lambda_3^2} \\right) = \\sigma^2 \\qquad \\cdots \\,\\ (3)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a370c94-1382-410e-8d59-6661f566a640",
   "metadata": {},
   "source": [
    "By $(1), \\,\\ (2), \\,\\ \\text{and} \\,\\ (3)$,\n",
    "\n",
    "$$\n",
    "\\therefore \\,\\ p(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left\\{ - \\frac{(x - \\mu)^2}{2\\sigma^2} \\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630e605c-a525-4316-80ec-acaaf0caf9de",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
