<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>[InformationTheory] CH01. Entropy | YANG’s note</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="[InformationTheory] CH01. Entropy" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Information theory summary note." />
<meta property="og:description" content="Information theory summary note." />
<link rel="canonical" href="https://ndo04343.github.io/blog/information-theory/2022/02/24/ch01-entropy.html" />
<meta property="og:url" content="https://ndo04343.github.io/blog/information-theory/2022/02/24/ch01-entropy.html" />
<meta property="og:site_name" content="YANG’s note" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-02-24T00:00:00-06:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="[InformationTheory] CH01. Entropy" />
<script type="application/ld+json">
{"url":"https://ndo04343.github.io/blog/information-theory/2022/02/24/ch01-entropy.html","@type":"BlogPosting","headline":"[InformationTheory] CH01. Entropy","dateModified":"2022-02-24T00:00:00-06:00","datePublished":"2022-02-24T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://ndo04343.github.io/blog/information-theory/2022/02/24/ch01-entropy.html"},"description":"Information theory summary note.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://ndo04343.github.io/blog/feed.xml" title="YANG's note" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">YANG&#39;s note</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/reports/">Reports</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/self-reflection/">Self-Reflection</a><a class="page-link" href="/blog/categories/">Categories</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">[InformationTheory] CH01. Entropy</h1><p class="page-description">Information theory summary note.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-02-24T00:00:00-06:00" itemprop="datePublished">
        Feb 24, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      5 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#information-theory">information-theory</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/ch01-entropy.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="1.0.-Overview">1.0. Overview<a class="anchor-link" href="#1.0.-Overview"> </a></h4><p>Information theory was developed to study the problem of sending messages by sending discrete alphabets over noised channel like wireless communication. In machine learning, its context lies in the application of information theory to continuous variables. _TM Cover, 2006<strong>, </strong>MacKay, 2003__<br /><br /></p>
<p>The following are the core intuitions of information theory.</p>
<blockquote><p>Learning an event that is less likely to occur is more informative than learning an event that is more likely to occur.</p>
</blockquote>
<p>For example, "there was a solar eclipse this morning" than "the sun rose this morning" means that you have more information. That is, to quantify the amount of information, the following properties must be satisfied.</p>
<ul>
<li>Events with a high probability of occurrence should have less information.</li>
<li>There is no information about an event that must occur.</li>
<li>Events with a low probability of occurrence should have more information.</li>
<li>The amount of information for individual events should be additive.<ul>
<li>Ex) In coin toss, $I([H, H]) = 2I([H])$</li>
</ul>
</li>
</ul>
<h5 id="Definition.1.1.-Self-information-of-Event-x=$x$">Definition.1.1. Self-information of Event x=$x$<a class="anchor-link" href="#Definition.1.1.-Self-information-of-Event-x=$x$"> </a></h5>$$
I_p(x) = -\log P(x).
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since $\log$ is a natural logarithm, the unit of $I(x)$ above is nat. nat is the amount of information obtained by observing an event with probability $\frac{1}{e}$. If $\log_2$ is used instead of the natural logarithm, the unit is called bit or shannon, which means the amount of information obtained by observing an event with probability $\frac{1}{2}$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Consider the binomial distribution of the probability that a coin is tossed 3 times and heads are x. The expression is:</p>
$$
p(x) = {3 \choose x}p^x (1 - p)^{3 - x} \quad \text{for} \,\ x=0,1,2,3.
$$<p>In the above equation, for each event $x=0, x=1, x=2, x=3$, the probability value is as follows.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\begin{matrix}
p(x=0) = 0.125 \\
p(x=1) = 0.375 \\
p(x=2) = 0.375 \\
p(x=3) = 0.125 \\
\end{matrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The self-information for it is as follows.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\begin{matrix}
I_p(x=0) = 3 \\
I_p(x=1) = 1.415037\cdots \\
I_p(x=2) = 1.415037\cdots \\
I_p(x=3) = 3 \\
\end{matrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The information amount value is a structure that receives the probability value as $\log$, so it is not negative and has a relatively large value at a low probability value.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="1.1.-Shannon-Entropy">1.1. Shannon Entropy<a class="anchor-link" href="#1.1.-Shannon-Entropy"> </a></h4><p>The above case of self-information deals with only one event. The uncertainty of the entire probability distribution can be quantified with the Shannon entropy.</p>
<h5 id="Definition.1.2.-Shannon-Entropy">Definition.1.2. Shannon Entropy<a class="anchor-link" href="#Definition.1.2.-Shannon-Entropy"> </a></h5>$$
H[p] = \mathbb{E}_{x \sim  p}[I(x)] = - \mathbb{E}_{x \sim  p}[\log p(x)].
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is the average amount of information for the events in the distribution. This value tells the lower bound of the average number of bits required to encode information drawn from the distribution $p$. In the example above, the Shannon entropy value is calculated as follows.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
H[p] = \frac{1}{4} (0.125 \cdot 3 + 0.375 \cdot 1.415037 + 0.375 \cdot 1.415037 + 0.125 \cdot 3) = 0.4528194.
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The value of Shannon's entropy is low in the deterministic case, and the closer it is to an even distribution(i.e., like uniform distribution), the higher the entropy. In particular, when $x$ is a continuous variable, the Shannon entropy is called differential entropy.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<h5 id="Application.1.1.-Maximization-of-Shannon-Entropy">Application.1.1. Maximization of Shannon Entropy<a class="anchor-link" href="#Application.1.1.-Maximization-of-Shannon-Entropy"> </a></h5><p>Let probability density function $p$ be for $x\in [a, b]$.<br />
Then,</p>
$$
\begin{matrix}
\int_{a}^{b} p(x)dx = 1 \\
\end{matrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Consider following problem</p>
$$
\max_{p(x)} H[p(x)] = \max_{p(x)} - \mathbb{E}[\log p(x)] = \max_{p(x)} - \int_a^b p(x) \log p(x) dx
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Above problem is equality constraind optimization problem. Therefore we have to get lagrangian function $\mathcal{L}$.</p>
$$
\mathcal{L} = - \int_a^b p(x) \log p(x) dx + \lambda_1 \left( \int_a^b p(x) dx - 1 \right)
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Above $\mathcal{L}$ is functional, then</p>
$$
\frac{\delta \mathcal{L}}{\delta p(x)} = - \log p(x) - 1 + \lambda_1 = 0
$$$$
\therefore \,\ p(x) = \exp(-1 + \lambda_1) = c \,\ \text{for some constant} \,\ c.
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By equality constraint, 
$$
\int_a^b c dx = c(b - a) = 1.
$$
Then</p>
$$
\therefore \,\ c = \frac{1}{b - a} = p(x) \sim U(a, b)
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In above result, we can know that the uniform distribution is maximized shannon entropy distribution.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<h5 id="Application.1.2.-Maximization-of-Shannon-Entropy-with-Fixed-Variation">Application.1.2. Maximization of Shannon Entropy with Fixed Variation<a class="anchor-link" href="#Application.1.2.-Maximization-of-Shannon-Entropy-with-Fixed-Variation"> </a></h5><p>Let the expectation of probability density function $p$ be $\mu$ and variation be $\sigma^2$.<br />
Then,</p>
$$
\begin{matrix}
\int_{-\infty}^{\infty} p(x)dx = 1 \\
\int_{-\infty}^{\infty} xp(x)dx = \mu \\
\int_{-\infty}^{\infty} (x - \mu)^2p(x)dx = \sigma^2 \\
\end{matrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For maximization of $H[p]$, we have to use lagrange multiplier.<br />
Then,</p>
$$
\max \mathcal{L}[p](\mathbf{\lambda}) = \max - \int_{-\infty}^{\infty} p(x) \log p(x) dx + \mathbf{\lambda}^T 
\begin{bmatrix}
\int_{-\infty}^{\infty} p(x)dx - 1 \\
\int_{-\infty}^{\infty} xp(x)dx - \mu \\
\int_{-\infty}^{\infty} (x-\mu)^2 p(x)dx - \sigma^2 \\
\end{bmatrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then,</p>
$$
\frac{\delta \mathcal{L}}{\delta p(x)} = - \log p(x) - 1 + \lambda_1  + \lambda_2x + \lambda_3(x-\mu)^2  = 0
$$$$
\therefore \,\ p(x) = \exp(-1 + \lambda_1 + \lambda_2x + \lambda_3(x-\mu)^2)
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By first equality constraint,</p>
$$
\begin{matrix}
\int_{-\infty}^{\infty} p(x) dx = \int_{-\infty}^{\infty} \exp(-1 + \lambda_1 + \lambda_2x + \lambda_3(x - \mu)^2) dx \\
= \int_{-\infty}^{\infty} \exp( \lambda_3x^2 + (\lambda_2 - 2\mu)x + \mu^2 \lambda_3 + \lambda_1 - 1) dx \quad (\lambda_3 &lt; 0)\\ 
\end{matrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\begin{matrix}
- |\lambda_3|(x^2 + \frac{\lambda_2 - 2\mu}{|\lambda_3|}x + \frac{(\lambda_2 - 2\mu)^2}{4\lambda_3^2} ) - \frac{(\lambda_2 - 2\mu)^2}{4|\lambda_3|} + \mu^2 \lambda_3 + \lambda_1 - 1 \\
= - |\lambda_3|( x + \frac{\lambda_2 - 2\mu}{2\lambda_3} )^2 - \frac{(\lambda_2 - 2\mu)^2}{4|\lambda_3|} + \mu^2 \lambda_3 + \lambda_1 - 1 \\
\end{matrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\begin{matrix}
\int_{-\infty}^{\infty} \exp( \lambda_3x^2 + (\lambda_2 - 2\mu)x + \mu^2 \lambda_3 + \lambda_1 - 1) dx \\
= \int_{-\infty}^{\infty} \exp(- |\lambda_3|( x + \frac{\lambda_2 - 2\mu}{2\lambda_3} )^2 - \frac{(\lambda_2 - 2\mu)^2}{4|\lambda_3|} + \mu^2 \lambda_3 + \lambda_1 - 1) dx \\
\end{matrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let $\sqrt{|\lambda_3|}(x + \frac{\lambda_2}{2\lambda_3}) = t$.<br />
Then</p>
$$
\begin{matrix}
\int_{-\infty}^{\infty} \exp(- |\lambda_3|( x + \frac{\lambda_2 - 2\mu}{2\lambda_3} )^2 - \frac{(\lambda_2 - 2\mu)^2}{4|\lambda_3|} + \mu^2 \lambda_3 + \lambda_1 - 1) dx \\
= \frac{1}{\sqrt{|\lambda_3|}} \exp\left\{ - \frac{(\lambda_2 - 2\mu)^2}{4|\lambda_3|} + \mu^2\lambda_3 + \lambda_1 - 1 \right\} \int_{-\infty}^{\infty} \exp(-t^2) dt
\end{matrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
$$
\therefore \,\ \frac{\sqrt{\pi}}{\sqrt{|\lambda_3|}} \exp\left\{ - \frac{(\lambda_2 - 2\mu)^2}{4|\lambda_3|} + \mu^2\lambda_3 + \lambda_1 - 1 \right\} = 1 \qquad \cdots \,\ (1) 
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In similar way by second equality constraint,</p>
$$
- \frac{(\lambda_2 - 2\mu)\sqrt{\pi}}{2\lambda_3 \sqrt{|\lambda_3|}} \exp\left\{ - \frac{(\lambda_2 - 2\mu)^2}{4|\lambda_3|} + \mu^2\lambda_3 + \lambda_1 - 1 \right\} = \mu \qquad \cdots \,\ (2)
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In similar way by third equality constraint,</p>
$$
\frac{\sqrt{\pi}}{\sqrt{|\lambda_3|}} \left( \frac{1}{2|\lambda_3|} + \frac{(\lambda_2 - 2\mu + 2\lambda_3 \mu)^2}{4\lambda_3^2} \right) = \sigma^2 \qquad \cdots \,\ (3)
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By $(1), \,\ (2), \,\ \text{and} \,\ (3)$,</p>
$$
\therefore \,\ p(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left\{ - \frac{(x - \mu)^2}{2\sigma^2} \right\}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
</div>





  </div><a class="u-url" href="/blog/information-theory/2022/02/24/ch01-entropy.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Heesung Yang&#39;s note</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/ndo04343" target="_blank" title="ndo04343"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
